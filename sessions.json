{
  "7a38fc32-bf66-43c9-bafd-2e9da706c9ea": {
    "created_at": "2025-11-29T21:28:47.423752",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T21:28:47.424766"
      }
    ]
  }
}