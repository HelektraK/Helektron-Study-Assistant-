{
  "7a38fc32-bf66-43c9-bafd-2e9da706c9ea": {
    "created_at": "2025-11-29T21:28:47.423752",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T21:28:47.424766"
      }
    ]
  },
  "f7adc01c-845a-4cca-bf2e-5a75de12d753": {
    "created_at": "2025-11-29T21:44:16.279938",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T21:44:16.286191"
      },
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T21:44:28.429655"
      },
      {
        "name": "MATH 456 Intro Presentation.pdf",
        "type": "document",
        "text": "Helektra Katsoulakis\nFrom Biophysical Neurons to \nArtificial and Spiking NNs\nMATH 456: Math Modeling\n\nWhat\nWhat Is the Project?\n\u2022 Model a single biological neuron using the Leaky Integrate-and-Fire (LIF) \nequation \n\u2022 Introduce Spiking Neural Networks as an architecture built from the LIF model \n\u2022 Compare to feed-forward NN activations (ReLU, sigmoid) \n\u2022 Goal: Understand how closely common NN activations match the biological \nLIF response and why SNNs may be more useful for brain-inspired AI\n\nWhy Why It Matters\n\u2022 By comparing a biological neuron \nmodel (LIF) to artificial activations, \nwe can see how closely machine \nlearning mimics real brain behavior. \n\u2022 This connection helps us understand \nhow mathematical models can \nexplain biological intelligence. \n\u2022 Spiking networks follow biological \nrules more closely, which may make \nthem more efficient than FFNNs. \n\u2022 Neuromorphic chips like Spikey \nshow that spiking networks can run \nuseful computations while using very \nlittle power.\n\nSpiking Neural Networks (SNNs)\n\u2022 SNNs are built directly from the LIF neuron model \n\u2022 SNNs are networks that compute with spikes, not continuous values \n\u2022 A neuron only fires when its input is large enough to produce an output \n\u2022 SNNs are often cheaper to run than standard NNs\n\nHow\nApproach \n\u2022 Start from LIF ODE:  \n         \n(\u00c9douard Lapicque, 1907) \n\u2022 Simulate voltage for different input currents \nand measure firing rate. \n\u2022 Fit functions (sigmoid/ReLU) to the curve. \n\u2022 Compare how well these fits match the \nbehavior of a simple spiking neuron \n\u2022 Relate these results to how SNNs process \ninformation\nCm\ndV\ndt = \u2212gL(V \u2212EL) + Iin\n\nReferences\n\u2022 1.3 Integrate-And-Fire Models | Neuronal Dynamics online book. (n.d.). \nhttps://neuronaldynamics.epfl.ch/online/Ch1.S3.html \n\u2022 Jr, A. (2025). Spiking Neural Networks: The Future of Brain-Inspired \nComputing. \\textit{International Journal of Engineering Trends and \nTechnology, 73}(10). \\url{https://doi.org/10.14445/22315381/ijett-v73i10p104} \n\u2022 Marshall, M. (2012). Chip that mimics the brain outstrips normal computers. \n\\textit{New Scientist}. \\url{https://www.sciencedirect.com/science/article/pii/\nS0262407912629972}\n\nThanks\n",
        "added_at": "2025-11-29T21:45:02.707851"
      }
    ]
  },
  "86f85249-4d11-47f6-8044-a44c067c86ec": {
    "created_at": "2025-11-29T21:45:14.584247",
    "files": [
      {
        "name": "MATH 456 Intro Presentation.pdf",
        "type": "document",
        "text": "Helektra Katsoulakis\nFrom Biophysical Neurons to \nArtificial and Spiking NNs\nMATH 456: Math Modeling\n\nWhat\nWhat Is the Project?\n\u2022 Model a single biological neuron using the Leaky Integrate-and-Fire (LIF) \nequation \n\u2022 Introduce Spiking Neural Networks as an architecture built from the LIF model \n\u2022 Compare to feed-forward NN activations (ReLU, sigmoid) \n\u2022 Goal: Understand how closely common NN activations match the biological \nLIF response and why SNNs may be more useful for brain-inspired AI\n\nWhy Why It Matters\n\u2022 By comparing a biological neuron \nmodel (LIF) to artificial activations, \nwe can see how closely machine \nlearning mimics real brain behavior. \n\u2022 This connection helps us understand \nhow mathematical models can \nexplain biological intelligence. \n\u2022 Spiking networks follow biological \nrules more closely, which may make \nthem more efficient than FFNNs. \n\u2022 Neuromorphic chips like Spikey \nshow that spiking networks can run \nuseful computations while using very \nlittle power.\n\nSpiking Neural Networks (SNNs)\n\u2022 SNNs are built directly from the LIF neuron model \n\u2022 SNNs are networks that compute with spikes, not continuous values \n\u2022 A neuron only fires when its input is large enough to produce an output \n\u2022 SNNs are often cheaper to run than standard NNs\n\nHow\nApproach \n\u2022 Start from LIF ODE:  \n         \n(\u00c9douard Lapicque, 1907) \n\u2022 Simulate voltage for different input currents \nand measure firing rate. \n\u2022 Fit functions (sigmoid/ReLU) to the curve. \n\u2022 Compare how well these fits match the \nbehavior of a simple spiking neuron \n\u2022 Relate these results to how SNNs process \ninformation\nCm\ndV\ndt = \u2212gL(V \u2212EL) + Iin\n\nReferences\n\u2022 1.3 Integrate-And-Fire Models | Neuronal Dynamics online book. (n.d.). \nhttps://neuronaldynamics.epfl.ch/online/Ch1.S3.html \n\u2022 Jr, A. (2025). Spiking Neural Networks: The Future of Brain-Inspired \nComputing. \\textit{International Journal of Engineering Trends and \nTechnology, 73}(10). \\url{https://doi.org/10.14445/22315381/ijett-v73i10p104} \n\u2022 Marshall, M. (2012). Chip that mimics the brain outstrips normal computers. \n\\textit{New Scientist}. \\url{https://www.sciencedirect.com/science/article/pii/\nS0262407912629972}\n\nThanks\n",
        "added_at": "2025-11-29T21:45:14.595240"
      }
    ]
  },
  "d1db0a2b-8f9b-43b1-a4e6-beaa3f7a88a5": {
    "created_at": "2025-11-29T21:56:49.566426",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T21:56:49.568074"
      }
    ]
  },
  "e571114b-6d5d-46f4-8df8-2d089a02da19": {
    "created_at": "2025-11-29T23:58:28.684161",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T23:58:28.686446"
      }
    ]
  },
  "ac1d9614-f618-4585-b0a5-d96b5221ba4f": {
    "created_at": "2025-11-29T23:58:46.178838",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T23:58:46.180206"
      }
    ]
  },
  "9facd3d6-ef51-49da-9409-b3d58cc48956": {
    "created_at": "2025-11-29T23:59:35.568853",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T23:59:35.572634"
      }
    ]
  },
  "63c44681-37f1-4483-b3a6-f547ff75a429": {
    "created_at": "2025-11-30T00:59:51.580201",
    "files": [
      {
        "name": "326 Proj.pdf",
        "type": "document",
        "text": "Python\ngit add .  \ngit commit -m \u201c   \u201c \ngit push -u origin main  \n \n \nmain.py:  \n \nimport os \nimport json \nimport uuid \nimport subprocess \nfrom datetime import datetime \nfrom typing import Dict, Any \n \nimport fitz  # PyMuPDF \nfrom pptx import Presentation \nfrom fastapi import FastAPI, Request, UploadFile, File, Form \nfrom fastapi.responses import HTMLResponse, PlainTextResponse \nfrom fastapi.staticfiles import StaticFiles \nfrom fastapi.templating import Jinja2Templates \nimport google.generativeai as genai \n \n# Ai model  \ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\")) \nGEM_MODEL = genai.GenerativeModel(\"gemini-1.5-flash\") \n \n# ----------------- CONFIG ----------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \nUPLOAD_DIR = os.path.join(BASE_DIR, \"upload\") \nAUDIO_DIR = os.path.join(UPLOAD_DIR, \"audio\") \nDOC_DIR = os.path.join(UPLOAD_DIR, \"docs\") \nSESSIONS_PATH = os.path.join(BASE_DIR, \"sessions.json\") \n \nos.makedirs(UPLOAD_DIR, exist_ok=True) \nos.makedirs(AUDIO_DIR, exist_ok=True) \nos.makedirs(DOC_DIR, exist_ok=True) \n \n#client = (api_key=os.getenv(\"GEMINI_API_KEY\")) \n \napp = FastAPI(title=\"Helektron Study Assistant\") \napp.mount(\"/static\", StaticFiles(directory=os.path.join(BASE_DIR, \"static\")), \nname=\"static\") \ntemplates = Jinja2Templates(directory=os.path.join(BASE_DIR, \"templates\")) \n \n\n# ----------------- SESSION STORAGE ----------------- \ndef load_sessions() -> Dict[str, Any]: \n    if not os.path.exists(SESSIONS_PATH): \n        return {} \n    with open(SESSIONS_PATH, \"r\") as f: \n        try: \n            return json.load(f) \n        except json.JSONDecodeError: \n            return {} \n \ndef save_sessions(sessions: Dict[str, Any]) -> None: \n    with open(SESSIONS_PATH, \"w\") as f: \n        json.dump(sessions, f, indent=2) \n \nsessions: Dict[str, Any] = load_sessions() \n \ndef get_or_create_session(session_id: str | None) -> str: \n    global sessions \n    if session_id and session_id in sessions: \n        return session_id \n    new_id = str(uuid.uuid4()) \n    sessions[new_id] = { \n        \"created_at\": datetime.utcnow().isoformat(), \n        \"files\": [],   # list of {\"name\",\"type\",\"text\"} \n    } \n    save_sessions(sessions) \n    return new_id \n \ndef add_text_to_session(session_id: str, filename: str, file_type: str, text: \nstr) -> None: \n    global sessions \n    if session_id not in sessions: \n        session_id = get_or_create_session(None) \n    sessions[session_id][\"files\"].append({ \n        \"name\": filename, \n        \"type\": file_type, \n        \"text\": text, \n        \"added_at\": datetime.utcnow().isoformat() \n    }) \n    save_sessions(sessions) \n \ndef get_session_text(session_id: str) -> str: \n    session = sessions.get(session_id) \n    if not session: \n\n        return \"\" \n    blocks = [f\"--- {f['name']} ({f['type']}) ---\\n{f['text']}\" for f in \nsession[\"files\"] if f.get(\"text\")] \n    return \"\\n\\n\".join(blocks) \n \n# ----------------- HELPERS: FILE TEXT EXTRACTION ----------------- \ndef extract_txt(path: str) -> str: \n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        return f.read() \n \ndef extract_pdf(path: str) -> str: \n    doc = fitz.open(path) \n    texts = [] \n    for page in doc: \n        texts.append(page.get_text()) \n    doc.close() \n    return \"\\n\".join(texts) \n \ndef extract_pptx(path: str) -> str: \n    prs = Presentation(path) \n    texts = [] \n    for slide in prs.slides: \n        for shape in slide.shapes: \n            if hasattr(shape, \"text\"): \n                texts.append(shape.text) \n    return \"\\n\".join(texts) \n \ndef transcribe_audio(path: str) -> str: \n    # Find whisper binary \n    candidates = [ \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"build\", \"bin\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper-main\", \"build\", \"bin\", \"whisper-cli\"), \n    ] \n    whisper_bin = next((c for c in candidates if os.path.exists(c)), None) \n    if not whisper_bin: \n        raise RuntimeError(\"Whisper binary not found. Build whisper.cpp \nfirst.\") \n \n    wav_path = path + \".wav\" \n    subprocess.run( \n        [\"ffmpeg\", \"-y\", \"-i\", path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \n\"-ac\", \"1\", wav_path], \n        check=True \n\n    ) \n \n    out_prefix = path + \"_out\" \n    subprocess.run( \n        [whisper_bin, \"-m\", os.path.join(BASE_DIR, \"whisper.cpp\", \"models\", \n\"ggml-base.en.bin\"), \n         \"-f\", wav_path, \"--output-txt\", \"--output-file\", out_prefix], \n        check=True \n    ) \n \n    txt_path = out_prefix + \".txt\" \n    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        transcript = f.read() \n \n    # cleanup \n    for p in [wav_path, txt_path]: \n        if os.path.exists(p): \n            os.remove(p) \n \n    return transcript \n \n# ----------------- GPT PROMPTS ----------------- \ndef get_summary_prompt(transcript: str) -> str: \n    return f\"\"\" \nBased on the following combined study materials (lectures, slides, PDFs, notes, \ntranscripts), \ncreate a **structured, detailed, and academically accurate study summary**. \n \nOrganize the output into the following clearly labeled sections: \n \n- **Overview**: Briefly describe the overall topic and goals. \n- **Key Concepts Introduced**: List and explain major ideas, theories, \nformulas, or processes. \n- **Detailed Topic Breakdown**: Group related ideas and summarize explanations, \nreasoning steps, and relationships. \n- **Important Definitions**: Provide concise definitions for technical terms or \ndomain-specific vocabulary. \n- **Examples or Applications**: Summarize any examples, demonstrations, or \nreal-world applications. \n- **Main Takeaways**: What a student should remember after studying this \nmaterial. \n \nExpectations: \n- Use clear, concise bullet points. \n\n- Prioritize conceptual correctness. \n- Use simple academic language suitable for undergraduate study. \n- If information is unclear or missing, label it as `TBD`. \n \n---BEGIN MATERIAL--- \n{transcript} \n---END MATERIAL--- \n\"\"\" \n \ndef get_keyterms_prompt(text: str) -> str: \n    return f\"\"\" \nFrom the combined study materials below, extract **10\u201320 key terms** with short \ndefinitions. \n \nFor each term, include: \n- The term \n- A 1\u20132 sentence definition \n- (Optional) A quick note on why it matters in this context. \n \nFormat as a bulleted list. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_questions_prompt(text: str) -> str: \n    return f\"\"\" \nCreate **8\u201312 practice questions** based on the combined study materials below. \n \nInclude a mix of: \n- Conceptual understanding questions \n- Short-answer questions \n- Application/problem-style questions (where possible) \n \nDo NOT provide answers, only questions. \nGroup them into sections if appropriate (e.g., 'Conceptual', 'Application'). \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_resources_prompt(text: str) -> str: \n\n    return f\"\"\" \nBased on the topics and concepts in the combined study materials below, \nrecommend **3\u20137 high-quality external resources** for further study. \n \nEach resource should include: \n- Title \n- Type (e.g., textbook chapter, .edu article, video lecture) \n- Source (e.g., university, well-known platform) \n- 1\u20132 sentences on why it is relevant. \n \nPrefer: \n- .edu domains \n- Reputable textbooks \n- Well-known educational channels \n \nYou do NOT need to provide URLs, just clear, identifiable references. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef call_gemini(prompt: str) -> str: \n    try: \n        response = GEM_MODEL.generate_content(prompt) \n        return response.text.strip() \n    except Exception as e: \n        return f\"[Gemini API Error: {e}]\" \n \n# ----------------- ROUTES ----------------- \n@app.get(\"/\", response_class=HTMLResponse) \ndef index(request: Request): \n    return templates.TemplateResponse(\"index.html\", {\"request\": request}) \n \n# Upload any material (txt, pdf, pptx, mp4, m4a) \n@app.post(\"/upload\", response_class=HTMLResponse) \nasync def upload_material( \n    request: Request, \n    file: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    global sessions \n \n    session_id = get_or_create_session(session_id) \n\n    filename = file.filename \n    ext = filename.split(\".\")[-1].lower() \n \n    # Save file \n    if ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n        save_dir = AUDIO_DIR \n        kind = \"audio\" \n    else: \n        save_dir = DOC_DIR \n        kind = \"document\" \n \n    os.makedirs(save_dir, exist_ok=True) \n    saved_path = os.path.join(save_dir, f\"{uuid.uuid4().hex}_{filename}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await file.read()) \n \n    # Extract text \n    try: \n        if ext == \"txt\": \n            text = extract_txt(saved_path) \n        elif ext == \"pdf\": \n            text = extract_pdf(saved_path) \n        elif ext in [\"pptx\"]: \n            text = extract_pptx(saved_path) \n        elif ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n            text = transcribe_audio(saved_path) \n        else: \n            text = f\"[Unsupported file type: {ext}]\" \n    except Exception as e: \n        text = f\"[Error processing file {filename}: {e}]\" \n \n    add_text_to_session(session_id, filename, kind, text) \n \n    # Render updated materials list (fragment) \n    session = sessions[session_id] \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n\n \n# Live transcription: audio blob from the browser \n@app.post(\"/upload_live_audio\", response_class=HTMLResponse) \nasync def upload_live_audio( \n    request: Request, \n    audio: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    session_id = get_or_create_session(session_id) \n    filename = audio.filename or \"live_recording.webm\" \n    ext = filename.split(\".\")[-1].lower() \n \n    os.makedirs(AUDIO_DIR, exist_ok=True) \n    saved_path = os.path.join(AUDIO_DIR, f\"live_{uuid.uuid4().hex}.{ext}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await audio.read()) \n \n    try: \n        text = transcribe_audio(saved_path) \n    except Exception as e: \n        text = f\"[Error transcribing live audio: {e}]\" \n \n    add_text_to_session(session_id, filename, \"audio-live\", text) \n    session = sessions[session_id] \n \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# ---- Study tools (summary, key terms, questions, resources) ---- \n \n@app.get(\"/summary/{session_id}\", response_class=PlainTextResponse) \ndef generate_summary(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_summary_prompt(text) \n\nPython\n        content = call_gemini(prompt) \n    # Return plain text directly to HTMX target \n    return content \n \n \n@app.get(\"/keyterms/{session_id}\", response_class=PlainTextResponse) \ndef generate_keyterms(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_keyterms_prompt(text) \n        content = call_gemini(prompt) \n    return content \n \n \n@app.get(\"/questions/{session_id}\", response_class=PlainTextResponse) \ndef generate_questions_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_questions_prompt(text) \n        content = call_gemini(prompt) \n    return content \n \n \n@app.get(\"/resources/{session_id}\", response_class=PlainTextResponse) \ndef generate_resources_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_resources_prompt(text) \n        content = call_gemini(prompt) \n    return content \nRag Folder:  \n1.\u200b build_vector_store.py  \n \nimport os \nimport json \n\nimport faiss \nfrom langchain_community.vectorstores import FAISS \nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain_core.documents import Document \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter \n \n# -------------------------------------------- \n# Correct project paths \n# -------------------------------------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \n \n# Uploaded document text is stored here by your FastAPI app \nDOC_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"docs\") \n \n# Vector store output directory \nVS_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"vs\") \n \nos.makedirs(VS_DIR, exist_ok=True) \n \n \n# -------------------------------------------- \n# Load documents from upload/docs/ \n# -------------------------------------------- \ndef load_documents(): \n    docs = [] \n    if not os.path.exists(DOC_DIR): \n        print(f\"Missing directory {DOC_DIR}. No documents found.\") \n        return docs \n \n    for filename in os.listdir(DOC_DIR): \n        path = os.path.join(DOC_DIR, filename) \n        if not os.path.isfile(path): \n            continue \n \n        try: \n            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n                text = f.read().strip() \n                if text: \n                    docs.append(Document(page_content=text, metadata={\"source\": \nfilename})) \n        except Exception as e: \n            print(f\"Error loading {filename}: {e}\") \n \n    return docs \n\n \n \n# -------------------------------------------- \n# Build vector store \n# -------------------------------------------- \ndef build_vector_store(): \n    print(f\"Building vector store from {DOC_DIR}...\") \n \n    docs = load_documents() \n \n    if not docs: \n        print(\"No documents found in upload/docs/. Upload files in the web app \nfirst.\") \n        return \n \n    print(f\"Loaded {len(docs)} raw documents.\") \n \n    # ------------------------------- \n    # Chunk documents \n    # ------------------------------- \n    splitter = RecursiveCharacterTextSplitter( \n        chunk_size=1000, # ~750 tokens \n        chunk_overlap=200, # contextual overlap \n        length_function=len \n    ) \n \n    split_docs = [] \n    for doc in docs: \n        chunks = splitter.split_text(doc.page_content) \n        for chunk in chunks: \n            split_docs.append( \n                Document( \n                    page_content=chunk, \n                    metadata=doc.metadata  # keep source info \n                ) \n            ) \n \n    print(f\"Created {len(split_docs)} total chunks.\") \n \n    # ------------------------------- \n    # Generate embeddings \n    # ------------------------------- \n    embeddings = HuggingFaceEmbeddings() \n \n\nPython\n    print(\"Generating embeddings...\") \n    vectorstore = FAISS.from_documents(split_docs, embeddings) \n \n    # ------------------------------- \n    # Save FAISS index + metadata \n    # ------------------------------- \n    print(\"Saving FAISS index and metadata...\") \n \n    faiss.write_index(vectorstore.index, os.path.join(VS_DIR, \"vs.faiss\")) \n \n    with open(os.path.join(VS_DIR, \"docs.json\"), \"w\") as f: \n        json.dump([d.dict() for d in split_docs], f, indent=2) \n \n    print(\"\\n\ud83c\udf89 Vector store built successfully!\") \n    print(f\"Saved to: {VS_DIR}\") \n \n \nif __name__ == \"__main__\": \n    build_vector_store() \n \n2.\u200b rag_utils.py \n \nimport os \nimport json \nimport numpy as np \nfrom google import genai \n \n# Load Gemini client \nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\")) \n \n \n# ----------------------------- \n# Helper: Load vector store \n# ----------------------------- \ndef load_store(path): \n    if not os.path.exists(path): \n        return [] \n    with open(path, \"r\") as f: \n        return json.load(f) \n \n \n\n# ----------------------------- \n# Helper: Save vector store \n# ----------------------------- \ndef save_store(path, data): \n    with open(path, \"w\") as f: \n        json.dump(data, f, indent=4) \n \n \n# ----------------------------- \n# Embed text using Gemini \n# ----------------------------- \ndef embed_text(text: str): \n    \"\"\" \n    Uses Gemini embeddings. \n    Returns list[float]. \n    \"\"\" \n \n    result = client.models.embed_content( \n        model=\"text-embedding-004\", \n        contents=text, \n    ) \n \n    # Gemini wraps embeddings inside .output_embedding \n    return result.output_embedding \n \n \n# ----------------------------- \n# Add text to class vector store \n# ----------------------------- \ndef add_to_vector_store(class_id: str, text: str): \n    \"\"\" \n    Creates or appends to the vector store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    store = load_store(store_path) \n \n    embedding = embed_text(text) \n \n    store.append({ \n        \"text\": text, \n        \"embedding\": embedding, \n\n    }) \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Build store from scratch (optional) \n# ----------------------------- \ndef build_vector_store(class_id: str, text: str): \n    \"\"\" \n    Clears and rebuilds the entire store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    embedding = embed_text(text) \n \n    store = [ \n        { \n            \"text\": text, \n            \"embedding\": embedding, \n        } \n    ] \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Search RAG store using cosine similarity \n# ----------------------------- \ndef search_vector_store(class_id: str, query: str): \n    \"\"\" \n    Returns the most relevant text chunk for the given query. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    store = load_store(store_path) \n \n    if not store: \n        return None \n \n    query_embedding = embed_text(query) \n    query_vec = np.array(query_embedding) \n\nHTML\n \n    best_score = -999 \n    best_text = None \n \n    for item in store: \n        vec = np.array(item[\"embedding\"]) \n \n        # cosine similarity \n        score = np.dot(query_vec, vec) / ( \n            np.linalg.norm(query_vec) * np.linalg.norm(vec) \n        ) \n \n        if score > best_score: \n            best_score = score \n            best_text = item[\"text\"] \n \n    return best_text \n \n \nTemplates \n1.\u200b templates/index.html  \n \n{% extends \"base.html\" %} \n{% block content %} \n \n<h1>Helektron Study Assistant</h1> \n<p class=\"subtitle\"> \n    Upload lecture materials (PDF, slides, audio, notes), then generate \nsummaries, key terms, questions, and more. \n</p> \n \n<div class=\"layout\"> \n \n    <!-- LEFT PANEL --> \n    <div class=\"card\"> \n        <h2>1. Upload Materials</h2> \n \n        <div class=\"upload-box\"> \n            <p><strong>Upload files:</strong> .txt, .pdf, .pptx, .mp4, .m4a</p> \n \n            <form \n\n                hx-post=\"/upload\" \n                hx-target=\"#materials-panel\" \n                hx-swap=\"innerHTML\" \n                hx-indicator=\"#upload-loading\" \n                enctype=\"multipart/form-data\"> \n \n                <input type=\"file\" name=\"file\" required> \n                <input type=\"hidden\" name=\"session_id\" id=\"session_id_input\"> \n                <br><br> \n                <button class=\"primary\" type=\"submit\">Upload</button> \n            </form> \n        </div> \n \n        <div class=\"upload-box\"> \n            <p><strong>Live Transcription:</strong> record audio and transcribe \nwith Whisper.</p> \n            <button class=\"secondary\" id=\"record-btn\">\ud83c\udf99 Start \nRecording</button> \n            <p id=\"record-status\" style=\"font-size: 12px; color:#555; \nmargin-top:6px;\"></p> \n        </div> \n \n        <div id=\"upload-loading\" class=\"htmx-indicator\"> \n            Uploading/processing material... \n        </div> \n \n        <!-- \ud83d\udd25 THIS GETS UPDATED BY upload_status.html --> \n        <div id=\"materials-panel\"> \n            <em>No materials uploaded yet.</em> \n        </div> \n \n    </div> \n \n    <!-- RIGHT PANEL --> \n    <div class=\"card\"> \n        <h2>2. Generate Study Material</h2> \n \n        <p style=\"font-size:13px; color:#555;\">Choose what you want the AI to \ngenerate.</p> \n \n        <div style=\"display:flex; gap:8px; flex-wrap:wrap; \nmargin-bottom:10px;\"> \n            <button class=\"primary\" \n                    id=\"btn-summary\" \n\n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Summary \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-keyterms\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Key Terms \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-questions\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Practice Questions \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-resources\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                External Resources \n            </button> \n        </div> \n \n        <div id=\"generate-loading\" class=\"htmx-indicator\"> \n            Generating material... \n        </div> \n \n        <div id=\"results-panel\"> \n            <em>Your generated study content will appear here.</em> \n        </div> \n \n    </div> \n\nHTML\nHTML\n</div> \n \n<script src=\"/static/app.js\"></script> \n \n{% endblock %} \n \n2.\u200b templates/base.html  \n \n<!DOCTYPE html> \n<html lang=\"en\"> \n<head> \n    <meta charset=\"UTF-8\"> \n    <title>Helektron Study Assistant</title> \n    <link rel=\"stylesheet\" href=\"/static/styles.css\"> \n</head> \n \n<body> \n<div class=\"app-shell\"> \n    {% block content %}{% endblock %} \n</div> \n \n<script src=\"https://unpkg.com/htmx.org@1.9.10\"></script> \n</body> \n</html> \n \n3.\u200b Fragments folder \na.\u200b templates/fragments/class_list.html  \n \n<ul> \n    {% for cls in classes %} \n    <li> \n        <button  \n            hx-post=\"/set_active_class\"  \n            hx-vals='{\"class_id\": \"{{ cls.id }}\"}' \n            hx-target=\"#upload-list\" \n            hx-swap=\"innerHTML\" \n            class=\"{% if cls.id == active_class_id %}active{% endif %}\" \n\nHTML\nHTML\nHTML\n        > \n            {{ cls.name }} \n        </button> \n    </li> \n    {% endfor %} \n</ul> \n \nb.\u200b templates/fragments/upload_status.html  \n \n{% if session_id %} \n<div data-session-id=\"{{ session_id }}\"></div> \n{% endif %} \n \n{% if files %} \n<ul> \n    {% for f in files %} \n    <li><strong>{{ f.name }}</strong> ({{ f.type }})</li> \n    {% endfor %} \n</ul> \n{% else %} \n<em>No materials uploaded yet.</em> \n{% endif %} \n \nc.\u200b templates/fragments/summary.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n \nd.\u200b templates/fragments/resources.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n\nHTML\nHTML\nCSS\ne.\u200b templates/fragments/questions.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \nf.\u200b\ntemplates/fragments/keyterms.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n \n \nStatic:  \n1.\u200b static/styles.css \n \nbody { \n    margin: 0; \n    font-family: system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", \nsans-serif; \n    background: #f4f4fb; \n} \n \n.app-shell { \n    max-width: 1100px; \n    margin: 0 auto; \n    padding: 24px; \n} \n \nh1 { \n    font-size: 28px; \n    margin-bottom: 8px; \n} \n \n.subtitle { \n    color: #555; \n    margin-bottom: 24px; \n} \n\n \n.layout { \n    display: grid; \n    grid-template-columns: 1.2fr 1.8fr; \n    gap: 20px; \n} \n \n.card { \n    background: white; \n    border-radius: 12px; \n    padding: 16px 20px; \n    box-shadow: 0 1px 3px rgba(15,23,42,0.08); \n    border: 1px solid #e2e8f0; \n} \n \n.card h2 { \n    font-size: 18px; \n    margin-top: 0; \n    margin-bottom: 12px; \n} \n \n.upload-box { \n    border: 2px dashed #cbd5f5; \n    border-radius: 10px; \n    padding: 16px; \n    text-align: center; \n    background: #f9fafb; \n    margin-bottom: 12px; \n} \n \n.upload-box input[type=\"file\"] { \n    margin-top: 8px; \n} \n \nbutton.primary { \n    background: #4f46e5; \n    color: white; \n    border: none; \n    border-radius: 8px; \n    padding: 10px 16px; \n    font-weight: 600; \n    cursor: pointer; \n    font-size: 14px; \n} \n\n \nbutton.primary:disabled { \n    opacity: 0.6; \n    cursor: not-allowed; \n} \n \nbutton.primary:hover:not(:disabled) { \n    background: #4338ca; \n} \n \nbutton.secondary { \n    background: white; \n    color: #4f46e5; \n    border: 1px solid #cbd5f5; \n    border-radius: 8px; \n    padding: 8px 14px; \n    font-weight: 500; \n    cursor: pointer; \n    font-size: 13px; \n} \n \nbutton.secondary:hover { \n    background: #eef2ff; \n} \n \n#materials-list { \n    font-size: 14px; \n    max-height: 200px; \n    overflow-y: auto; \n} \n \n#materials-list ul { \n    padding-left: 18px; \n} \n \n#results-panel { \n    min-height: 250px; \n    font-size: 14px; \n    white-space: pre-wrap; \n} \n \n/* htmx loading indicator */ \n.htmx-indicator { \n    display: none; \n\nJavaScript\n    margin-bottom: 8px; \n    font-size: 13px; \n    color: #6b21a8; \n} \n \n.htmx-request .htmx-indicator { \n    display: block; \n} \n2.\u200b static/app.js \n \nlet currentSessionId = null; \n \nfunction setSessionId(id) { \n    currentSessionId = id; \n    const hidden = document.getElementById(\"session_id_input\"); \n    if (hidden) hidden.value = id; \n    enableStudyButtons(); \n} \n \nfunction enableStudyButtons() { \n    if (!currentSessionId) return; \n \n    const summaryBtn = document.getElementById(\"btn-summary\"); \n    const keyBtn     = document.getElementById(\"btn-keyterms\"); \n    const qBtn       = document.getElementById(\"btn-questions\"); \n    const rBtn       = document.getElementById(\"btn-resources\"); \n \n    [summaryBtn, keyBtn, qBtn, rBtn].forEach(btn => btn.disabled = false); \n \n    summaryBtn.setAttribute(\"hx-get\", `/summary/${currentSessionId}`); \n    keyBtn.setAttribute(\"hx-get\", `/keyterms/${currentSessionId}`); \n    qBtn.setAttribute(\"hx-get\", `/questions/${currentSessionId}`); \n    rBtn.setAttribute(\"hx-get\", `/resources/${currentSessionId}`); \n} \n \ndocument.body.addEventListener(\"htmx:afterSettle\", function () { \n    // Look for the element *inside* #materials-panel that has data-session-id \n    const sidElem = document.querySelector(\"#materials-panel \n[data-session-id]\"); \n    const sid = sidElem ? sidElem.getAttribute(\"data-session-id\") : null; \n \n\n    if (sid && sid !== currentSessionId) { \n        setSessionId(sid); \n    } \n}); \n \n// recording JS: \nlet mediaRecorder = null; \nlet recordedChunks = []; \n \nconst recordBtn = document.getElementById(\"record-btn\"); \nconst recordStatus = document.getElementById(\"record-status\"); \n \nrecordBtn.addEventListener(\"click\", async () => { \n    if (!mediaRecorder || mediaRecorder.state === \"inactive\") { \n        try { \n            const stream = await navigator.mediaDevices.getUserMedia({ audio: \ntrue }); \n            mediaRecorder = new MediaRecorder(stream); \n            recordedChunks = []; \n \n            mediaRecorder.ondataavailable = (e) => { \n                if (e.data.size > 0) recordedChunks.push(e.data); \n            }; \n \n            mediaRecorder.onstop = async () => { \n                const blob = new Blob(recordedChunks, { type: \"audio/webm\" }); \n                await uploadLiveAudio(blob); \n                stream.getTracks().forEach(t => t.stop()); \n                recordStatus.textContent = \"Recording finished. \nTranscribing...\"; \n            }; \n \n            mediaRecorder.start(); \n            recordBtn.textContent = \"\u23f9 Stop Recording\"; \n            recordStatus.textContent = \"Recording...\"; \n        } catch (err) { \n            console.error(err); \n            alert(\"Could not access microphone.\"); \n        } \n    } else if (mediaRecorder.state === \"recording\") { \n        mediaRecorder.stop(); \n        recordBtn.textContent = \"\ud83c\udf99 Start Recording\"; \n    } \n}); \n\nJavaScript\n \nasync function uploadLiveAudio(blob) { \n    const formData = new FormData(); \n    formData.append(\"audio\", blob, \"live_recording.webm\"); \n    if (currentSessionId) { \n        formData.append(\"session_id\", currentSessionId); \n    } \n \n    const resp = await fetch(\"/upload_live_audio\", { \n        method: \"POST\", \n        body: formData \n    }); \n \n    const html = await resp.text(); \n    document.getElementById(\"materials-panel\").innerHTML = html; \n    recordStatus.textContent = \"Live audio processed.\"; \n} \n3.\u200b static/htmx.min.js \n \n(function(e,t){if(typeof define===\"function\"&&define.amd){define([],t)}else if(typeof module===\"object\"&&module.exports){module.exports=t()}else{e.htmx=e.htmx||t()}})(typeof self!==\"undefined\"?self:this,function(){return function(){\"use strict\";var \nQ={onLoad:F,process:zt,on:de,off:ge,trigger:ce,ajax:Nr,find:C,findAll:f,closest:v,values:function(e,t){var r=dr(e,t||\"post\");return \nr.values},remove:_,addClass:z,removeClass:n,toggleClass:$,takeClass:W,defineExtension:Ur,removeExtension:Br,logAll:V,logNone:j,logger:null,config:{historyEnabled:true,historyCacheSize:10,refreshOnHistoryMiss:false,defaultSwapStyle:\"innerHTML\",defaultSwapDelay:0,defaultSettleDelay:20,includeIndicatorStyles:true,indicat\norClass:\"htmx-indicator\",requestClass:\"htmx-request\",addedClass:\"htmx-added\",settlingClass:\"htmx-settling\",swappingClass:\"htmx-swapping\",allowEval:true,allowScriptTags:true,inlineScriptNonce:\"\",attributesToSettle:[\"class\",\"style\",\"width\",\"height\"],withCredentials:false,timeout:0,wsReconnectDelay:\"full-jitter\",wsBinary\nType:\"blob\",disableSelector:\"[hx-disable], \n[data-hx-disable]\",useTemplateFragments:false,scrollBehavior:\"smooth\",defaultFocusScroll:false,getCacheBusterParam:false,globalViewTransitions:false,methodsThatUseUrlParams:[\"get\"],selfRequestsOnly:false,ignoreTitle:false,scrollIntoViewOnBoost:true,triggerSpecsCache:null},parseInterval:d,_:t,createEventSource:function\n(e){return new EventSource(e,{withCredentials:true})},createWebSocket:function(e){var t=new WebSocket(e,[]);t.binaryType=Q.config.wsBinaryType;return t},version:\"1.9.10\"};var \nr={addTriggerHandler:Lt,bodyContains:se,canAccessLocalStorage:U,findThisElement:xe,filterValues:yr,hasAttribute:o,getAttributeValue:te,getClosestAttributeValue:ne,getClosestMatch:c,getExpressionVars:Hr,getHeaders:xr,getInputValues:dr,getInternalData:ae,getSwapSpecification:wr,getTriggerSpecs:it,getTarget:ye,makeFragme\nnt:l,mergeObjects:le,makeSettleInfo:T,oobSwap:Ee,querySelectorExt:ue,selectAndSwap:je,settleImmediately:nr,shouldCancel:ut,triggerEvent:ce,triggerErrorEvent:fe,withExtensions:R};var w=[\"get\",\"post\",\"put\",\"delete\",\"patch\"];var i=w.map(function(e){return\"[hx-\"+e+\"], [data-hx-\"+e+\"]\"}).join(\", \");var \nS=e(\"head\"),q=e(\"title\"),H=e(\"svg\",true);function e(e,t=false){return new RegExp(`<${e}(\\\\s[^>]*>|>)([\\\\s\\\\S]*?)<\\\\/${e}>`,t?\"gim\":\"im\")}function d(e){if(e==undefined){return undefined}let t=NaN;if(e.slice(-2)==\"ms\"){t=parseFloat(e.slice(0,-2))}else if(e.slice(-1)==\"s\"){t=parseFloat(e.slice(0,-1))*1e3}else \nif(e.slice(-1)==\"m\"){t=parseFloat(e.slice(0,-1))*1e3*60}else{t=parseFloat(e)}return isNaN(t)?undefined:t}function ee(e,t){return e.getAttribute&&e.getAttribute(t)}function o(e,t){return e.hasAttribute&&(e.hasAttribute(t)||e.hasAttribute(\"data-\"+t))}function te(e,t){return ee(e,t)||ee(e,\"data-\"+t)}function u(e){return \ne.parentElement}function re(){return document}function c(e,t){while(e&&!t(e)){e=u(e)}return e?e:null}function L(e,t,r){var n=te(t,r);var i=te(t,\"hx-disinherit\");if(e!==t&&i&&(i===\"*\"||i.split(\" \").indexOf(r)>=0)){return\"unset\"}else{return n}}function ne(t,r){var n=null;c(t,function(e){return \nn=L(t,e,r)});if(n!==\"unset\"){return n}}function h(e,t){var r=e.matches||e.matchesSelector||e.msMatchesSelector||e.mozMatchesSelector||e.webkitMatchesSelector||e.oMatchesSelector;return r&&r.call(e,t)}function A(e){var t=/<([a-z][^\\/\\0>\\x20\\t\\r\\n\\f]*)/i;var r=t.exec(e);if(r){return \nr[1].toLowerCase()}else{return\"\"}}function a(e,t){var r=new DOMParser;var n=r.parseFromString(e,\"text/html\");var i=n.body;while(t>0){t--;i=i.firstChild}if(i==null){i=re().createDocumentFragment()}return i}function N(e){return/<body/.test(e)}function l(e){var t=!N(e);var r=A(e);var \nn=e;if(r===\"head\"){n=n.replace(S,\"\")}if(Q.config.useTemplateFragments&&t){var i=a(\"<body><template>\"+n+\"</template></body>\",0);return i.querySelector(\"template\").content}switch(r){case\"thead\":case\"tbody\":case\"tfoot\":case\"colgroup\":case\"caption\":return a(\"<table>\"+n+\"</table>\",1);case\"col\":return \na(\"<table><colgroup>\"+n+\"</colgroup></table>\",2);case\"tr\":return a(\"<table><tbody>\"+n+\"</tbody></table>\",2);case\"td\":case\"th\":return a(\"<table><tbody><tr>\"+n+\"</tr></tbody></table>\",3);case\"script\":case\"style\":return a(\"<div>\"+n+\"</div>\",1);default:return a(n,0)}}function ie(e){if(e){e()}}function I(e,t){return \nObject.prototype.toString.call(e)===\"[object \"+t+\"]\"}function k(e){return I(e,\"Function\")}function P(e){return I(e,\"Object\")}function ae(e){var t=\"htmx-internal-data\";var r=e[t];if(!r){r=e[t]={}}return r}function M(e){var t=[];if(e){for(var r=0;r<e.length;r++){t.push(e[r])}}return t}function oe(e,t){if(e){for(var \nr=0;r<e.length;r++){t(e[r])}}}function X(e){var t=e.getBoundingClientRect();var r=t.top;var n=t.bottom;return r<window.innerHeight&&n>=0}function se(e){if(e.getRootNode&&e.getRootNode()instanceof window.ShadowRoot){return re().body.contains(e.getRootNode().host)}else{return re().body.contains(e)}}function D(e){return \ne.trim().split(/\\s+/)}function le(e,t){for(var r in t){if(t.hasOwnProperty(r)){e[r]=t[r]}}return e}function E(e){try{return JSON.parse(e)}catch(e){b(e);return null}}function U(){var e=\"htmx:localStorageTest\";try{localStorage.setItem(e,e);localStorage.removeItem(e);return true}catch(e){return false}}function \nB(t){try{var e=new URL(t);if(e){t=e.pathname+e.search}if(!/^\\/$/.test(t)){t=t.replace(/\\/+$/,\"\")}return t}catch(e){return t}}function t(e){return Tr(re().body,function(){return eval(e)})}function F(t){var e=Q.on(\"htmx:load\",function(e){t(e.detail.elt)});return e}function \nV(){Q.logger=function(e,t,r){if(console){console.log(t,e,r)}}}function j(){Q.logger=null}function C(e,t){if(t){return e.querySelector(t)}else{return C(re(),e)}}function f(e,t){if(t){return e.querySelectorAll(t)}else{return f(re(),e)}}function \n_(e,t){e=g(e);if(t){setTimeout(function(){_(e);e=null},t)}else{e.parentElement.removeChild(e)}}function z(e,t,r){e=g(e);if(r){setTimeout(function(){z(e,t);e=null},r)}else{e.classList&&e.classList.add(t)}}function \nn(e,t,r){e=g(e);if(r){setTimeout(function(){n(e,t);e=null},r)}else{if(e.classList){e.classList.remove(t);if(e.classList.length===0){e.removeAttribute(\"class\")}}}}function $(e,t){e=g(e);e.classList.toggle(t)}function W(e,t){e=g(e);oe(e.parentElement.children,function(e){n(e,t)});z(e,t)}function \nv(e,t){e=g(e);if(e.closest){return e.closest(t)}else{do{if(e==null||h(e,t)){return e}}while(e=e&&u(e));return null}}function s(e,t){return e.substring(0,t.length)===t}function G(e,t){return e.substring(e.length-t.length)===t}function J(e){var t=e.trim();if(s(t,\"<\")&&G(t,\"/>\")){return \nt.substring(1,t.length-2)}else{return t}}function Z(e,t){if(t.indexOf(\"closest \")===0){return[v(e,J(t.substr(8)))]}else if(t.indexOf(\"find \")===0){return[C(e,J(t.substr(5)))]}else if(t===\"next\"){return[e.nextElementSibling]}else if(t.indexOf(\"next \")===0){return[K(e,J(t.substr(5)))]}else \nif(t===\"previous\"){return[e.previousElementSibling]}else if(t.indexOf(\"previous \")===0){return[Y(e,J(t.substr(9)))]}else if(t===\"document\"){return[document]}else if(t===\"window\"){return[window]}else if(t===\"body\"){return[document.body]}else{return re().querySelectorAll(J(t))}}var K=function(e,t){var \nr=re().querySelectorAll(t);for(var n=0;n<r.length;n++){var i=r[n];if(i.compareDocumentPosition(e)===Node.DOCUMENT_POSITION_PRECEDING){return i}}};var Y=function(e,t){var r=re().querySelectorAll(t);for(var n=r.length-1;n>=0;n--){var i=r[n];if(i.compareDocumentPosition(e)===Node.DOCUMENT_POSITION_FOLLOWING){return \ni}}};function ue(e,t){if(t){return Z(e,t)[0]}else{return Z(re().body,e)[0]}}function g(e){if(I(e,\"String\")){return C(e)}else{return e}}function ve(e,t,r){if(k(t)){return{target:re().body,event:e,listener:t}}else{return{target:g(e),event:t,listener:r}}}function de(t,r,n){jr(function(){var \ne=ve(t,r,n);e.target.addEventListener(e.event,e.listener)});var e=k(r);return e?r:n}function ge(t,r,n){jr(function(){var e=ve(t,r,n);e.target.removeEventListener(e.event,e.listener)});return k(r)?r:n}var me=re().createElement(\"output\");function pe(e,t){var r=ne(e,t);if(r){if(r===\"this\"){return[xe(e,t)]}else{var \nn=Z(e,r);if(n.length===0){b('The selector \"'+r+'\" on '+t+\" returned no matches!\");return[me]}else{return n}}}}function xe(e,t){return c(e,function(e){return te(e,t)!=null})}function ye(e){var t=ne(e,\"hx-target\");if(t){if(t===\"this\"){return xe(e,\"hx-target\")}else{return ue(e,t)}}else{var r=ae(e);if(r.boosted){return \nre().body}else{return e}}}function be(e){var t=Q.config.attributesToSettle;for(var r=0;r<t.length;r++){if(e===t[r]){return true}}return false}function \nwe(t,r){oe(t.attributes,function(e){if(!r.hasAttribute(e.name)&&be(e.name)){t.removeAttribute(e.name)}});oe(r.attributes,function(e){if(be(e.name)){t.setAttribute(e.name,e.value)}})}function Se(e,t){var r=Fr(t);for(var n=0;n<r.length;n++){var i=r[n];try{if(i.isInlineSwap(e)){return true}}catch(e){b(e)}}return \ne===\"outerHTML\"}function Ee(e,i,a){var t=\"#\"+ee(i,\"id\");var o=\"outerHTML\";if(e===\"true\"){}else if(e.indexOf(\":\")>0){o=e.substr(0,e.indexOf(\":\"));t=e.substr(e.indexOf(\":\")+1,e.length)}else{o=e}var r=re().querySelectorAll(t);if(r){oe(r,function(e){var t;var \nr=i.cloneNode(true);t=re().createDocumentFragment();t.appendChild(r);if(!Se(o,e)){t=r}var \nn={shouldSwap:true,target:e,fragment:t};if(!ce(e,\"htmx:oobBeforeSwap\",n))return;e=n.target;if(n[\"shouldSwap\"]){Fe(o,e,e,t,a)}oe(a.elts,function(e){ce(e,\"htmx:oobAfterSwap\",n)})});i.parentNode.removeChild(i)}else{i.parentNode.removeChild(i);fe(re().body,\"htmx:oobErrorNoTarget\",{content:i})}return e}function \nCe(e,t,r){var n=ne(e,\"hx-select-oob\");if(n){var i=n.split(\",\");for(var a=0;a<i.length;a++){var o=i[a].split(\":\",2);var s=o[0].trim();if(s.indexOf(\"#\")===0){s=s.substring(1)}var l=o[1]||\"true\";var u=t.querySelector(\"#\"+s);if(u){Ee(l,u,r)}}}oe(f(t,\"[hx-swap-oob], [data-hx-swap-oob]\"),function(e){var \nt=te(e,\"hx-swap-oob\");if(t!=null){Ee(t,e,r)}})}function Re(e){oe(f(e,\"[hx-preserve], [data-hx-preserve]\"),function(e){var t=te(e,\"id\");var r=re().getElementById(t);if(r!=null){e.parentNode.replaceChild(r,e)}})}function Te(o,e,s){oe(e.querySelectorAll(\"[id]\"),function(e){var t=ee(e,\"id\");if(t&&t.length>0){var \nr=t.replace(\"'\",\"\\\\'\");var n=e.tagName.replace(\":\",\"\\\\:\");var i=o.querySelector(n+\"[id='\"+r+\"']\");if(i&&i!==o){var a=e.cloneNode();we(e,i);s.tasks.push(function(){we(e,a)})}}})}function Oe(e){return function(){n(e,Q.config.addedClass);zt(e);Nt(e);qe(e);ce(e,\"htmx:load\")}}function qe(e){var t=\"[autofocus]\";var \nr=h(e,t)?e:e.querySelector(t);if(r!=null){r.focus()}}function m(e,t,r,n){Te(e,r,n);while(r.childNodes.length>0){var i=r.firstChild;z(i,Q.config.addedClass);e.insertBefore(i,t);if(i.nodeType!==Node.TEXT_NODE&&i.nodeType!==Node.COMMENT_NODE){n.tasks.push(Oe(i))}}}function He(e,t){var \nr=0;while(r<e.length){t=(t<<5)-t+e.charCodeAt(r++)|0}return t}function Le(e){var t=0;if(e.attributes){for(var r=0;r<e.attributes.length;r++){var n=e.attributes[r];if(n.value){t=He(n.name,t);t=He(n.value,t)}}}return t}function Ae(e){var t=ae(e);if(t.onHandlers){for(var r=0;r<t.onHandlers.length;r++){const \nn=t.onHandlers[r];e.removeEventListener(n.event,n.listener)}delete t.onHandlers}}function Ne(e){var \nt=ae(e);if(t.timeout){clearTimeout(t.timeout)}if(t.webSocket){t.webSocket.close()}if(t.sseEventSource){t.sseEventSource.close()}if(t.listenerInfos){oe(t.listenerInfos,function(e){if(e.on){e.on.removeEventListener(e.trigger,e.listener)}})}Ae(e);oe(Object.keys(t),function(e){delete t[e]})}function \np(e){ce(e,\"htmx:beforeCleanupElement\");Ne(e);if(e.children){oe(e.children,function(e){p(e)})}}function Ie(t,e,r){if(t.tagName===\"BODY\"){return Ue(t,e,r)}else{var n;var i=t.previousSibling;m(u(t),t,e,r);if(i==null){n=u(t).firstChild}else{n=i.nextSibling}r.elts=r.elts.filter(function(e){return \ne!=t});while(n&&n!==t){if(n.nodeType===Node.ELEMENT_NODE){r.elts.push(n)}n=n.nextElementSibling}p(t);u(t).removeChild(t)}}function ke(e,t,r){return m(e,e.firstChild,t,r)}function Pe(e,t,r){return m(u(e),e,t,r)}function Me(e,t,r){return m(e,null,t,r)}function Xe(e,t,r){return m(u(e),e.nextSibling,t,r)}function \nDe(e,t,r){p(e);return u(e).removeChild(e)}function Ue(e,t,r){var n=e.firstChild;m(e,n,t,r);if(n){while(n.nextSibling){p(n.nextSibling);e.removeChild(n.nextSibling)}p(n);e.removeChild(n)}}function Be(e,t,r){var n=r||ne(e,\"hx-select\");if(n){var \ni=re().createDocumentFragment();oe(t.querySelectorAll(n),function(e){i.appendChild(e)});t=i}return t}function \nFe(e,t,r,n,i){switch(e){case\"none\":return;case\"outerHTML\":Ie(r,n,i);return;case\"afterbegin\":ke(r,n,i);return;case\"beforebegin\":Pe(r,n,i);return;case\"beforeend\":Me(r,n,i);return;case\"afterend\":Xe(r,n,i);return;case\"delete\":De(r,n,i);return;default:var a=Fr(t);for(var o=0;o<a.length;o++){var s=a[o];try{var \nl=s.handleSwap(e,r,n,i);if(l){if(typeof l.length!==\"undefined\"){for(var u=0;u<l.length;u++){var f=l[u];if(f.nodeType!==Node.TEXT_NODE&&f.nodeType!==Node.COMMENT_NODE){i.tasks.push(Oe(f))}}}return}}catch(e){b(e)}}if(e===\"innerHTML\"){Ue(r,n,i)}else{Fe(Q.config.defaultSwapStyle,t,r,n,i)}}}function \nVe(e){if(e.indexOf(\"<title\")>-1){var t=e.replace(H,\"\");var r=t.match(q);if(r){return r[2]}}}function je(e,t,r,n,i,a){i.title=Ve(n);var o=l(n);if(o){Ce(r,o,i);o=Be(r,o,a);Re(o);return Fe(e,r,t,o,i)}}function _e(e,t,r){var n=e.getResponseHeader(t);if(n.indexOf(\"{\")===0){var i=E(n);for(var a in \ni){if(i.hasOwnProperty(a)){var o=i[a];if(!P(o)){o={value:o}}ce(r,a,o)}}}else{var s=n.split(\",\");for(var l=0;l<s.length;l++){ce(r,s[l].trim(),[])}}}var ze=/\\s/;var x=/[\\s,]/;var $e=/[_$a-zA-Z]/;var We=/[_$a-zA-Z0-9]/;var Ge=['\"',\"'\",\"/\"];var Je=/[^\\s]/;var Ze=/[{(]/;var Ke=/[})]/;function Ye(e){var t=[];var \nr=0;while(r<e.length){if($e.exec(e.charAt(r))){var n=r;while(We.exec(e.charAt(r+1))){r++}t.push(e.substr(n,r-n+1))}else if(Ge.indexOf(e.charAt(r))!==-1){var i=e.charAt(r);var n=r;r++;while(r<e.length&&e.charAt(r)!==i){if(e.charAt(r)===\"\\\\\"){r++}r++}t.push(e.substr(n,r-n+1))}else{var a=e.charAt(r);t.push(a)}r++}return \nt}function Qe(e,t,r){return $e.exec(e.charAt(0))&&e!==\"true\"&&e!==\"false\"&&e!==\"this\"&&e!==r&&t!==\".\"}function et(e,t,r){if(t[0]===\"[\"){t.shift();var n=1;var i=\" return (function(\"+r+\"){ return (\";var a=null;while(t.length>0){var o=t[0];if(o===\"]\"){n--;if(n===0){if(a===null){i=i+\"true\"}t.shift();i+=\")})\";try{var \ns=Tr(e,function(){return Function(i)()},function(){return true});s.source=i;return s}catch(e){fe(re().body,\"htmx:syntax:error\",{error:e,source:i});return null}}}else if(o===\"[\"){n++}if(Qe(o,a,r)){i+=\"((\"+r+\".\"+o+\") ? (\"+r+\".\"+o+\") : (window.\"+o+\"))\"}else{i=i+o}a=t.shift()}}}function y(e,t){var \nr=\"\";while(e.length>0&&!t.test(e[0])){r+=e.shift()}return r}function tt(e){var t;if(e.length>0&&Ze.test(e[0])){e.shift();t=y(e,Ke).trim();e.shift()}else{t=y(e,x)}return t}var rt=\"input, textarea, select\";function nt(e,t,r){var n=[];var i=Ye(t);do{y(i,Je);var a=i.length;var \no=y(i,/[,\\[\\s]/);if(o!==\"\"){if(o===\"every\"){var s={trigger:\"every\"};y(i,Je);s.pollInterval=d(y(i,/[,\\[\\s]/));y(i,Je);var l=et(e,i,\"event\");if(l){s.eventFilter=l}n.push(s)}else if(o.indexOf(\"sse:\")===0){n.push({trigger:\"sse\",sseEvent:o.substr(4)})}else{var u={trigger:o};var \nl=et(e,i,\"event\");if(l){u.eventFilter=l}while(i.length>0&&i[0]!==\",\"){y(i,Je);var f=i.shift();if(f===\"changed\"){u.changed=true}else if(f===\"once\"){u.once=true}else if(f===\"consume\"){u.consume=true}else if(f===\"delay\"&&i[0]===\":\"){i.shift();u.delay=d(y(i,x))}else \nif(f===\"from\"&&i[0]===\":\"){i.shift();if(Ze.test(i[0])){var c=tt(i)}else{var c=y(i,x);if(c===\"closest\"||c===\"find\"||c===\"next\"||c===\"previous\"){i.shift();var h=tt(i);if(h.length>0){c+=\" \"+h}}}u.from=c}else if(f===\"target\"&&i[0]===\":\"){i.shift();u.target=tt(i)}else \nif(f===\"throttle\"&&i[0]===\":\"){i.shift();u.throttle=d(y(i,x))}else if(f===\"queue\"&&i[0]===\":\"){i.shift();u.queue=y(i,x)}else if(f===\"root\"&&i[0]===\":\"){i.shift();u[f]=tt(i)}else \nif(f===\"threshold\"&&i[0]===\":\"){i.shift();u[f]=y(i,x)}else{fe(e,\"htmx:syntax:error\",{token:i.shift()})}}n.push(u)}}if(i.length===a){fe(e,\"htmx:syntax:error\",{token:i.shift()})}y(i,Je)}while(i[0]===\",\"&&i.shift());if(r){r[t]=n}return n}function it(e){var t=te(e,\"hx-trigger\");var r=[];if(t){var \nn=Q.config.triggerSpecsCache;r=n&&n[t]||nt(e,t,n)}if(r.length>0){return r}else if(h(e,\"form\")){return[{trigger:\"submit\"}]}else if(h(e,'input[type=\"button\"], input[type=\"submit\"]')){return[{trigger:\"click\"}]}else if(h(e,rt)){return[{trigger:\"change\"}]}else{return[{trigger:\"click\"}]}}function \nat(e){ae(e).cancelled=true}function ot(e,t,r){var n=ae(e);n.timeout=setTimeout(function(){if(se(e)&&n.cancelled!==true){if(!ct(r,e,Wt(\"hx:poll:trigger\",{triggerSpec:r,target:e}))){t(e)}ot(e,t,r)}},r.pollInterval)}function st(e){return location.hostname===e.hostname&&ee(e,\"href\")&&ee(e,\"href\").indexOf(\"#\")!==0}function \nlt(t,r,e){if(t.tagName===\"A\"&&st(t)&&(t.target===\"\"||t.target===\"_self\")||t.tagName===\"FORM\"){r.boosted=true;var n,i;if(t.tagName===\"A\"){n=\"get\";i=ee(t,\"href\")}else{var \na=ee(t,\"method\");n=a?a.toLowerCase():\"get\";if(n===\"get\"){}i=ee(t,\"action\")}e.forEach(function(e){ht(t,function(e,t){if(v(e,Q.config.disableSelector)){p(e);return}he(n,i,e,t)},r,e,true)})}}function ut(e,t){if(e.type===\"submit\"||e.type===\"click\"){if(t.tagName===\"FORM\"){return true}if(h(t,'input[type=\"submit\"], \nbutton')&&v(t,\"form\")!==null){return true}if(t.tagName===\"A\"&&t.href&&(t.getAttribute(\"href\")===\"#\"||t.getAttribute(\"href\").indexOf(\"#\")!==0)){return true}}return false}function ft(e,t){return ae(e).boosted&&e.tagName===\"A\"&&t.type===\"click\"&&(t.ctrlKey||t.metaKey)}function ct(e,t,r){var \nn=e.eventFilter;if(n){try{return n.call(t,r)!==true}catch(e){fe(re().body,\"htmx:eventFilter:error\",{error:e,source:n.source});return true}}return false}function ht(a,o,e,s,l){var u=ae(a);var t;if(s.from){t=Z(a,s.from)}else{t=[a]}if(s.changed){t.forEach(function(e){var t=ae(e);t.lastValue=e.value})}oe(t,function(n){var \ni=function(e){if(!se(a)){n.removeEventListener(s.trigger,i);return}if(ft(a,e)){return}if(l||ut(e,a)){e.preventDefault()}if(ct(s,a,e)){return}var \nt=ae(e);t.triggerSpec=s;if(t.handledFor==null){t.handledFor=[]}if(t.handledFor.indexOf(a)<0){t.handledFor.push(a);if(s.consume){e.stopPropagation()}if(s.target&&e.target){if(!h(e.target,s.target)){return}}if(s.once){if(u.triggeredOnce){return}else{u.triggeredOnce=true}}if(s.changed){var \nr=ae(n);if(r.lastValue===n.value){return}r.lastValue=n.value}if(u.delayed){clearTimeout(u.delayed)}if(u.throttle){return}if(s.throttle>0){if(!u.throttle){o(a,e);u.throttle=setTimeout(function(){u.throttle=null},s.throttle)}}else \nif(s.delay>0){u.delayed=setTimeout(function(){o(a,e)},s.delay)}else{ce(a,\"htmx:trigger\");o(a,e)}}};if(e.listenerInfos==null){e.listenerInfos=[]}e.listenerInfos.push({trigger:s.trigger,listener:i,on:n});n.addEventListener(s.trigger,i)})}var vt=false;var dt=null;function \ngt(){if(!dt){dt=function(){vt=true};window.addEventListener(\"scroll\",dt);setInterval(function(){if(vt){vt=false;oe(re().querySelectorAll(\"[hx-trigger='revealed'],[data-hx-trigger='revealed']\"),function(e){mt(e)})}},200)}}function mt(t){if(!o(t,\"data-hx-revealed\")&&X(t)){t.setAttribute(\"data-hx-revealed\",\"true\");var \ne=ae(t);if(e.initHash){ce(t,\"revealed\")}else{t.addEventListener(\"htmx:afterProcessNode\",function(e){ce(t,\"revealed\")},{once:true})}}}function pt(e,t,r){var n=D(r);for(var i=0;i<n.length;i++){var a=n[i].split(/:(.+)/);if(a[0]===\"connect\"){xt(e,a[1],0)}if(a[0]===\"send\"){bt(e)}}}function \nxt(s,r,n){if(!se(s)){return}if(r.indexOf(\"/\")==0){var e=location.hostname+(location.port?\":\"+location.port:\"\");if(location.protocol==\"https:\"){r=\"wss://\"+e+r}else if(location.protocol==\"http:\"){r=\"ws://\"+e+r}}var \nt=Q.createWebSocket(r);t.onerror=function(e){fe(s,\"htmx:wsError\",{error:e,socket:t});yt(s)};t.onclose=function(e){if([1006,1012,1013].indexOf(e.code)>=0){var t=wt(n);setTimeout(function(){xt(s,r,n+1)},t)}};t.onopen=function(e){n=0};ae(s).webSocket=t;t.addEventListener(\"message\",function(e){if(yt(s)){return}var \nt=e.data;R(s,function(e){t=e.transformResponse(t,null,s)});var r=T(s);var n=l(t);var i=M(n.children);for(var a=0;a<i.length;a++){var o=i[a];Ee(te(o,\"hx-swap-oob\")||\"true\",o,r)}nr(r.tasks)})}function yt(e){if(!se(e)){ae(e).webSocket.close();return true}}function bt(u){var f=c(u,function(e){return \nae(e).webSocket!=null});if(f){u.addEventListener(it(u)[0].trigger,function(e){var t=ae(f).webSocket;var r=xr(u,f);var n=dr(u,\"post\");var i=n.errors;var a=n.values;var o=Hr(u);var s=le(a,o);var \nl=yr(s,u);l[\"HEADERS\"]=r;if(i&&i.length>0){ce(u,\"htmx:validation:halted\",i);return}t.send(JSON.stringify(l));if(ut(e,u)){e.preventDefault()}})}else{fe(u,\"htmx:noWebSocketSourceError\")}}function wt(e){var t=Q.config.wsReconnectDelay;if(typeof t===\"function\"){return t(e)}if(t===\"full-jitter\"){var r=Math.min(e,6);var \nn=1e3*Math.pow(2,r);return n*Math.random()}b('htmx.config.wsReconnectDelay must either be a function or the string \"full-jitter\"')}function St(e,t,r){var n=D(r);for(var i=0;i<n.length;i++){var a=n[i].split(/:(.+)/);if(a[0]===\"connect\"){Et(e,a[1])}if(a[0]===\"swap\"){Ct(e,a[1])}}}function Et(t,e){var \nr=Q.createEventSource(e);r.onerror=function(e){fe(t,\"htmx:sseError\",{error:e,source:r});Tt(t)};ae(t).sseEventSource=r}function Ct(a,o){var s=c(a,Ot);if(s){var l=ae(s).sseEventSource;var u=function(e){if(Tt(s)){return}if(!se(a)){l.removeEventListener(o,u);return}var \nt=e.data;R(a,function(e){t=e.transformResponse(t,null,a)});var r=wr(a);var n=ye(a);var i=T(a);je(r.swapStyle,n,a,t,i);nr(i.tasks);ce(a,\"htmx:sseMessage\",e)};ae(a).sseListener=u;l.addEventListener(o,u)}else{fe(a,\"htmx:noSSESourceError\")}}function Rt(e,t,r){var n=c(e,Ot);if(n){var i=ae(n).sseEventSource;var \na=function(){if(!Tt(n)){if(se(e)){t(e)}else{i.removeEventListener(r,a)}}};ae(e).sseListener=a;i.addEventListener(r,a)}else{fe(e,\"htmx:noSSESourceError\")}}function Tt(e){if(!se(e)){ae(e).sseEventSource.close();return true}}function Ot(e){return ae(e).sseEventSource!=null}function qt(e,t,r,n){var \ni=function(){if(!r.loaded){r.loaded=true;t(e)}};if(n>0){setTimeout(i,n)}else{i()}}function Ht(t,i,e){var a=false;oe(w,function(r){if(o(t,\"hx-\"+r)){var n=te(t,\"hx-\"+r);a=true;i.path=n;i.verb=r;e.forEach(function(e){Lt(t,e,i,function(e,t){if(v(e,Q.config.disableSelector)){p(e);return}he(r,n,e,t)})})}});return a}function \nLt(n,e,t,r){if(e.sseEvent){Rt(n,r,e.sseEvent)}else if(e.trigger===\"revealed\"){gt();ht(n,r,t,e);mt(n)}else if(e.trigger===\"intersect\"){var i={};if(e.root){i.root=ue(n,e.root)}if(e.threshold){i.threshold=parseFloat(e.threshold)}var a=new IntersectionObserver(function(e){for(var t=0;t<e.length;t++){var \nr=e[t];if(r.isIntersecting){ce(n,\"intersect\");break}}},i);a.observe(n);ht(n,r,t,e)}else if(e.trigger===\"load\"){if(!ct(e,n,Wt(\"load\",{elt:n}))){qt(n,r,t,e.delay)}}else if(e.pollInterval>0){t.polling=true;ot(n,r,e)}else{ht(n,r,t,e)}}function \nAt(e){if(Q.config.allowScriptTags&&(e.type===\"text/javascript\"||e.type===\"module\"||e.type===\"\")){var t=re().createElement(\"script\");oe(e.attributes,function(e){t.setAttribute(e.name,e.value)});t.textContent=e.textContent;t.async=false;if(Q.config.inlineScriptNonce){t.nonce=Q.config.inlineScriptNonce}var \nr=e.parentElement;try{r.insertBefore(t,e)}catch(e){b(e)}finally{if(e.parentElement){e.parentElement.removeChild(e)}}}}function Nt(e){if(h(e,\"script\")){At(e)}oe(f(e,\"script\"),function(e){At(e)})}function It(e){var t=e.attributes;for(var r=0;r<t.length;r++){var \nn=t[r].name;if(s(n,\"hx-on:\")||s(n,\"data-hx-on:\")||s(n,\"hx-on-\")||s(n,\"data-hx-on-\")){return true}}return false}function kt(e){var t=null;var r=[];if(It(e)){r.push(e)}if(document.evaluate){var n=document.evaluate('.//*[@*[ starts-with(name(), \"hx-on:\") or starts-with(name(), \"data-hx-on:\") or'+' starts-with(name(), \n\"hx-on-\") or starts-with(name(), \"data-hx-on-\") ]]',e);while(t=n.iterateNext())r.push(t)}else{var i=e.getElementsByTagName(\"*\");for(var a=0;a<i.length;a++){if(It(i[a])){r.push(i[a])}}}return r}function Pt(e){if(e.querySelectorAll){var t=\", [hx-boost] a, [data-hx-boost] a, a[hx-boost], a[data-hx-boost]\";var \nr=e.querySelectorAll(i+t+\", form, [type='submit'], [hx-sse], [data-hx-sse], [hx-ws],\"+\" [data-hx-ws], [hx-ext], [data-hx-ext], [hx-trigger], [data-hx-trigger], [hx-on], [data-hx-on]\");return r}else{return[]}}function Mt(e){var t=v(e.target,\"button, input[type='submit']\");var \n\nr=Dt(e);if(r){r.lastButtonClicked=t}}function Xt(e){var t=Dt(e);if(t){t.lastButtonClicked=null}}function Dt(e){var t=v(e.target,\"button, input[type='submit']\");if(!t){return}var r=g(\"#\"+ee(t,\"form\"))||v(t,\"form\");if(!r){return}return ae(r)}function \nUt(e){e.addEventListener(\"click\",Mt);e.addEventListener(\"focusin\",Mt);e.addEventListener(\"focusout\",Xt)}function Bt(e){var t=Ye(e);var r=0;for(var n=0;n<t.length;n++){const i=t[n];if(i===\"{\"){r++}else if(i===\"}\"){r--}}return r}function Ft(t,e,r){var n=ae(t);if(!Array.isArray(n.onHandlers)){n.onHandlers=[]}var i;var \na=function(e){return Tr(t,function(){if(!i){i=new Function(\"event\",r)}i.call(t,e)})};t.addEventListener(e,a);n.onHandlers.push({event:e,listener:a})}function Vt(e){var t=te(e,\"hx-on\");if(t){var r={};var n=t.split(\"\\n\");var i=null;var a=0;while(n.length>0){var o=n.shift();var \ns=o.match(/^\\s*([a-zA-Z:\\-\\.]+:)(.*)/);if(a===0&&s){o.split(\":\");i=s[1].slice(0,-1);r[i]=s[2]}else{r[i]+=o}a+=Bt(o)}for(var l in r){Ft(e,l,r[l])}}}function jt(e){Ae(e);for(var t=0;t<e.attributes.length;t++){var r=e.attributes[t].name;var n=e.attributes[t].value;if(s(r,\"hx-on\")||s(r,\"data-hx-on\")){var \ni=r.indexOf(\"-on\")+3;var a=r.slice(i,i+1);if(a===\"-\"||a===\":\"){var o=r.slice(i+1);if(s(o,\":\")){o=\"htmx\"+o}else if(s(o,\"-\")){o=\"htmx:\"+o.slice(1)}else if(s(o,\"htmx-\")){o=\"htmx:\"+o.slice(5)}Ft(e,o,n)}}}}function _t(t){if(v(t,Q.config.disableSelector)){p(t);return}var \nr=ae(t);if(r.initHash!==Le(t)){Ne(t);r.initHash=Le(t);Vt(t);ce(t,\"htmx:beforeProcessNode\");if(t.value){r.lastValue=t.value}var e=it(t);var n=Ht(t,r,e);if(!n){if(ne(t,\"hx-boost\")===\"true\"){lt(t,r,e)}else \nif(o(t,\"hx-trigger\")){e.forEach(function(e){Lt(t,e,r,function(){})})}}if(t.tagName===\"FORM\"||ee(t,\"type\")===\"submit\"&&o(t,\"form\")){Ut(t)}var i=te(t,\"hx-sse\");if(i){St(t,r,i)}var a=te(t,\"hx-ws\");if(a){pt(t,r,a)}ce(t,\"htmx:afterProcessNode\")}}function \nzt(e){e=g(e);if(v(e,Q.config.disableSelector)){p(e);return}_t(e);oe(Pt(e),function(e){_t(e)});oe(kt(e),jt)}function $t(e){return e.replace(/([a-z0-9])([A-Z])/g,\"$1-$2\").toLowerCase()}function Wt(e,t){var r;if(window.CustomEvent&&typeof window.CustomEvent===\"function\"){r=new \nCustomEvent(e,{bubbles:true,cancelable:true,detail:t})}else{r=re().createEvent(\"CustomEvent\");r.initCustomEvent(e,true,true,t)}return r}function fe(e,t,r){ce(e,t,le({error:t},r))}function Gt(e){return e===\"htmx:afterProcessNode\"}function R(e,t){oe(Fr(e),function(e){try{t(e)}catch(e){b(e)}})}function \nb(e){if(console.error){console.error(e)}else if(console.log){console.log(\"ERROR: \",e)}}function ce(e,t,r){e=g(e);if(r==null){r={}}r[\"elt\"]=e;var n=Wt(t,r);if(Q.logger&&!Gt(t)){Q.logger(e,t,r)}if(r.error){b(r.error);ce(e,\"htmx:error\",{errorInfo:r})}var i=e.dispatchEvent(n);var a=$t(t);if(i&&a!==t){var \no=Wt(a,n.detail);i=i&&e.dispatchEvent(o)}R(e,function(e){i=i&&(e.onEvent(t,n)!==false&&!n.defaultPrevented)});return i}var Jt=location.pathname+location.search;function Zt(){var e=re().querySelector(\"[hx-history-elt],[data-hx-history-elt]\");return e||re().body}function \nKt(e,t,r,n){if(!U()){return}if(Q.config.historyCacheSize<=0){localStorage.removeItem(\"htmx-history-cache\");return}e=B(e);var i=E(localStorage.getItem(\"htmx-history-cache\"))||[];for(var a=0;a<i.length;a++){if(i[a].url===e){i.splice(a,1);break}}var \no={url:e,content:t,title:r,scroll:n};ce(re().body,\"htmx:historyItemCreated\",{item:o,cache:i});i.push(o);while(i.length>Q.config.historyCacheSize){i.shift()}while(i.length>0){try{localStorage.setItem(\"htmx-history-cache\",JSON.stringify(i));break}catch(e){fe(re().body,\"htmx:historyCacheError\",{cause:e,cache:i});i.shift(\n)}}}function Yt(e){if(!U()){return null}e=B(e);var t=E(localStorage.getItem(\"htmx-history-cache\"))||[];for(var r=0;r<t.length;r++){if(t[r].url===e){return t[r]}}return null}function Qt(e){var t=Q.config.requestClass;var r=e.cloneNode(true);oe(f(r,\".\"+t),function(e){n(e,t)});return r.innerHTML}function er(){var \ne=Zt();var t=Jt||location.pathname+location.search;var r;try{r=re().querySelector('[hx-history=\"false\" i],[data-hx-history=\"false\" \ni]')}catch(e){r=re().querySelector('[hx-history=\"false\"],[data-hx-history=\"false\"]')}if(!r){ce(re().body,\"htmx:beforeHistorySave\",{path:t,historyElt:e});Kt(t,Qt(e),re().title,window.scrollY)}if(Q.config.historyEnabled)history.replaceState({htmx:true},re().title,window.location.href)}function \ntr(e){if(Q.config.getCacheBusterParam){e=e.replace(/org\\.htmx\\.cache-buster=[^&]*&?/,\"\");if(G(e,\"&\")||G(e,\"?\")){e=e.slice(0,-1)}}if(Q.config.historyEnabled){history.pushState({htmx:true},\"\",e)}Jt=e}function rr(e){if(Q.config.historyEnabled)history.replaceState({htmx:true},\"\",e);Jt=e}function \nnr(e){oe(e,function(e){e.call()})}function ir(a){var e=new XMLHttpRequest;var \no={path:a,xhr:e};ce(re().body,\"htmx:historyCacheMiss\",o);e.open(\"GET\",a,true);e.setRequestHeader(\"HX-Request\",\"true\");e.setRequestHeader(\"HX-History-Restore-Request\",\"true\");e.setRequestHeader(\"HX-Current-URL\",re().location.href);e.onload=function(){if(this.status>=200&&this.status<400){ce(re().body,\"htmx:historyCache\nMissLoad\",o);var e=l(this.response);e=e.querySelector(\"[hx-history-elt],[data-hx-history-elt]\")||e;var t=Zt();var r=T(t);var n=Ve(this.response);if(n){var \ni=C(\"title\");if(i){i.innerHTML=n}else{window.document.title=n}}Ue(t,e,r);nr(r.tasks);Jt=a;ce(re().body,\"htmx:historyRestore\",{path:a,cacheMiss:true,serverResponse:this.response})}else{fe(re().body,\"htmx:historyCacheMissLoadError\",o)}};e.send()}function ar(e){er();e=e||location.pathname+location.search;var \nt=Yt(e);if(t){var r=l(t.content);var n=Zt();var i=T(n);Ue(n,r,i);nr(i.tasks);document.title=t.title;setTimeout(function(){window.scrollTo(0,t.scroll)},0);Jt=e;ce(re().body,\"htmx:historyRestore\",{path:e,item:t})}else{if(Q.config.refreshOnHistoryMiss){window.location.reload(true)}else{ir(e)}}}function or(e){var \nt=pe(e,\"hx-indicator\");if(t==null){t=[e]}oe(t,function(e){var t=ae(e);t.requestCount=(t.requestCount||0)+1;e.classList[\"add\"].call(e.classList,Q.config.requestClass)});return t}function sr(e){var t=pe(e,\"hx-disabled-elt\");if(t==null){t=[]}oe(t,function(e){var \nt=ae(e);t.requestCount=(t.requestCount||0)+1;e.setAttribute(\"disabled\",\"\")});return t}function lr(e,t){oe(e,function(e){var t=ae(e);t.requestCount=(t.requestCount||0)-1;if(t.requestCount===0){e.classList[\"remove\"].call(e.classList,Q.config.requestClass)}});oe(t,function(e){var \nt=ae(e);t.requestCount=(t.requestCount||0)-1;if(t.requestCount===0){e.removeAttribute(\"disabled\")}})}function ur(e,t){for(var r=0;r<e.length;r++){var n=e[r];if(n.isSameNode(t)){return true}}return false}function fr(e){if(e.name===\"\"||e.name==null||e.disabled||v(e,\"fieldset[disabled]\")){return \nfalse}if(e.type===\"button\"||e.type===\"submit\"||e.tagName===\"image\"||e.tagName===\"reset\"||e.tagName===\"file\"){return false}if(e.type===\"checkbox\"||e.type===\"radio\"){return e.checked}return true}function cr(e,t,r){if(e!=null&&t!=null){var n=r[e];if(n===undefined){r[e]=t}else \nif(Array.isArray(n)){if(Array.isArray(t)){r[e]=n.concat(t)}else{n.push(t)}}else{if(Array.isArray(t)){r[e]=[n].concat(t)}else{r[e]=[n,t]}}}}function hr(t,r,n,e,i){if(e==null||ur(t,e)){return}else{t.push(e)}if(fr(e)){var a=ee(e,\"name\");var \no=e.value;if(e.multiple&&e.tagName===\"SELECT\"){o=M(e.querySelectorAll(\"option:checked\")).map(function(e){return e.value})}if(e.files){o=M(e.files)}cr(a,o,r);if(i){vr(e,n)}}if(h(e,\"form\")){var s=e.elements;oe(s,function(e){hr(t,r,n,e,i)})}}function \nvr(e,t){if(e.willValidate){ce(e,\"htmx:validation:validate\");if(!e.checkValidity()){t.push({elt:e,message:e.validationMessage,validity:e.validity});ce(e,\"htmx:validation:failed\",{message:e.validationMessage,validity:e.validity})}}}function dr(e,t){var r=[];var n={};var i={};var a=[];var \no=ae(e);if(o.lastButtonClicked&&!se(o.lastButtonClicked)){o.lastButtonClicked=null}var \ns=h(e,\"form\")&&e.noValidate!==true||te(e,\"hx-validate\")===\"true\";if(o.lastButtonClicked){s=s&&o.lastButtonClicked.formNoValidate!==true}if(t!==\"get\"){hr(r,i,a,v(e,\"form\"),s)}hr(r,n,a,e,s);if(o.lastButtonClicked||e.tagName===\"BUTTON\"||e.tagName===\"INPUT\"&&ee(e,\"type\")===\"submit\"){var l=o.lastButtonClicked||e;var \nu=ee(l,\"name\");cr(u,l.value,i)}var f=pe(e,\"hx-include\");oe(f,function(e){hr(r,n,a,e,s);if(!h(e,\"form\")){oe(e.querySelectorAll(rt),function(e){hr(r,n,a,e,s)})}});n=le(n,i);return{errors:a,values:n}}function gr(e,t,r){if(e!==\"\"){e+=\"&\"}if(String(r)===\"[object Object]\"){r=JSON.stringify(r)}var \nn=encodeURIComponent(r);e+=encodeURIComponent(t)+\"=\"+n;return e}function mr(e){var t=\"\";for(var r in e){if(e.hasOwnProperty(r)){var n=e[r];if(Array.isArray(n)){oe(n,function(e){t=gr(t,r,e)})}else{t=gr(t,r,n)}}}return t}function pr(e){var t=new FormData;for(var r in e){if(e.hasOwnProperty(r)){var \nn=e[r];if(Array.isArray(n)){oe(n,function(e){t.append(r,e)})}else{t.append(r,n)}}}return t}function xr(e,t,r){var \nn={\"HX-Request\":\"true\",\"HX-Trigger\":ee(e,\"id\"),\"HX-Trigger-Name\":ee(e,\"name\"),\"HX-Target\":te(t,\"id\"),\"HX-Current-URL\":re().location.href};Rr(e,\"hx-headers\",false,n);if(r!==undefined){n[\"HX-Prompt\"]=r}if(ae(e).boosted){n[\"HX-Boosted\"]=\"true\"}return n}function yr(t,e){var \nr=ne(e,\"hx-params\");if(r){if(r===\"none\"){return{}}else if(r===\"*\"){return t}else if(r.indexOf(\"not \")===0){oe(r.substr(4).split(\",\"),function(e){e=e.trim();delete t[e]});return t}else{var n={};oe(r.split(\",\"),function(e){e=e.trim();n[e]=t[e]});return n}}else{return t}}function br(e){return \nee(e,\"href\")&&ee(e,\"href\").indexOf(\"#\")>=0}function wr(e,t){var r=t?t:ne(e,\"hx-swap\");var n={swapStyle:ae(e).boosted?\"innerHTML\":Q.config.defaultSwapStyle,swapDelay:Q.config.defaultSwapDelay,settleDelay:Q.config.defaultSettleDelay};if(Q.config.scrollIntoViewOnBoost&&ae(e).boosted&&!br(e)){n[\"show\"]=\"top\"}if(r){var \ni=D(r);if(i.length>0){for(var a=0;a<i.length;a++){var o=i[a];if(o.indexOf(\"swap:\")===0){n[\"swapDelay\"]=d(o.substr(5))}else if(o.indexOf(\"settle:\")===0){n[\"settleDelay\"]=d(o.substr(7))}else if(o.indexOf(\"transition:\")===0){n[\"transition\"]=o.substr(11)===\"true\"}else \nif(o.indexOf(\"ignoreTitle:\")===0){n[\"ignoreTitle\"]=o.substr(12)===\"true\"}else if(o.indexOf(\"scroll:\")===0){var s=o.substr(7);var l=s.split(\":\");var u=l.pop();var f=l.length>0?l.join(\":\"):null;n[\"scroll\"]=u;n[\"scrollTarget\"]=f}else if(o.indexOf(\"show:\")===0){var c=o.substr(5);var l=c.split(\":\");var h=l.pop();var \nf=l.length>0?l.join(\":\"):null;n[\"show\"]=h;n[\"showTarget\"]=f}else if(o.indexOf(\"focus-scroll:\")===0){var v=o.substr(\"focus-scroll:\".length);n[\"focusScroll\"]=v==\"true\"}else if(a==0){n[\"swapStyle\"]=o}else{b(\"Unknown modifier in hx-swap: \"+o)}}}}return n}function Sr(e){return \nne(e,\"hx-encoding\")===\"multipart/form-data\"||h(e,\"form\")&&ee(e,\"enctype\")===\"multipart/form-data\"}function Er(t,r,n){var i=null;R(r,function(e){if(i==null){i=e.encodeParameters(t,n,r)}});if(i!=null){return i}else{if(Sr(r)){return pr(n)}else{return mr(n)}}}function T(e){return{tasks:[],elts:[e]}}function Cr(e,t){var \nr=e[0];var n=e[e.length-1];if(t.scroll){var i=null;if(t.scrollTarget){i=ue(r,t.scrollTarget)}if(t.scroll===\"top\"&&(r||i)){i=i||r;i.scrollTop=0}if(t.scroll===\"bottom\"&&(n||i)){i=i||n;i.scrollTop=i.scrollHeight}}if(t.show){var i=null;if(t.showTarget){var \na=t.showTarget;if(t.showTarget===\"window\"){a=\"body\"}i=ue(r,a)}if(t.show===\"top\"&&(r||i)){i=i||r;i.scrollIntoView({block:\"start\",behavior:Q.config.scrollBehavior})}if(t.show===\"bottom\"&&(n||i)){i=i||n;i.scrollIntoView({block:\"end\",behavior:Q.config.scrollBehavior})}}}function \nRr(e,t,r,n){if(n==null){n={}}if(e==null){return n}var i=te(e,t);if(i){var a=i.trim();var o=r;if(a===\"unset\"){return null}if(a.indexOf(\"javascript:\")===0){a=a.substr(11);o=true}else if(a.indexOf(\"js:\")===0){a=a.substr(3);o=true}if(a.indexOf(\"{\")!==0){a=\"{\"+a+\"}\"}var s;if(o){s=Tr(e,function(){return Function(\"return \n(\"+a+\")\")()},{})}else{s=E(a)}for(var l in s){if(s.hasOwnProperty(l)){if(n[l]==null){n[l]=s[l]}}}}return Rr(u(e),t,r,n)}function Tr(e,t,r){if(Q.config.allowEval){return t()}else{fe(e,\"htmx:evalDisallowedError\");return r}}function Or(e,t){return Rr(e,\"hx-vars\",true,t)}function qr(e,t){return \nRr(e,\"hx-vals\",false,t)}function Hr(e){return le(Or(e),qr(e))}function Lr(t,r,n){if(n!==null){try{t.setRequestHeader(r,n)}catch(e){t.setRequestHeader(r,encodeURIComponent(n));t.setRequestHeader(r+\"-URI-AutoEncoded\",\"true\")}}}function Ar(t){if(t.responseURL&&typeof URL!==\"undefined\"){try{var e=new \nURL(t.responseURL);return e.pathname+e.search}catch(e){fe(re().body,\"htmx:badResponseUrl\",{url:t.responseURL})}}}function O(e,t){return t.test(e.getAllResponseHeaders())}function Nr(e,t,r){e=e.toLowerCase();if(r){if(r instanceof Element||I(r,\"String\")){return \nhe(e,t,null,null,{targetOverride:g(r),returnPromise:true})}else{return he(e,t,g(r.source),r.event,{handler:r.handler,headers:r.headers,values:r.values,targetOverride:g(r.target),swapOverride:r.swap,select:r.select,returnPromise:true})}}else{return he(e,t,null,null,{returnPromise:true})}}function Ir(e){var \nt=[];while(e){t.push(e);e=e.parentElement}return t}function kr(e,t,r){var n;var i;if(typeof URL===\"function\"){i=new URL(t,document.location.href);var a=document.location.origin;n=a===i.origin}else{i=t;n=s(t,document.location.origin)}if(Q.config.selfRequestsOnly){if(!n){return false}}return \nce(e,\"htmx:validateUrl\",le({url:i,sameHost:n},r))}function he(t,r,n,i,a,e){var o=null;var s=null;a=a!=null?a:{};if(a.returnPromise&&typeof Promise!==\"undefined\"){var l=new Promise(function(e,t){o=e;s=t})}if(n==null){n=re().body}var M=a.handler||Mr;var X=a.select||null;if(!se(n)){ie(o);return l}var \nu=a.targetOverride||ye(n);if(u==null||u==me){fe(n,\"htmx:targetError\",{target:te(n,\"hx-target\")});ie(s);return l}var f=ae(n);var c=f.lastButtonClicked;if(c){var h=ee(c,\"formaction\");if(h!=null){r=h}var v=ee(c,\"formmethod\");if(v!=null){if(v.toLowerCase()!==\"dialog\"){t=v}}}var d=ne(n,\"hx-confirm\");if(e===undefined){var \nD=function(e){return he(t,r,n,i,a,!!e)};var U={target:u,elt:n,path:r,verb:t,triggeringEvent:i,etc:a,issueRequest:D,question:d};if(ce(n,\"htmx:confirm\",U)===false){ie(o);return l}}var g=n;var m=ne(n,\"hx-sync\");var p=null;var x=false;if(m){var B=m.split(\":\");var \nF=B[0].trim();if(F===\"this\"){g=xe(n,\"hx-sync\")}else{g=ue(n,F)}m=(B[1]||\"drop\").trim();f=ae(g);if(m===\"drop\"&&f.xhr&&f.abortable!==true){ie(o);return l}else if(m===\"abort\"){if(f.xhr){ie(o);return l}else{x=true}}else if(m===\"replace\"){ce(g,\"htmx:abort\")}else if(m.indexOf(\"queue\")===0){var V=m.split(\" \n\");p=(V[1]||\"last\").trim()}}if(f.xhr){if(f.abortable){ce(g,\"htmx:abort\")}else{if(p==null){if(i){var \ny=ae(i);if(y&&y.triggerSpec&&y.triggerSpec.queue){p=y.triggerSpec.queue}}if(p==null){p=\"last\"}}if(f.queuedRequests==null){f.queuedRequests=[]}if(p===\"first\"&&f.queuedRequests.length===0){f.queuedRequests.push(function(){he(t,r,n,i,a)})}else if(p===\"all\"){f.queuedRequests.push(function(){he(t,r,n,i,a)})}else \nif(p===\"last\"){f.queuedRequests=[];f.queuedRequests.push(function(){he(t,r,n,i,a)})}ie(o);return l}}var b=new XMLHttpRequest;f.xhr=b;f.abortable=x;var w=function(){f.xhr=null;f.abortable=false;if(f.queuedRequests!=null&&f.queuedRequests.length>0){var e=f.queuedRequests.shift();e()}};var j=ne(n,\"hx-prompt\");if(j){var \nS=prompt(j);if(S===null||!ce(n,\"htmx:prompt\",{prompt:S,target:u})){ie(o);w();return l}}if(d&&!e){if(!confirm(d)){ie(o);w();return l}}var E=xr(n,u,S);if(t!==\"get\"&&!Sr(n)){E[\"Content-Type\"]=\"application/x-www-form-urlencoded\"}if(a.headers){E=le(E,a.headers)}var _=dr(n,t);var C=_.errors;var \nR=_.values;if(a.values){R=le(R,a.values)}var z=Hr(n);var $=le(R,z);var T=yr($,n);if(Q.config.getCacheBusterParam&&t===\"get\"){T[\"org.htmx.cache-buster\"]=ee(u,\"id\")||\"true\"}if(r==null||r===\"\"){r=re().location.href}var O=Rr(n,\"hx-request\");var W=ae(n).boosted;var q=Q.config.methodsThatUseUrlParams.indexOf(t)>=0;var \nH={boosted:W,useUrlParams:q,parameters:T,unfilteredParameters:$,headers:E,target:u,verb:t,errors:C,withCredentials:a.credentials||O.credentials||Q.config.withCredentials,timeout:a.timeout||O.timeout||Q.config.timeout,path:r,triggeringEvent:i};if(!ce(n,\"htmx:configRequest\",H)){ie(o);w();return \nl}r=H.path;t=H.verb;E=H.headers;T=H.parameters;C=H.errors;q=H.useUrlParams;if(C&&C.length>0){ce(n,\"htmx:validation:halted\",H);ie(o);w();return l}var G=r.split(\"#\");var J=G[0];var L=G[1];var A=r;if(q){A=J;var \nZ=Object.keys(T).length!==0;if(Z){if(A.indexOf(\"?\")<0){A+=\"?\"}else{A+=\"&\"}A+=mr(T);if(L){A+=\"#\"+L}}}if(!kr(n,A,H)){fe(n,\"htmx:invalidPath\",H);ie(s);return l}b.open(t.toUpperCase(),A,true);b.overrideMimeType(\"text/html\");b.withCredentials=H.withCredentials;b.timeout=H.timeout;if(O.noHeaders){}else{for(var N in \nE){if(E.hasOwnProperty(N)){var K=E[N];Lr(b,N,K)}}}var I={xhr:b,target:u,requestConfig:H,etc:a,boosted:W,select:X,pathInfo:{requestPath:r,finalRequestPath:A,anchor:L}};b.onload=function(){try{var e=Ir(n);I.pathInfo.responsePath=Ar(b);M(n,I);lr(k,P);ce(n,\"htmx:afterRequest\",I);ce(n,\"htmx:afterOnLoad\",I);if(!se(n)){var \nt=null;while(e.length>0&&t==null){var r=e.shift();if(se(r)){t=r}}if(t){ce(t,\"htmx:afterRequest\",I);ce(t,\"htmx:afterOnLoad\",I)}}ie(o);w()}catch(e){fe(n,\"htmx:onLoadError\",le({error:e},I));throw \ne}};b.onerror=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:sendError\",I);ie(s);w()};b.onabort=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:sendAbort\",I);ie(s);w()};b.ontimeout=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:timeout\",I);ie(s);w()};if(!ce(n,\"htmx:beforeRequest\",I)){ie\n(o);w();return l}var k=or(n);var P=sr(n);oe([\"loadstart\",\"loadend\",\"progress\",\"abort\"],function(t){oe([b,b.upload],function(e){e.addEventListener(t,function(e){ce(n,\"htmx:xhr:\"+t,{lengthComputable:e.lengthComputable,loaded:e.loaded,total:e.total})})})});ce(n,\"htmx:beforeSend\",I);var Y=q?null:Er(b,n,T);b.send(Y);return \nl}function Pr(e,t){var r=t.xhr;var n=null;var i=null;if(O(r,/HX-Push:/i)){n=r.getResponseHeader(\"HX-Push\");i=\"push\"}else if(O(r,/HX-Push-Url:/i)){n=r.getResponseHeader(\"HX-Push-Url\");i=\"push\"}else \nif(O(r,/HX-Replace-Url:/i)){n=r.getResponseHeader(\"HX-Replace-Url\");i=\"replace\"}if(n){if(n===\"false\"){return{}}else{return{type:i,path:n}}}var a=t.pathInfo.finalRequestPath;var o=t.pathInfo.responsePath;var s=ne(e,\"hx-push-url\");var l=ne(e,\"hx-replace-url\");var u=ae(e).boosted;var f=null;var \nc=null;if(s){f=\"push\";c=s}else if(l){f=\"replace\";c=l}else if(u){f=\"push\";c=o||a}if(c){if(c===\"false\"){return{}}if(c===\"true\"){c=o||a}if(t.pathInfo.anchor&&c.indexOf(\"#\")===-1){c=c+\"#\"+t.pathInfo.anchor}return{type:f,path:c}}else{return{}}}function Mr(l,u){var f=u.xhr;var c=u.target;var e=u.etc;var \nt=u.requestConfig;var h=u.select;if(!ce(l,\"htmx:beforeOnLoad\",u))return;if(O(f,/HX-Trigger:/i)){_e(f,\"HX-Trigger\",l)}if(O(f,/HX-Location:/i)){er();var r=f.getResponseHeader(\"HX-Location\");var v;if(r.indexOf(\"{\")===0){v=E(r);r=v[\"path\"];delete v[\"path\"]}Nr(\"GET\",r,v).then(function(){tr(r)});return}var \nn=O(f,/HX-Refresh:/i)&&\"true\"===f.getResponseHeader(\"HX-Refresh\");if(O(f,/HX-Redirect:/i)){location.href=f.getResponseHeader(\"HX-Redirect\");n&&location.reload();return}if(n){location.reload();return}if(O(f,/HX-Retarget:/i)){if(f.getResponseHeader(\"HX-Retarget\")===\"this\"){u.target=l}else{u.target=ue(l,f.getResponseHead\ner(\"HX-Retarget\"))}}var d=Pr(l,u);var i=f.status>=200&&f.status<400&&f.status!==204;var g=f.response;var a=f.status>=400;var m=Q.config.ignoreTitle;var \no=le({shouldSwap:i,serverResponse:g,isError:a,ignoreTitle:m},u);if(!ce(c,\"htmx:beforeSwap\",o))return;c=o.target;g=o.serverResponse;a=o.isError;m=o.ignoreTitle;u.target=c;u.failed=a;u.successful=!a;if(o.shouldSwap){if(f.status===286){at(l)}R(l,function(e){g=e.transformResponse(g,f,l)});if(d.type){er()}var \ns=e.swapOverride;if(O(f,/HX-Reswap:/i)){s=f.getResponseHeader(\"HX-Reswap\")}var v=wr(l,s);if(v.hasOwnProperty(\"ignoreTitle\")){m=v.ignoreTitle}c.classList.add(Q.config.swappingClass);var p=null;var x=null;var y=function(){try{var e=document.activeElement;var \nt={};try{t={elt:e,start:e?e.selectionStart:null,end:e?e.selectionEnd:null}}catch(e){}var \nr;if(h){r=h}if(O(f,/HX-Reselect:/i)){r=f.getResponseHeader(\"HX-Reselect\")}if(d.type){ce(re().body,\"htmx:beforeHistoryUpdate\",le({history:d},u));if(d.type===\"push\"){tr(d.path);ce(re().body,\"htmx:pushedIntoHistory\",{path:d.path})}else{rr(d.path);ce(re().body,\"htmx:replacedInHistory\",{path:d.path})}}var \nn=T(c);je(v.swapStyle,c,l,g,n,r);if(t.elt&&!se(t.elt)&&ee(t.elt,\"id\")){var i=document.getElementById(ee(t.elt,\"id\"));var \na={preventScroll:v.focusScroll!==undefined?!v.focusScroll:!Q.config.defaultFocusScroll};if(i){if(t.start&&i.setSelectionRange){try{i.setSelectionRange(t.start,t.end)}catch(e){}}i.focus(a)}}c.classList.remove(Q.config.swappingClass);oe(n.elts,function(e){if(e.classList){e.classList.add(Q.config.settlingClass)}ce(e,\"htm\nx:afterSwap\",u)});if(O(f,/HX-Trigger-After-Swap:/i)){var o=l;if(!se(l)){o=re().body}_e(f,\"HX-Trigger-After-Swap\",o)}var s=function(){oe(n.tasks,function(e){e.call()});oe(n.elts,function(e){if(e.classList){e.classList.remove(Q.config.settlingClass)}ce(e,\"htmx:afterSettle\",u)});if(u.pathInfo.anchor){var \ne=re().getElementById(u.pathInfo.anchor);if(e){e.scrollIntoView({block:\"start\",behavior:\"auto\"})}}if(n.title&&!m){var t=C(\"title\");if(t){t.innerHTML=n.title}else{window.document.title=n.title}}Cr(n.elts,v);if(O(f,/HX-Trigger-After-Settle:/i)){var \nr=l;if(!se(l)){r=re().body}_e(f,\"HX-Trigger-After-Settle\",r)}ie(p)};if(v.settleDelay>0){setTimeout(s,v.settleDelay)}else{s()}}catch(e){fe(l,\"htmx:swapError\",u);ie(x);throw e}};var b=Q.config.globalViewTransitions;if(v.hasOwnProperty(\"transition\")){b=v.transition}if(b&&ce(l,\"htmx:beforeTransition\",u)&&typeof \nPromise!==\"undefined\"&&document.startViewTransition){var w=new Promise(function(e,t){p=e;x=t});var S=y;y=function(){document.startViewTransition(function(){S();return w})}}if(v.swapDelay>0){setTimeout(y,v.swapDelay)}else{y()}}if(a){fe(l,\"htmx:responseError\",le({error:\"Response Status Error Code \"+f.status+\" from \n\"+u.pathInfo.requestPath},u))}}var Xr={};function Dr(){return{init:function(e){return null},onEvent:function(e,t){return true},transformResponse:function(e,t,r){return e},isInlineSwap:function(e){return false},handleSwap:function(e,t,r,n){return false},encodeParameters:function(e,t,r){return null}}}function \nUr(e,t){if(t.init){t.init(r)}Xr[e]=le(Dr(),t)}function Br(e){delete Xr[e]}function Fr(e,r,n){if(e==undefined){return r}if(r==undefined){r=[]}if(n==undefined){n=[]}var t=te(e,\"hx-ext\");if(t){oe(t.split(\",\"),function(e){e=e.replace(/ /g,\"\");if(e.slice(0,7)==\"ignore:\"){n.push(e.slice(7));return}if(n.indexOf(e)<0){var \nt=Xr[e];if(t&&r.indexOf(t)<0){r.push(t)}}})}return Fr(u(e),r,n)}var Vr=false;re().addEventListener(\"DOMContentLoaded\",function(){Vr=true});function jr(e){if(Vr||re().readyState===\"complete\"){e()}else{re().addEventListener(\"DOMContentLoaded\",e)}}function \n_r(){if(Q.config.includeIndicatorStyles!==false){re().head.insertAdjacentHTML(\"beforeend\",\"<style>                      .\"+Q.config.indicatorClass+\"{opacity:0}                      .\"+Q.config.requestClass+\" .\"+Q.config.indicatorClass+\"{opacity:1; transition: opacity 200ms ease-in;}                      \n.\"+Q.config.requestClass+\".\"+Q.config.indicatorClass+\"{opacity:1; transition: opacity 200ms ease-in;}                    </style>\")}}function zr(){var e=re().querySelector('meta[name=\"htmx-config\"]');if(e){return E(e.content)}else{return null}}function $r(){var \ne=zr();if(e){Q.config=le(Q.config,e)}}jr(function(){$r();_r();var e=re().body;zt(e);var t=re().querySelectorAll(\"[hx-trigger='restored'],[data-hx-trigger='restored']\");e.addEventListener(\"htmx:abort\",function(e){var t=e.target;var r=ae(t);if(r&&r.xhr){r.xhr.abort()}});const \nr=window.onpopstate?window.onpopstate.bind(window):null;window.onpopstate=function(e){if(e.state&&e.state.htmx){ar();oe(t,function(e){ce(e,\"htmx:restored\",{document:re(),triggerEvent:ce})})}else{if(r){r(e)}}};setTimeout(function(){ce(e,\"htmx:load\",{});e=null},0)});return Q}()}); \n \nFix:  \n1.\u200b The following buttons do not work on panel 2. Generate Study Material: \na.\u200b Summary \nb.\u200b Key Terms \nc.\u200b Practice Questions \nd.\u200b External Resources \n \n",
        "added_at": "2025-11-30T00:59:51.683939"
      }
    ]
  },
  "77f5793c-1064-4cf8-9888-10aaf0a6f6a0": {
    "created_at": "2025-11-30T01:09:19.366837",
    "files": [
      {
        "name": "326 Proj.pdf",
        "type": "document",
        "text": "Python\ngit add .  \ngit commit -m \u201c   \u201c \ngit push -u origin main  \n \n \nmain.py:  \n \nimport os \nimport json \nimport uuid \nimport subprocess \nfrom datetime import datetime \nfrom typing import Dict, Any \n \nimport fitz  # PyMuPDF \nfrom pptx import Presentation \nfrom fastapi import FastAPI, Request, UploadFile, File, Form \nfrom fastapi.responses import HTMLResponse, PlainTextResponse \nfrom fastapi.staticfiles import StaticFiles \nfrom fastapi.templating import Jinja2Templates \nimport google.generativeai as genai \n \n# Ai model  \ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\")) \nGEM_MODEL = genai.GenerativeModel(\"gemini-1.5-flash\") \n \n# ----------------- CONFIG ----------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \nUPLOAD_DIR = os.path.join(BASE_DIR, \"upload\") \nAUDIO_DIR = os.path.join(UPLOAD_DIR, \"audio\") \nDOC_DIR = os.path.join(UPLOAD_DIR, \"docs\") \nSESSIONS_PATH = os.path.join(BASE_DIR, \"sessions.json\") \n \nos.makedirs(UPLOAD_DIR, exist_ok=True) \nos.makedirs(AUDIO_DIR, exist_ok=True) \nos.makedirs(DOC_DIR, exist_ok=True) \n \n#client = (api_key=os.getenv(\"GEMINI_API_KEY\")) \n \napp = FastAPI(title=\"Helektron Study Assistant\") \napp.mount(\"/static\", StaticFiles(directory=os.path.join(BASE_DIR, \"static\")), \nname=\"static\") \ntemplates = Jinja2Templates(directory=os.path.join(BASE_DIR, \"templates\")) \n \n\n# ----------------- SESSION STORAGE ----------------- \ndef load_sessions() -> Dict[str, Any]: \n    if not os.path.exists(SESSIONS_PATH): \n        return {} \n    with open(SESSIONS_PATH, \"r\") as f: \n        try: \n            return json.load(f) \n        except json.JSONDecodeError: \n            return {} \n \ndef save_sessions(sessions: Dict[str, Any]) -> None: \n    with open(SESSIONS_PATH, \"w\") as f: \n        json.dump(sessions, f, indent=2) \n \nsessions: Dict[str, Any] = load_sessions() \n \ndef get_or_create_session(session_id: str | None) -> str: \n    global sessions \n    if session_id and session_id in sessions: \n        return session_id \n    new_id = str(uuid.uuid4()) \n    sessions[new_id] = { \n        \"created_at\": datetime.utcnow().isoformat(), \n        \"files\": [],   # list of {\"name\",\"type\",\"text\"} \n    } \n    save_sessions(sessions) \n    return new_id \n \ndef add_text_to_session(session_id: str, filename: str, file_type: str, text: \nstr) -> None: \n    global sessions \n    if session_id not in sessions: \n        session_id = get_or_create_session(None) \n    sessions[session_id][\"files\"].append({ \n        \"name\": filename, \n        \"type\": file_type, \n        \"text\": text, \n        \"added_at\": datetime.utcnow().isoformat() \n    }) \n    save_sessions(sessions) \n \ndef get_session_text(session_id: str) -> str: \n    session = sessions.get(session_id) \n    if not session: \n\n        return \"\" \n    blocks = [f\"--- {f['name']} ({f['type']}) ---\\n{f['text']}\" for f in \nsession[\"files\"] if f.get(\"text\")] \n    return \"\\n\\n\".join(blocks) \n \n# ----------------- HELPERS: FILE TEXT EXTRACTION ----------------- \ndef extract_txt(path: str) -> str: \n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        return f.read() \n \ndef extract_pdf(path: str) -> str: \n    doc = fitz.open(path) \n    texts = [] \n    for page in doc: \n        texts.append(page.get_text()) \n    doc.close() \n    return \"\\n\".join(texts) \n \ndef extract_pptx(path: str) -> str: \n    prs = Presentation(path) \n    texts = [] \n    for slide in prs.slides: \n        for shape in slide.shapes: \n            if hasattr(shape, \"text\"): \n                texts.append(shape.text) \n    return \"\\n\".join(texts) \n \ndef transcribe_audio(path: str) -> str: \n    # Find whisper binary \n    candidates = [ \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"build\", \"bin\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper-main\", \"build\", \"bin\", \"whisper-cli\"), \n    ] \n    whisper_bin = next((c for c in candidates if os.path.exists(c)), None) \n    if not whisper_bin: \n        raise RuntimeError(\"Whisper binary not found. Build whisper.cpp \nfirst.\") \n \n    wav_path = path + \".wav\" \n    subprocess.run( \n        [\"ffmpeg\", \"-y\", \"-i\", path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \n\"-ac\", \"1\", wav_path], \n        check=True \n\n    ) \n \n    out_prefix = path + \"_out\" \n    subprocess.run( \n        [whisper_bin, \"-m\", os.path.join(BASE_DIR, \"whisper.cpp\", \"models\", \n\"ggml-base.en.bin\"), \n         \"-f\", wav_path, \"--output-txt\", \"--output-file\", out_prefix], \n        check=True \n    ) \n \n    txt_path = out_prefix + \".txt\" \n    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        transcript = f.read() \n \n    # cleanup \n    for p in [wav_path, txt_path]: \n        if os.path.exists(p): \n            os.remove(p) \n \n    return transcript \n \n# ----------------- GPT PROMPTS ----------------- \ndef get_summary_prompt(transcript: str) -> str: \n    return f\"\"\" \nBased on the following combined study materials (lectures, slides, PDFs, notes, \ntranscripts), \ncreate a **structured, detailed, and academically accurate study summary**. \n \nOrganize the output into the following clearly labeled sections: \n \n- **Overview**: Briefly describe the overall topic and goals. \n- **Key Concepts Introduced**: List and explain major ideas, theories, \nformulas, or processes. \n- **Detailed Topic Breakdown**: Group related ideas and summarize explanations, \nreasoning steps, and relationships. \n- **Important Definitions**: Provide concise definitions for technical terms or \ndomain-specific vocabulary. \n- **Examples or Applications**: Summarize any examples, demonstrations, or \nreal-world applications. \n- **Main Takeaways**: What a student should remember after studying this \nmaterial. \n \nExpectations: \n- Use clear, concise bullet points. \n\n- Prioritize conceptual correctness. \n- Use simple academic language suitable for undergraduate study. \n- If information is unclear or missing, label it as `TBD`. \n \n---BEGIN MATERIAL--- \n{transcript} \n---END MATERIAL--- \n\"\"\" \n \ndef get_keyterms_prompt(text: str) -> str: \n    return f\"\"\" \nFrom the combined study materials below, extract **10\u201320 key terms** with short \ndefinitions. \n \nFor each term, include: \n- The term \n- A 1\u20132 sentence definition \n- (Optional) A quick note on why it matters in this context. \n \nFormat as a bulleted list. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_questions_prompt(text: str) -> str: \n    return f\"\"\" \nCreate **8\u201312 practice questions** based on the combined study materials below. \n \nInclude a mix of: \n- Conceptual understanding questions \n- Short-answer questions \n- Application/problem-style questions (where possible) \n \nDo NOT provide answers, only questions. \nGroup them into sections if appropriate (e.g., 'Conceptual', 'Application'). \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_resources_prompt(text: str) -> str: \n\n    return f\"\"\" \nBased on the topics and concepts in the combined study materials below, \nrecommend **3\u20137 high-quality external resources** for further study. \n \nEach resource should include: \n- Title \n- Type (e.g., textbook chapter, .edu article, video lecture) \n- Source (e.g., university, well-known platform) \n- 1\u20132 sentences on why it is relevant. \n \nPrefer: \n- .edu domains \n- Reputable textbooks \n- Well-known educational channels \n \nYou do NOT need to provide URLs, just clear, identifiable references. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef call_gemini(prompt: str) -> str: \n    try: \n        response = GEM_MODEL.generate_content(prompt) \n        return response.text.strip() \n    except Exception as e: \n        return f\"[Gemini API Error: {e}]\" \n \n# ----------------- ROUTES ----------------- \n@app.get(\"/\", response_class=HTMLResponse) \ndef index(request: Request): \n    return templates.TemplateResponse(\"index.html\", {\"request\": request}) \n \n# Upload any material (txt, pdf, pptx, mp4, m4a) \n@app.post(\"/upload\", response_class=HTMLResponse) \nasync def upload_material( \n    request: Request, \n    file: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    global sessions \n \n    session_id = get_or_create_session(session_id) \n\n    filename = file.filename \n    ext = filename.split(\".\")[-1].lower() \n \n    # Save file \n    if ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n        save_dir = AUDIO_DIR \n        kind = \"audio\" \n    else: \n        save_dir = DOC_DIR \n        kind = \"document\" \n \n    os.makedirs(save_dir, exist_ok=True) \n    saved_path = os.path.join(save_dir, f\"{uuid.uuid4().hex}_{filename}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await file.read()) \n \n    # Extract text \n    try: \n        if ext == \"txt\": \n            text = extract_txt(saved_path) \n        elif ext == \"pdf\": \n            text = extract_pdf(saved_path) \n        elif ext in [\"pptx\"]: \n            text = extract_pptx(saved_path) \n        elif ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n            text = transcribe_audio(saved_path) \n        else: \n            text = f\"[Unsupported file type: {ext}]\" \n    except Exception as e: \n        text = f\"[Error processing file {filename}: {e}]\" \n \n    add_text_to_session(session_id, filename, kind, text) \n \n    # Render updated materials list (fragment) \n    session = sessions[session_id] \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n\n \n# Live transcription: audio blob from the browser \n@app.post(\"/upload_live_audio\", response_class=HTMLResponse) \nasync def upload_live_audio( \n    request: Request, \n    audio: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    session_id = get_or_create_session(session_id) \n    filename = audio.filename or \"live_recording.webm\" \n    ext = filename.split(\".\")[-1].lower() \n \n    os.makedirs(AUDIO_DIR, exist_ok=True) \n    saved_path = os.path.join(AUDIO_DIR, f\"live_{uuid.uuid4().hex}.{ext}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await audio.read()) \n \n    try: \n        text = transcribe_audio(saved_path) \n    except Exception as e: \n        text = f\"[Error transcribing live audio: {e}]\" \n \n    add_text_to_session(session_id, filename, \"audio-live\", text) \n    session = sessions[session_id] \n \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# ---- Study tools (summary, key terms, questions, resources) ---- \n \n@app.get(\"/summary/{session_id}\", response_class=PlainTextResponse) \ndef generate_summary(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_summary_prompt(text) \n\nPython\n        content = call_gemini(prompt) \n    # Return plain text directly to HTMX target \n    return content \n \n \n@app.get(\"/keyterms/{session_id}\", response_class=PlainTextResponse) \ndef generate_keyterms(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_keyterms_prompt(text) \n        content = call_gemini(prompt) \n    return content \n \n \n@app.get(\"/questions/{session_id}\", response_class=PlainTextResponse) \ndef generate_questions_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_questions_prompt(text) \n        content = call_gemini(prompt) \n    return content \n \n \n@app.get(\"/resources/{session_id}\", response_class=PlainTextResponse) \ndef generate_resources_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_resources_prompt(text) \n        content = call_gemini(prompt) \n    return content \nRag Folder:  \n1.\u200b build_vector_store.py  \n \nimport os \nimport json \n\nimport faiss \nfrom langchain_community.vectorstores import FAISS \nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain_core.documents import Document \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter \n \n# -------------------------------------------- \n# Correct project paths \n# -------------------------------------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \n \n# Uploaded document text is stored here by your FastAPI app \nDOC_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"docs\") \n \n# Vector store output directory \nVS_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"vs\") \n \nos.makedirs(VS_DIR, exist_ok=True) \n \n \n# -------------------------------------------- \n# Load documents from upload/docs/ \n# -------------------------------------------- \ndef load_documents(): \n    docs = [] \n    if not os.path.exists(DOC_DIR): \n        print(f\"Missing directory {DOC_DIR}. No documents found.\") \n        return docs \n \n    for filename in os.listdir(DOC_DIR): \n        path = os.path.join(DOC_DIR, filename) \n        if not os.path.isfile(path): \n            continue \n \n        try: \n            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n                text = f.read().strip() \n                if text: \n                    docs.append(Document(page_content=text, metadata={\"source\": \nfilename})) \n        except Exception as e: \n            print(f\"Error loading {filename}: {e}\") \n \n    return docs \n\n \n \n# -------------------------------------------- \n# Build vector store \n# -------------------------------------------- \ndef build_vector_store(): \n    print(f\"Building vector store from {DOC_DIR}...\") \n \n    docs = load_documents() \n \n    if not docs: \n        print(\"No documents found in upload/docs/. Upload files in the web app \nfirst.\") \n        return \n \n    print(f\"Loaded {len(docs)} raw documents.\") \n \n    # ------------------------------- \n    # Chunk documents \n    # ------------------------------- \n    splitter = RecursiveCharacterTextSplitter( \n        chunk_size=1000, # ~750 tokens \n        chunk_overlap=200, # contextual overlap \n        length_function=len \n    ) \n \n    split_docs = [] \n    for doc in docs: \n        chunks = splitter.split_text(doc.page_content) \n        for chunk in chunks: \n            split_docs.append( \n                Document( \n                    page_content=chunk, \n                    metadata=doc.metadata  # keep source info \n                ) \n            ) \n \n    print(f\"Created {len(split_docs)} total chunks.\") \n \n    # ------------------------------- \n    # Generate embeddings \n    # ------------------------------- \n    embeddings = HuggingFaceEmbeddings() \n \n\nPython\n    print(\"Generating embeddings...\") \n    vectorstore = FAISS.from_documents(split_docs, embeddings) \n \n    # ------------------------------- \n    # Save FAISS index + metadata \n    # ------------------------------- \n    print(\"Saving FAISS index and metadata...\") \n \n    faiss.write_index(vectorstore.index, os.path.join(VS_DIR, \"vs.faiss\")) \n \n    with open(os.path.join(VS_DIR, \"docs.json\"), \"w\") as f: \n        json.dump([d.dict() for d in split_docs], f, indent=2) \n \n    print(\"\\n\ud83c\udf89 Vector store built successfully!\") \n    print(f\"Saved to: {VS_DIR}\") \n \n \nif __name__ == \"__main__\": \n    build_vector_store() \n \n2.\u200b rag_utils.py \n \nimport os \nimport json \nimport numpy as np \nfrom google import genai \n \n# Load Gemini client \nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\")) \n \n \n# ----------------------------- \n# Helper: Load vector store \n# ----------------------------- \ndef load_store(path): \n    if not os.path.exists(path): \n        return [] \n    with open(path, \"r\") as f: \n        return json.load(f) \n \n \n\n# ----------------------------- \n# Helper: Save vector store \n# ----------------------------- \ndef save_store(path, data): \n    with open(path, \"w\") as f: \n        json.dump(data, f, indent=4) \n \n \n# ----------------------------- \n# Embed text using Gemini \n# ----------------------------- \ndef embed_text(text: str): \n    \"\"\" \n    Uses Gemini embeddings. \n    Returns list[float]. \n    \"\"\" \n \n    result = client.models.embed_content( \n        model=\"text-embedding-004\", \n        contents=text, \n    ) \n \n    # Gemini wraps embeddings inside .output_embedding \n    return result.output_embedding \n \n \n# ----------------------------- \n# Add text to class vector store \n# ----------------------------- \ndef add_to_vector_store(class_id: str, text: str): \n    \"\"\" \n    Creates or appends to the vector store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    store = load_store(store_path) \n \n    embedding = embed_text(text) \n \n    store.append({ \n        \"text\": text, \n        \"embedding\": embedding, \n\n    }) \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Build store from scratch (optional) \n# ----------------------------- \ndef build_vector_store(class_id: str, text: str): \n    \"\"\" \n    Clears and rebuilds the entire store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    embedding = embed_text(text) \n \n    store = [ \n        { \n            \"text\": text, \n            \"embedding\": embedding, \n        } \n    ] \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Search RAG store using cosine similarity \n# ----------------------------- \ndef search_vector_store(class_id: str, query: str): \n    \"\"\" \n    Returns the most relevant text chunk for the given query. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    store = load_store(store_path) \n \n    if not store: \n        return None \n \n    query_embedding = embed_text(query) \n    query_vec = np.array(query_embedding) \n\nHTML\n \n    best_score = -999 \n    best_text = None \n \n    for item in store: \n        vec = np.array(item[\"embedding\"]) \n \n        # cosine similarity \n        score = np.dot(query_vec, vec) / ( \n            np.linalg.norm(query_vec) * np.linalg.norm(vec) \n        ) \n \n        if score > best_score: \n            best_score = score \n            best_text = item[\"text\"] \n \n    return best_text \n \n \nTemplates \n1.\u200b templates/index.html  \n \n{% extends \"base.html\" %} \n{% block content %} \n \n<h1>Helektron Study Assistant</h1> \n<p class=\"subtitle\"> \n    Upload lecture materials (PDF, slides, audio, notes), then generate \nsummaries, key terms, questions, and more. \n</p> \n \n<div class=\"layout\"> \n \n    <!-- LEFT PANEL --> \n    <div class=\"card\"> \n        <h2>1. Upload Materials</h2> \n \n        <div class=\"upload-box\"> \n            <p><strong>Upload files:</strong> .txt, .pdf, .pptx, .mp4, .m4a</p> \n \n            <form \n\n                hx-post=\"/upload\" \n                hx-target=\"#materials-panel\" \n                hx-swap=\"innerHTML\" \n                hx-indicator=\"#upload-loading\" \n                enctype=\"multipart/form-data\"> \n \n                <input type=\"file\" name=\"file\" required> \n                <input type=\"hidden\" name=\"session_id\" id=\"session_id_input\"> \n                <br><br> \n                <button class=\"primary\" type=\"submit\">Upload</button> \n            </form> \n        </div> \n \n        <div class=\"upload-box\"> \n            <p><strong>Live Transcription:</strong> record audio and transcribe \nwith Whisper.</p> \n            <button class=\"secondary\" id=\"record-btn\">\ud83c\udf99 Start \nRecording</button> \n            <p id=\"record-status\" style=\"font-size: 12px; color:#555; \nmargin-top:6px;\"></p> \n        </div> \n \n        <div id=\"upload-loading\" class=\"htmx-indicator\"> \n            Uploading/processing material... \n        </div> \n \n        <!-- \ud83d\udd25 THIS GETS UPDATED BY upload_status.html --> \n        <div id=\"materials-panel\"> \n            <em>No materials uploaded yet.</em> \n        </div> \n \n    </div> \n \n    <!-- RIGHT PANEL --> \n    <div class=\"card\"> \n        <h2>2. Generate Study Material</h2> \n \n        <p style=\"font-size:13px; color:#555;\">Choose what you want the AI to \ngenerate.</p> \n \n        <div style=\"display:flex; gap:8px; flex-wrap:wrap; \nmargin-bottom:10px;\"> \n            <button class=\"primary\" \n                    id=\"btn-summary\" \n\n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Summary \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-keyterms\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Key Terms \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-questions\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Practice Questions \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-resources\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                External Resources \n            </button> \n        </div> \n \n        <div id=\"generate-loading\" class=\"htmx-indicator\"> \n            Generating material... \n        </div> \n \n        <div id=\"results-panel\"> \n            <em>Your generated study content will appear here.</em> \n        </div> \n \n    </div> \n\nHTML\nHTML\n</div> \n \n<script src=\"/static/app.js\"></script> \n \n{% endblock %} \n \n2.\u200b templates/base.html  \n \n<!DOCTYPE html> \n<html lang=\"en\"> \n<head> \n    <meta charset=\"UTF-8\"> \n    <title>Helektron Study Assistant</title> \n    <link rel=\"stylesheet\" href=\"/static/styles.css\"> \n</head> \n \n<body> \n<div class=\"app-shell\"> \n    {% block content %}{% endblock %} \n</div> \n \n<script src=\"https://unpkg.com/htmx.org@1.9.10\"></script> \n</body> \n</html> \n \n3.\u200b Fragments folder \na.\u200b templates/fragments/class_list.html  \n \n<ul> \n    {% for cls in classes %} \n    <li> \n        <button  \n            hx-post=\"/set_active_class\"  \n            hx-vals='{\"class_id\": \"{{ cls.id }}\"}' \n            hx-target=\"#upload-list\" \n            hx-swap=\"innerHTML\" \n            class=\"{% if cls.id == active_class_id %}active{% endif %}\" \n\nHTML\nHTML\nHTML\n        > \n            {{ cls.name }} \n        </button> \n    </li> \n    {% endfor %} \n</ul> \n \nb.\u200b templates/fragments/upload_status.html  \n \n{% if session_id %} \n<div data-session-id=\"{{ session_id }}\"></div> \n{% endif %} \n \n{% if files %} \n<ul> \n    {% for f in files %} \n    <li><strong>{{ f.name }}</strong> ({{ f.type }})</li> \n    {% endfor %} \n</ul> \n{% else %} \n<em>No materials uploaded yet.</em> \n{% endif %} \n \nc.\u200b templates/fragments/summary.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n \nd.\u200b templates/fragments/resources.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n\nHTML\nHTML\nCSS\ne.\u200b templates/fragments/questions.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \nf.\u200b\ntemplates/fragments/keyterms.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n \n \nStatic:  \n1.\u200b static/styles.css \n \nbody { \n    margin: 0; \n    font-family: system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", \nsans-serif; \n    background: #f4f4fb; \n} \n \n.app-shell { \n    max-width: 1100px; \n    margin: 0 auto; \n    padding: 24px; \n} \n \nh1 { \n    font-size: 28px; \n    margin-bottom: 8px; \n} \n \n.subtitle { \n    color: #555; \n    margin-bottom: 24px; \n} \n\n \n.layout { \n    display: grid; \n    grid-template-columns: 1.2fr 1.8fr; \n    gap: 20px; \n} \n \n.card { \n    background: white; \n    border-radius: 12px; \n    padding: 16px 20px; \n    box-shadow: 0 1px 3px rgba(15,23,42,0.08); \n    border: 1px solid #e2e8f0; \n} \n \n.card h2 { \n    font-size: 18px; \n    margin-top: 0; \n    margin-bottom: 12px; \n} \n \n.upload-box { \n    border: 2px dashed #cbd5f5; \n    border-radius: 10px; \n    padding: 16px; \n    text-align: center; \n    background: #f9fafb; \n    margin-bottom: 12px; \n} \n \n.upload-box input[type=\"file\"] { \n    margin-top: 8px; \n} \n \nbutton.primary { \n    background: #4f46e5; \n    color: white; \n    border: none; \n    border-radius: 8px; \n    padding: 10px 16px; \n    font-weight: 600; \n    cursor: pointer; \n    font-size: 14px; \n} \n\n \nbutton.primary:disabled { \n    opacity: 0.6; \n    cursor: not-allowed; \n} \n \nbutton.primary:hover:not(:disabled) { \n    background: #4338ca; \n} \n \nbutton.secondary { \n    background: white; \n    color: #4f46e5; \n    border: 1px solid #cbd5f5; \n    border-radius: 8px; \n    padding: 8px 14px; \n    font-weight: 500; \n    cursor: pointer; \n    font-size: 13px; \n} \n \nbutton.secondary:hover { \n    background: #eef2ff; \n} \n \n#materials-list { \n    font-size: 14px; \n    max-height: 200px; \n    overflow-y: auto; \n} \n \n#materials-list ul { \n    padding-left: 18px; \n} \n \n#results-panel { \n    min-height: 250px; \n    font-size: 14px; \n    white-space: pre-wrap; \n} \n \n/* htmx loading indicator */ \n.htmx-indicator { \n    display: none; \n\nJavaScript\n    margin-bottom: 8px; \n    font-size: 13px; \n    color: #6b21a8; \n} \n \n.htmx-request .htmx-indicator { \n    display: block; \n} \n2.\u200b static/app.js \n \nlet currentSessionId = null; \n \nfunction setSessionId(id) { \n    currentSessionId = id; \n    const hidden = document.getElementById(\"session_id_input\"); \n    if (hidden) hidden.value = id; \n    enableStudyButtons(); \n} \n \nfunction enableStudyButtons() { \n    if (!currentSessionId) return; \n \n    const summaryBtn = document.getElementById(\"btn-summary\"); \n    const keyBtn     = document.getElementById(\"btn-keyterms\"); \n    const qBtn       = document.getElementById(\"btn-questions\"); \n    const rBtn       = document.getElementById(\"btn-resources\"); \n \n    [summaryBtn, keyBtn, qBtn, rBtn].forEach(btn => btn.disabled = false); \n \n    summaryBtn.setAttribute(\"hx-get\", `/summary/${currentSessionId}`); \n    keyBtn.setAttribute(\"hx-get\", `/keyterms/${currentSessionId}`); \n    qBtn.setAttribute(\"hx-get\", `/questions/${currentSessionId}`); \n    rBtn.setAttribute(\"hx-get\", `/resources/${currentSessionId}`); \n} \n \ndocument.body.addEventListener(\"htmx:afterSettle\", function () { \n    // Look for the element *inside* #materials-panel that has data-session-id \n    const sidElem = document.querySelector(\"#materials-panel \n[data-session-id]\"); \n    const sid = sidElem ? sidElem.getAttribute(\"data-session-id\") : null; \n \n\n    if (sid && sid !== currentSessionId) { \n        setSessionId(sid); \n    } \n}); \n \n// recording JS: \nlet mediaRecorder = null; \nlet recordedChunks = []; \n \nconst recordBtn = document.getElementById(\"record-btn\"); \nconst recordStatus = document.getElementById(\"record-status\"); \n \nrecordBtn.addEventListener(\"click\", async () => { \n    if (!mediaRecorder || mediaRecorder.state === \"inactive\") { \n        try { \n            const stream = await navigator.mediaDevices.getUserMedia({ audio: \ntrue }); \n            mediaRecorder = new MediaRecorder(stream); \n            recordedChunks = []; \n \n            mediaRecorder.ondataavailable = (e) => { \n                if (e.data.size > 0) recordedChunks.push(e.data); \n            }; \n \n            mediaRecorder.onstop = async () => { \n                const blob = new Blob(recordedChunks, { type: \"audio/webm\" }); \n                await uploadLiveAudio(blob); \n                stream.getTracks().forEach(t => t.stop()); \n                recordStatus.textContent = \"Recording finished. \nTranscribing...\"; \n            }; \n \n            mediaRecorder.start(); \n            recordBtn.textContent = \"\u23f9 Stop Recording\"; \n            recordStatus.textContent = \"Recording...\"; \n        } catch (err) { \n            console.error(err); \n            alert(\"Could not access microphone.\"); \n        } \n    } else if (mediaRecorder.state === \"recording\") { \n        mediaRecorder.stop(); \n        recordBtn.textContent = \"\ud83c\udf99 Start Recording\"; \n    } \n}); \n\nJavaScript\n \nasync function uploadLiveAudio(blob) { \n    const formData = new FormData(); \n    formData.append(\"audio\", blob, \"live_recording.webm\"); \n    if (currentSessionId) { \n        formData.append(\"session_id\", currentSessionId); \n    } \n \n    const resp = await fetch(\"/upload_live_audio\", { \n        method: \"POST\", \n        body: formData \n    }); \n \n    const html = await resp.text(); \n    document.getElementById(\"materials-panel\").innerHTML = html; \n    recordStatus.textContent = \"Live audio processed.\"; \n} \n3.\u200b static/htmx.min.js \n \n(function(e,t){if(typeof define===\"function\"&&define.amd){define([],t)}else if(typeof module===\"object\"&&module.exports){module.exports=t()}else{e.htmx=e.htmx||t()}})(typeof self!==\"undefined\"?self:this,function(){return function(){\"use strict\";var \nQ={onLoad:F,process:zt,on:de,off:ge,trigger:ce,ajax:Nr,find:C,findAll:f,closest:v,values:function(e,t){var r=dr(e,t||\"post\");return \nr.values},remove:_,addClass:z,removeClass:n,toggleClass:$,takeClass:W,defineExtension:Ur,removeExtension:Br,logAll:V,logNone:j,logger:null,config:{historyEnabled:true,historyCacheSize:10,refreshOnHistoryMiss:false,defaultSwapStyle:\"innerHTML\",defaultSwapDelay:0,defaultSettleDelay:20,includeIndicatorStyles:true,indicat\norClass:\"htmx-indicator\",requestClass:\"htmx-request\",addedClass:\"htmx-added\",settlingClass:\"htmx-settling\",swappingClass:\"htmx-swapping\",allowEval:true,allowScriptTags:true,inlineScriptNonce:\"\",attributesToSettle:[\"class\",\"style\",\"width\",\"height\"],withCredentials:false,timeout:0,wsReconnectDelay:\"full-jitter\",wsBinary\nType:\"blob\",disableSelector:\"[hx-disable], \n[data-hx-disable]\",useTemplateFragments:false,scrollBehavior:\"smooth\",defaultFocusScroll:false,getCacheBusterParam:false,globalViewTransitions:false,methodsThatUseUrlParams:[\"get\"],selfRequestsOnly:false,ignoreTitle:false,scrollIntoViewOnBoost:true,triggerSpecsCache:null},parseInterval:d,_:t,createEventSource:function\n(e){return new EventSource(e,{withCredentials:true})},createWebSocket:function(e){var t=new WebSocket(e,[]);t.binaryType=Q.config.wsBinaryType;return t},version:\"1.9.10\"};var \nr={addTriggerHandler:Lt,bodyContains:se,canAccessLocalStorage:U,findThisElement:xe,filterValues:yr,hasAttribute:o,getAttributeValue:te,getClosestAttributeValue:ne,getClosestMatch:c,getExpressionVars:Hr,getHeaders:xr,getInputValues:dr,getInternalData:ae,getSwapSpecification:wr,getTriggerSpecs:it,getTarget:ye,makeFragme\nnt:l,mergeObjects:le,makeSettleInfo:T,oobSwap:Ee,querySelectorExt:ue,selectAndSwap:je,settleImmediately:nr,shouldCancel:ut,triggerEvent:ce,triggerErrorEvent:fe,withExtensions:R};var w=[\"get\",\"post\",\"put\",\"delete\",\"patch\"];var i=w.map(function(e){return\"[hx-\"+e+\"], [data-hx-\"+e+\"]\"}).join(\", \");var \nS=e(\"head\"),q=e(\"title\"),H=e(\"svg\",true);function e(e,t=false){return new RegExp(`<${e}(\\\\s[^>]*>|>)([\\\\s\\\\S]*?)<\\\\/${e}>`,t?\"gim\":\"im\")}function d(e){if(e==undefined){return undefined}let t=NaN;if(e.slice(-2)==\"ms\"){t=parseFloat(e.slice(0,-2))}else if(e.slice(-1)==\"s\"){t=parseFloat(e.slice(0,-1))*1e3}else \nif(e.slice(-1)==\"m\"){t=parseFloat(e.slice(0,-1))*1e3*60}else{t=parseFloat(e)}return isNaN(t)?undefined:t}function ee(e,t){return e.getAttribute&&e.getAttribute(t)}function o(e,t){return e.hasAttribute&&(e.hasAttribute(t)||e.hasAttribute(\"data-\"+t))}function te(e,t){return ee(e,t)||ee(e,\"data-\"+t)}function u(e){return \ne.parentElement}function re(){return document}function c(e,t){while(e&&!t(e)){e=u(e)}return e?e:null}function L(e,t,r){var n=te(t,r);var i=te(t,\"hx-disinherit\");if(e!==t&&i&&(i===\"*\"||i.split(\" \").indexOf(r)>=0)){return\"unset\"}else{return n}}function ne(t,r){var n=null;c(t,function(e){return \nn=L(t,e,r)});if(n!==\"unset\"){return n}}function h(e,t){var r=e.matches||e.matchesSelector||e.msMatchesSelector||e.mozMatchesSelector||e.webkitMatchesSelector||e.oMatchesSelector;return r&&r.call(e,t)}function A(e){var t=/<([a-z][^\\/\\0>\\x20\\t\\r\\n\\f]*)/i;var r=t.exec(e);if(r){return \nr[1].toLowerCase()}else{return\"\"}}function a(e,t){var r=new DOMParser;var n=r.parseFromString(e,\"text/html\");var i=n.body;while(t>0){t--;i=i.firstChild}if(i==null){i=re().createDocumentFragment()}return i}function N(e){return/<body/.test(e)}function l(e){var t=!N(e);var r=A(e);var \nn=e;if(r===\"head\"){n=n.replace(S,\"\")}if(Q.config.useTemplateFragments&&t){var i=a(\"<body><template>\"+n+\"</template></body>\",0);return i.querySelector(\"template\").content}switch(r){case\"thead\":case\"tbody\":case\"tfoot\":case\"colgroup\":case\"caption\":return a(\"<table>\"+n+\"</table>\",1);case\"col\":return \na(\"<table><colgroup>\"+n+\"</colgroup></table>\",2);case\"tr\":return a(\"<table><tbody>\"+n+\"</tbody></table>\",2);case\"td\":case\"th\":return a(\"<table><tbody><tr>\"+n+\"</tr></tbody></table>\",3);case\"script\":case\"style\":return a(\"<div>\"+n+\"</div>\",1);default:return a(n,0)}}function ie(e){if(e){e()}}function I(e,t){return \nObject.prototype.toString.call(e)===\"[object \"+t+\"]\"}function k(e){return I(e,\"Function\")}function P(e){return I(e,\"Object\")}function ae(e){var t=\"htmx-internal-data\";var r=e[t];if(!r){r=e[t]={}}return r}function M(e){var t=[];if(e){for(var r=0;r<e.length;r++){t.push(e[r])}}return t}function oe(e,t){if(e){for(var \nr=0;r<e.length;r++){t(e[r])}}}function X(e){var t=e.getBoundingClientRect();var r=t.top;var n=t.bottom;return r<window.innerHeight&&n>=0}function se(e){if(e.getRootNode&&e.getRootNode()instanceof window.ShadowRoot){return re().body.contains(e.getRootNode().host)}else{return re().body.contains(e)}}function D(e){return \ne.trim().split(/\\s+/)}function le(e,t){for(var r in t){if(t.hasOwnProperty(r)){e[r]=t[r]}}return e}function E(e){try{return JSON.parse(e)}catch(e){b(e);return null}}function U(){var e=\"htmx:localStorageTest\";try{localStorage.setItem(e,e);localStorage.removeItem(e);return true}catch(e){return false}}function \nB(t){try{var e=new URL(t);if(e){t=e.pathname+e.search}if(!/^\\/$/.test(t)){t=t.replace(/\\/+$/,\"\")}return t}catch(e){return t}}function t(e){return Tr(re().body,function(){return eval(e)})}function F(t){var e=Q.on(\"htmx:load\",function(e){t(e.detail.elt)});return e}function \nV(){Q.logger=function(e,t,r){if(console){console.log(t,e,r)}}}function j(){Q.logger=null}function C(e,t){if(t){return e.querySelector(t)}else{return C(re(),e)}}function f(e,t){if(t){return e.querySelectorAll(t)}else{return f(re(),e)}}function \n_(e,t){e=g(e);if(t){setTimeout(function(){_(e);e=null},t)}else{e.parentElement.removeChild(e)}}function z(e,t,r){e=g(e);if(r){setTimeout(function(){z(e,t);e=null},r)}else{e.classList&&e.classList.add(t)}}function \nn(e,t,r){e=g(e);if(r){setTimeout(function(){n(e,t);e=null},r)}else{if(e.classList){e.classList.remove(t);if(e.classList.length===0){e.removeAttribute(\"class\")}}}}function $(e,t){e=g(e);e.classList.toggle(t)}function W(e,t){e=g(e);oe(e.parentElement.children,function(e){n(e,t)});z(e,t)}function \nv(e,t){e=g(e);if(e.closest){return e.closest(t)}else{do{if(e==null||h(e,t)){return e}}while(e=e&&u(e));return null}}function s(e,t){return e.substring(0,t.length)===t}function G(e,t){return e.substring(e.length-t.length)===t}function J(e){var t=e.trim();if(s(t,\"<\")&&G(t,\"/>\")){return \nt.substring(1,t.length-2)}else{return t}}function Z(e,t){if(t.indexOf(\"closest \")===0){return[v(e,J(t.substr(8)))]}else if(t.indexOf(\"find \")===0){return[C(e,J(t.substr(5)))]}else if(t===\"next\"){return[e.nextElementSibling]}else if(t.indexOf(\"next \")===0){return[K(e,J(t.substr(5)))]}else \nif(t===\"previous\"){return[e.previousElementSibling]}else if(t.indexOf(\"previous \")===0){return[Y(e,J(t.substr(9)))]}else if(t===\"document\"){return[document]}else if(t===\"window\"){return[window]}else if(t===\"body\"){return[document.body]}else{return re().querySelectorAll(J(t))}}var K=function(e,t){var \nr=re().querySelectorAll(t);for(var n=0;n<r.length;n++){var i=r[n];if(i.compareDocumentPosition(e)===Node.DOCUMENT_POSITION_PRECEDING){return i}}};var Y=function(e,t){var r=re().querySelectorAll(t);for(var n=r.length-1;n>=0;n--){var i=r[n];if(i.compareDocumentPosition(e)===Node.DOCUMENT_POSITION_FOLLOWING){return \ni}}};function ue(e,t){if(t){return Z(e,t)[0]}else{return Z(re().body,e)[0]}}function g(e){if(I(e,\"String\")){return C(e)}else{return e}}function ve(e,t,r){if(k(t)){return{target:re().body,event:e,listener:t}}else{return{target:g(e),event:t,listener:r}}}function de(t,r,n){jr(function(){var \ne=ve(t,r,n);e.target.addEventListener(e.event,e.listener)});var e=k(r);return e?r:n}function ge(t,r,n){jr(function(){var e=ve(t,r,n);e.target.removeEventListener(e.event,e.listener)});return k(r)?r:n}var me=re().createElement(\"output\");function pe(e,t){var r=ne(e,t);if(r){if(r===\"this\"){return[xe(e,t)]}else{var \nn=Z(e,r);if(n.length===0){b('The selector \"'+r+'\" on '+t+\" returned no matches!\");return[me]}else{return n}}}}function xe(e,t){return c(e,function(e){return te(e,t)!=null})}function ye(e){var t=ne(e,\"hx-target\");if(t){if(t===\"this\"){return xe(e,\"hx-target\")}else{return ue(e,t)}}else{var r=ae(e);if(r.boosted){return \nre().body}else{return e}}}function be(e){var t=Q.config.attributesToSettle;for(var r=0;r<t.length;r++){if(e===t[r]){return true}}return false}function \nwe(t,r){oe(t.attributes,function(e){if(!r.hasAttribute(e.name)&&be(e.name)){t.removeAttribute(e.name)}});oe(r.attributes,function(e){if(be(e.name)){t.setAttribute(e.name,e.value)}})}function Se(e,t){var r=Fr(t);for(var n=0;n<r.length;n++){var i=r[n];try{if(i.isInlineSwap(e)){return true}}catch(e){b(e)}}return \ne===\"outerHTML\"}function Ee(e,i,a){var t=\"#\"+ee(i,\"id\");var o=\"outerHTML\";if(e===\"true\"){}else if(e.indexOf(\":\")>0){o=e.substr(0,e.indexOf(\":\"));t=e.substr(e.indexOf(\":\")+1,e.length)}else{o=e}var r=re().querySelectorAll(t);if(r){oe(r,function(e){var t;var \nr=i.cloneNode(true);t=re().createDocumentFragment();t.appendChild(r);if(!Se(o,e)){t=r}var \nn={shouldSwap:true,target:e,fragment:t};if(!ce(e,\"htmx:oobBeforeSwap\",n))return;e=n.target;if(n[\"shouldSwap\"]){Fe(o,e,e,t,a)}oe(a.elts,function(e){ce(e,\"htmx:oobAfterSwap\",n)})});i.parentNode.removeChild(i)}else{i.parentNode.removeChild(i);fe(re().body,\"htmx:oobErrorNoTarget\",{content:i})}return e}function \nCe(e,t,r){var n=ne(e,\"hx-select-oob\");if(n){var i=n.split(\",\");for(var a=0;a<i.length;a++){var o=i[a].split(\":\",2);var s=o[0].trim();if(s.indexOf(\"#\")===0){s=s.substring(1)}var l=o[1]||\"true\";var u=t.querySelector(\"#\"+s);if(u){Ee(l,u,r)}}}oe(f(t,\"[hx-swap-oob], [data-hx-swap-oob]\"),function(e){var \nt=te(e,\"hx-swap-oob\");if(t!=null){Ee(t,e,r)}})}function Re(e){oe(f(e,\"[hx-preserve], [data-hx-preserve]\"),function(e){var t=te(e,\"id\");var r=re().getElementById(t);if(r!=null){e.parentNode.replaceChild(r,e)}})}function Te(o,e,s){oe(e.querySelectorAll(\"[id]\"),function(e){var t=ee(e,\"id\");if(t&&t.length>0){var \nr=t.replace(\"'\",\"\\\\'\");var n=e.tagName.replace(\":\",\"\\\\:\");var i=o.querySelector(n+\"[id='\"+r+\"']\");if(i&&i!==o){var a=e.cloneNode();we(e,i);s.tasks.push(function(){we(e,a)})}}})}function Oe(e){return function(){n(e,Q.config.addedClass);zt(e);Nt(e);qe(e);ce(e,\"htmx:load\")}}function qe(e){var t=\"[autofocus]\";var \nr=h(e,t)?e:e.querySelector(t);if(r!=null){r.focus()}}function m(e,t,r,n){Te(e,r,n);while(r.childNodes.length>0){var i=r.firstChild;z(i,Q.config.addedClass);e.insertBefore(i,t);if(i.nodeType!==Node.TEXT_NODE&&i.nodeType!==Node.COMMENT_NODE){n.tasks.push(Oe(i))}}}function He(e,t){var \nr=0;while(r<e.length){t=(t<<5)-t+e.charCodeAt(r++)|0}return t}function Le(e){var t=0;if(e.attributes){for(var r=0;r<e.attributes.length;r++){var n=e.attributes[r];if(n.value){t=He(n.name,t);t=He(n.value,t)}}}return t}function Ae(e){var t=ae(e);if(t.onHandlers){for(var r=0;r<t.onHandlers.length;r++){const \nn=t.onHandlers[r];e.removeEventListener(n.event,n.listener)}delete t.onHandlers}}function Ne(e){var \nt=ae(e);if(t.timeout){clearTimeout(t.timeout)}if(t.webSocket){t.webSocket.close()}if(t.sseEventSource){t.sseEventSource.close()}if(t.listenerInfos){oe(t.listenerInfos,function(e){if(e.on){e.on.removeEventListener(e.trigger,e.listener)}})}Ae(e);oe(Object.keys(t),function(e){delete t[e]})}function \np(e){ce(e,\"htmx:beforeCleanupElement\");Ne(e);if(e.children){oe(e.children,function(e){p(e)})}}function Ie(t,e,r){if(t.tagName===\"BODY\"){return Ue(t,e,r)}else{var n;var i=t.previousSibling;m(u(t),t,e,r);if(i==null){n=u(t).firstChild}else{n=i.nextSibling}r.elts=r.elts.filter(function(e){return \ne!=t});while(n&&n!==t){if(n.nodeType===Node.ELEMENT_NODE){r.elts.push(n)}n=n.nextElementSibling}p(t);u(t).removeChild(t)}}function ke(e,t,r){return m(e,e.firstChild,t,r)}function Pe(e,t,r){return m(u(e),e,t,r)}function Me(e,t,r){return m(e,null,t,r)}function Xe(e,t,r){return m(u(e),e.nextSibling,t,r)}function \nDe(e,t,r){p(e);return u(e).removeChild(e)}function Ue(e,t,r){var n=e.firstChild;m(e,n,t,r);if(n){while(n.nextSibling){p(n.nextSibling);e.removeChild(n.nextSibling)}p(n);e.removeChild(n)}}function Be(e,t,r){var n=r||ne(e,\"hx-select\");if(n){var \ni=re().createDocumentFragment();oe(t.querySelectorAll(n),function(e){i.appendChild(e)});t=i}return t}function \nFe(e,t,r,n,i){switch(e){case\"none\":return;case\"outerHTML\":Ie(r,n,i);return;case\"afterbegin\":ke(r,n,i);return;case\"beforebegin\":Pe(r,n,i);return;case\"beforeend\":Me(r,n,i);return;case\"afterend\":Xe(r,n,i);return;case\"delete\":De(r,n,i);return;default:var a=Fr(t);for(var o=0;o<a.length;o++){var s=a[o];try{var \nl=s.handleSwap(e,r,n,i);if(l){if(typeof l.length!==\"undefined\"){for(var u=0;u<l.length;u++){var f=l[u];if(f.nodeType!==Node.TEXT_NODE&&f.nodeType!==Node.COMMENT_NODE){i.tasks.push(Oe(f))}}}return}}catch(e){b(e)}}if(e===\"innerHTML\"){Ue(r,n,i)}else{Fe(Q.config.defaultSwapStyle,t,r,n,i)}}}function \nVe(e){if(e.indexOf(\"<title\")>-1){var t=e.replace(H,\"\");var r=t.match(q);if(r){return r[2]}}}function je(e,t,r,n,i,a){i.title=Ve(n);var o=l(n);if(o){Ce(r,o,i);o=Be(r,o,a);Re(o);return Fe(e,r,t,o,i)}}function _e(e,t,r){var n=e.getResponseHeader(t);if(n.indexOf(\"{\")===0){var i=E(n);for(var a in \ni){if(i.hasOwnProperty(a)){var o=i[a];if(!P(o)){o={value:o}}ce(r,a,o)}}}else{var s=n.split(\",\");for(var l=0;l<s.length;l++){ce(r,s[l].trim(),[])}}}var ze=/\\s/;var x=/[\\s,]/;var $e=/[_$a-zA-Z]/;var We=/[_$a-zA-Z0-9]/;var Ge=['\"',\"'\",\"/\"];var Je=/[^\\s]/;var Ze=/[{(]/;var Ke=/[})]/;function Ye(e){var t=[];var \nr=0;while(r<e.length){if($e.exec(e.charAt(r))){var n=r;while(We.exec(e.charAt(r+1))){r++}t.push(e.substr(n,r-n+1))}else if(Ge.indexOf(e.charAt(r))!==-1){var i=e.charAt(r);var n=r;r++;while(r<e.length&&e.charAt(r)!==i){if(e.charAt(r)===\"\\\\\"){r++}r++}t.push(e.substr(n,r-n+1))}else{var a=e.charAt(r);t.push(a)}r++}return \nt}function Qe(e,t,r){return $e.exec(e.charAt(0))&&e!==\"true\"&&e!==\"false\"&&e!==\"this\"&&e!==r&&t!==\".\"}function et(e,t,r){if(t[0]===\"[\"){t.shift();var n=1;var i=\" return (function(\"+r+\"){ return (\";var a=null;while(t.length>0){var o=t[0];if(o===\"]\"){n--;if(n===0){if(a===null){i=i+\"true\"}t.shift();i+=\")})\";try{var \ns=Tr(e,function(){return Function(i)()},function(){return true});s.source=i;return s}catch(e){fe(re().body,\"htmx:syntax:error\",{error:e,source:i});return null}}}else if(o===\"[\"){n++}if(Qe(o,a,r)){i+=\"((\"+r+\".\"+o+\") ? (\"+r+\".\"+o+\") : (window.\"+o+\"))\"}else{i=i+o}a=t.shift()}}}function y(e,t){var \nr=\"\";while(e.length>0&&!t.test(e[0])){r+=e.shift()}return r}function tt(e){var t;if(e.length>0&&Ze.test(e[0])){e.shift();t=y(e,Ke).trim();e.shift()}else{t=y(e,x)}return t}var rt=\"input, textarea, select\";function nt(e,t,r){var n=[];var i=Ye(t);do{y(i,Je);var a=i.length;var \no=y(i,/[,\\[\\s]/);if(o!==\"\"){if(o===\"every\"){var s={trigger:\"every\"};y(i,Je);s.pollInterval=d(y(i,/[,\\[\\s]/));y(i,Je);var l=et(e,i,\"event\");if(l){s.eventFilter=l}n.push(s)}else if(o.indexOf(\"sse:\")===0){n.push({trigger:\"sse\",sseEvent:o.substr(4)})}else{var u={trigger:o};var \nl=et(e,i,\"event\");if(l){u.eventFilter=l}while(i.length>0&&i[0]!==\",\"){y(i,Je);var f=i.shift();if(f===\"changed\"){u.changed=true}else if(f===\"once\"){u.once=true}else if(f===\"consume\"){u.consume=true}else if(f===\"delay\"&&i[0]===\":\"){i.shift();u.delay=d(y(i,x))}else \nif(f===\"from\"&&i[0]===\":\"){i.shift();if(Ze.test(i[0])){var c=tt(i)}else{var c=y(i,x);if(c===\"closest\"||c===\"find\"||c===\"next\"||c===\"previous\"){i.shift();var h=tt(i);if(h.length>0){c+=\" \"+h}}}u.from=c}else if(f===\"target\"&&i[0]===\":\"){i.shift();u.target=tt(i)}else \nif(f===\"throttle\"&&i[0]===\":\"){i.shift();u.throttle=d(y(i,x))}else if(f===\"queue\"&&i[0]===\":\"){i.shift();u.queue=y(i,x)}else if(f===\"root\"&&i[0]===\":\"){i.shift();u[f]=tt(i)}else \nif(f===\"threshold\"&&i[0]===\":\"){i.shift();u[f]=y(i,x)}else{fe(e,\"htmx:syntax:error\",{token:i.shift()})}}n.push(u)}}if(i.length===a){fe(e,\"htmx:syntax:error\",{token:i.shift()})}y(i,Je)}while(i[0]===\",\"&&i.shift());if(r){r[t]=n}return n}function it(e){var t=te(e,\"hx-trigger\");var r=[];if(t){var \nn=Q.config.triggerSpecsCache;r=n&&n[t]||nt(e,t,n)}if(r.length>0){return r}else if(h(e,\"form\")){return[{trigger:\"submit\"}]}else if(h(e,'input[type=\"button\"], input[type=\"submit\"]')){return[{trigger:\"click\"}]}else if(h(e,rt)){return[{trigger:\"change\"}]}else{return[{trigger:\"click\"}]}}function \nat(e){ae(e).cancelled=true}function ot(e,t,r){var n=ae(e);n.timeout=setTimeout(function(){if(se(e)&&n.cancelled!==true){if(!ct(r,e,Wt(\"hx:poll:trigger\",{triggerSpec:r,target:e}))){t(e)}ot(e,t,r)}},r.pollInterval)}function st(e){return location.hostname===e.hostname&&ee(e,\"href\")&&ee(e,\"href\").indexOf(\"#\")!==0}function \nlt(t,r,e){if(t.tagName===\"A\"&&st(t)&&(t.target===\"\"||t.target===\"_self\")||t.tagName===\"FORM\"){r.boosted=true;var n,i;if(t.tagName===\"A\"){n=\"get\";i=ee(t,\"href\")}else{var \na=ee(t,\"method\");n=a?a.toLowerCase():\"get\";if(n===\"get\"){}i=ee(t,\"action\")}e.forEach(function(e){ht(t,function(e,t){if(v(e,Q.config.disableSelector)){p(e);return}he(n,i,e,t)},r,e,true)})}}function ut(e,t){if(e.type===\"submit\"||e.type===\"click\"){if(t.tagName===\"FORM\"){return true}if(h(t,'input[type=\"submit\"], \nbutton')&&v(t,\"form\")!==null){return true}if(t.tagName===\"A\"&&t.href&&(t.getAttribute(\"href\")===\"#\"||t.getAttribute(\"href\").indexOf(\"#\")!==0)){return true}}return false}function ft(e,t){return ae(e).boosted&&e.tagName===\"A\"&&t.type===\"click\"&&(t.ctrlKey||t.metaKey)}function ct(e,t,r){var \nn=e.eventFilter;if(n){try{return n.call(t,r)!==true}catch(e){fe(re().body,\"htmx:eventFilter:error\",{error:e,source:n.source});return true}}return false}function ht(a,o,e,s,l){var u=ae(a);var t;if(s.from){t=Z(a,s.from)}else{t=[a]}if(s.changed){t.forEach(function(e){var t=ae(e);t.lastValue=e.value})}oe(t,function(n){var \ni=function(e){if(!se(a)){n.removeEventListener(s.trigger,i);return}if(ft(a,e)){return}if(l||ut(e,a)){e.preventDefault()}if(ct(s,a,e)){return}var \nt=ae(e);t.triggerSpec=s;if(t.handledFor==null){t.handledFor=[]}if(t.handledFor.indexOf(a)<0){t.handledFor.push(a);if(s.consume){e.stopPropagation()}if(s.target&&e.target){if(!h(e.target,s.target)){return}}if(s.once){if(u.triggeredOnce){return}else{u.triggeredOnce=true}}if(s.changed){var \nr=ae(n);if(r.lastValue===n.value){return}r.lastValue=n.value}if(u.delayed){clearTimeout(u.delayed)}if(u.throttle){return}if(s.throttle>0){if(!u.throttle){o(a,e);u.throttle=setTimeout(function(){u.throttle=null},s.throttle)}}else \nif(s.delay>0){u.delayed=setTimeout(function(){o(a,e)},s.delay)}else{ce(a,\"htmx:trigger\");o(a,e)}}};if(e.listenerInfos==null){e.listenerInfos=[]}e.listenerInfos.push({trigger:s.trigger,listener:i,on:n});n.addEventListener(s.trigger,i)})}var vt=false;var dt=null;function \ngt(){if(!dt){dt=function(){vt=true};window.addEventListener(\"scroll\",dt);setInterval(function(){if(vt){vt=false;oe(re().querySelectorAll(\"[hx-trigger='revealed'],[data-hx-trigger='revealed']\"),function(e){mt(e)})}},200)}}function mt(t){if(!o(t,\"data-hx-revealed\")&&X(t)){t.setAttribute(\"data-hx-revealed\",\"true\");var \ne=ae(t);if(e.initHash){ce(t,\"revealed\")}else{t.addEventListener(\"htmx:afterProcessNode\",function(e){ce(t,\"revealed\")},{once:true})}}}function pt(e,t,r){var n=D(r);for(var i=0;i<n.length;i++){var a=n[i].split(/:(.+)/);if(a[0]===\"connect\"){xt(e,a[1],0)}if(a[0]===\"send\"){bt(e)}}}function \nxt(s,r,n){if(!se(s)){return}if(r.indexOf(\"/\")==0){var e=location.hostname+(location.port?\":\"+location.port:\"\");if(location.protocol==\"https:\"){r=\"wss://\"+e+r}else if(location.protocol==\"http:\"){r=\"ws://\"+e+r}}var \nt=Q.createWebSocket(r);t.onerror=function(e){fe(s,\"htmx:wsError\",{error:e,socket:t});yt(s)};t.onclose=function(e){if([1006,1012,1013].indexOf(e.code)>=0){var t=wt(n);setTimeout(function(){xt(s,r,n+1)},t)}};t.onopen=function(e){n=0};ae(s).webSocket=t;t.addEventListener(\"message\",function(e){if(yt(s)){return}var \nt=e.data;R(s,function(e){t=e.transformResponse(t,null,s)});var r=T(s);var n=l(t);var i=M(n.children);for(var a=0;a<i.length;a++){var o=i[a];Ee(te(o,\"hx-swap-oob\")||\"true\",o,r)}nr(r.tasks)})}function yt(e){if(!se(e)){ae(e).webSocket.close();return true}}function bt(u){var f=c(u,function(e){return \nae(e).webSocket!=null});if(f){u.addEventListener(it(u)[0].trigger,function(e){var t=ae(f).webSocket;var r=xr(u,f);var n=dr(u,\"post\");var i=n.errors;var a=n.values;var o=Hr(u);var s=le(a,o);var \nl=yr(s,u);l[\"HEADERS\"]=r;if(i&&i.length>0){ce(u,\"htmx:validation:halted\",i);return}t.send(JSON.stringify(l));if(ut(e,u)){e.preventDefault()}})}else{fe(u,\"htmx:noWebSocketSourceError\")}}function wt(e){var t=Q.config.wsReconnectDelay;if(typeof t===\"function\"){return t(e)}if(t===\"full-jitter\"){var r=Math.min(e,6);var \nn=1e3*Math.pow(2,r);return n*Math.random()}b('htmx.config.wsReconnectDelay must either be a function or the string \"full-jitter\"')}function St(e,t,r){var n=D(r);for(var i=0;i<n.length;i++){var a=n[i].split(/:(.+)/);if(a[0]===\"connect\"){Et(e,a[1])}if(a[0]===\"swap\"){Ct(e,a[1])}}}function Et(t,e){var \nr=Q.createEventSource(e);r.onerror=function(e){fe(t,\"htmx:sseError\",{error:e,source:r});Tt(t)};ae(t).sseEventSource=r}function Ct(a,o){var s=c(a,Ot);if(s){var l=ae(s).sseEventSource;var u=function(e){if(Tt(s)){return}if(!se(a)){l.removeEventListener(o,u);return}var \nt=e.data;R(a,function(e){t=e.transformResponse(t,null,a)});var r=wr(a);var n=ye(a);var i=T(a);je(r.swapStyle,n,a,t,i);nr(i.tasks);ce(a,\"htmx:sseMessage\",e)};ae(a).sseListener=u;l.addEventListener(o,u)}else{fe(a,\"htmx:noSSESourceError\")}}function Rt(e,t,r){var n=c(e,Ot);if(n){var i=ae(n).sseEventSource;var \na=function(){if(!Tt(n)){if(se(e)){t(e)}else{i.removeEventListener(r,a)}}};ae(e).sseListener=a;i.addEventListener(r,a)}else{fe(e,\"htmx:noSSESourceError\")}}function Tt(e){if(!se(e)){ae(e).sseEventSource.close();return true}}function Ot(e){return ae(e).sseEventSource!=null}function qt(e,t,r,n){var \ni=function(){if(!r.loaded){r.loaded=true;t(e)}};if(n>0){setTimeout(i,n)}else{i()}}function Ht(t,i,e){var a=false;oe(w,function(r){if(o(t,\"hx-\"+r)){var n=te(t,\"hx-\"+r);a=true;i.path=n;i.verb=r;e.forEach(function(e){Lt(t,e,i,function(e,t){if(v(e,Q.config.disableSelector)){p(e);return}he(r,n,e,t)})})}});return a}function \nLt(n,e,t,r){if(e.sseEvent){Rt(n,r,e.sseEvent)}else if(e.trigger===\"revealed\"){gt();ht(n,r,t,e);mt(n)}else if(e.trigger===\"intersect\"){var i={};if(e.root){i.root=ue(n,e.root)}if(e.threshold){i.threshold=parseFloat(e.threshold)}var a=new IntersectionObserver(function(e){for(var t=0;t<e.length;t++){var \nr=e[t];if(r.isIntersecting){ce(n,\"intersect\");break}}},i);a.observe(n);ht(n,r,t,e)}else if(e.trigger===\"load\"){if(!ct(e,n,Wt(\"load\",{elt:n}))){qt(n,r,t,e.delay)}}else if(e.pollInterval>0){t.polling=true;ot(n,r,e)}else{ht(n,r,t,e)}}function \nAt(e){if(Q.config.allowScriptTags&&(e.type===\"text/javascript\"||e.type===\"module\"||e.type===\"\")){var t=re().createElement(\"script\");oe(e.attributes,function(e){t.setAttribute(e.name,e.value)});t.textContent=e.textContent;t.async=false;if(Q.config.inlineScriptNonce){t.nonce=Q.config.inlineScriptNonce}var \nr=e.parentElement;try{r.insertBefore(t,e)}catch(e){b(e)}finally{if(e.parentElement){e.parentElement.removeChild(e)}}}}function Nt(e){if(h(e,\"script\")){At(e)}oe(f(e,\"script\"),function(e){At(e)})}function It(e){var t=e.attributes;for(var r=0;r<t.length;r++){var \nn=t[r].name;if(s(n,\"hx-on:\")||s(n,\"data-hx-on:\")||s(n,\"hx-on-\")||s(n,\"data-hx-on-\")){return true}}return false}function kt(e){var t=null;var r=[];if(It(e)){r.push(e)}if(document.evaluate){var n=document.evaluate('.//*[@*[ starts-with(name(), \"hx-on:\") or starts-with(name(), \"data-hx-on:\") or'+' starts-with(name(), \n\"hx-on-\") or starts-with(name(), \"data-hx-on-\") ]]',e);while(t=n.iterateNext())r.push(t)}else{var i=e.getElementsByTagName(\"*\");for(var a=0;a<i.length;a++){if(It(i[a])){r.push(i[a])}}}return r}function Pt(e){if(e.querySelectorAll){var t=\", [hx-boost] a, [data-hx-boost] a, a[hx-boost], a[data-hx-boost]\";var \nr=e.querySelectorAll(i+t+\", form, [type='submit'], [hx-sse], [data-hx-sse], [hx-ws],\"+\" [data-hx-ws], [hx-ext], [data-hx-ext], [hx-trigger], [data-hx-trigger], [hx-on], [data-hx-on]\");return r}else{return[]}}function Mt(e){var t=v(e.target,\"button, input[type='submit']\");var \n\nr=Dt(e);if(r){r.lastButtonClicked=t}}function Xt(e){var t=Dt(e);if(t){t.lastButtonClicked=null}}function Dt(e){var t=v(e.target,\"button, input[type='submit']\");if(!t){return}var r=g(\"#\"+ee(t,\"form\"))||v(t,\"form\");if(!r){return}return ae(r)}function \nUt(e){e.addEventListener(\"click\",Mt);e.addEventListener(\"focusin\",Mt);e.addEventListener(\"focusout\",Xt)}function Bt(e){var t=Ye(e);var r=0;for(var n=0;n<t.length;n++){const i=t[n];if(i===\"{\"){r++}else if(i===\"}\"){r--}}return r}function Ft(t,e,r){var n=ae(t);if(!Array.isArray(n.onHandlers)){n.onHandlers=[]}var i;var \na=function(e){return Tr(t,function(){if(!i){i=new Function(\"event\",r)}i.call(t,e)})};t.addEventListener(e,a);n.onHandlers.push({event:e,listener:a})}function Vt(e){var t=te(e,\"hx-on\");if(t){var r={};var n=t.split(\"\\n\");var i=null;var a=0;while(n.length>0){var o=n.shift();var \ns=o.match(/^\\s*([a-zA-Z:\\-\\.]+:)(.*)/);if(a===0&&s){o.split(\":\");i=s[1].slice(0,-1);r[i]=s[2]}else{r[i]+=o}a+=Bt(o)}for(var l in r){Ft(e,l,r[l])}}}function jt(e){Ae(e);for(var t=0;t<e.attributes.length;t++){var r=e.attributes[t].name;var n=e.attributes[t].value;if(s(r,\"hx-on\")||s(r,\"data-hx-on\")){var \ni=r.indexOf(\"-on\")+3;var a=r.slice(i,i+1);if(a===\"-\"||a===\":\"){var o=r.slice(i+1);if(s(o,\":\")){o=\"htmx\"+o}else if(s(o,\"-\")){o=\"htmx:\"+o.slice(1)}else if(s(o,\"htmx-\")){o=\"htmx:\"+o.slice(5)}Ft(e,o,n)}}}}function _t(t){if(v(t,Q.config.disableSelector)){p(t);return}var \nr=ae(t);if(r.initHash!==Le(t)){Ne(t);r.initHash=Le(t);Vt(t);ce(t,\"htmx:beforeProcessNode\");if(t.value){r.lastValue=t.value}var e=it(t);var n=Ht(t,r,e);if(!n){if(ne(t,\"hx-boost\")===\"true\"){lt(t,r,e)}else \nif(o(t,\"hx-trigger\")){e.forEach(function(e){Lt(t,e,r,function(){})})}}if(t.tagName===\"FORM\"||ee(t,\"type\")===\"submit\"&&o(t,\"form\")){Ut(t)}var i=te(t,\"hx-sse\");if(i){St(t,r,i)}var a=te(t,\"hx-ws\");if(a){pt(t,r,a)}ce(t,\"htmx:afterProcessNode\")}}function \nzt(e){e=g(e);if(v(e,Q.config.disableSelector)){p(e);return}_t(e);oe(Pt(e),function(e){_t(e)});oe(kt(e),jt)}function $t(e){return e.replace(/([a-z0-9])([A-Z])/g,\"$1-$2\").toLowerCase()}function Wt(e,t){var r;if(window.CustomEvent&&typeof window.CustomEvent===\"function\"){r=new \nCustomEvent(e,{bubbles:true,cancelable:true,detail:t})}else{r=re().createEvent(\"CustomEvent\");r.initCustomEvent(e,true,true,t)}return r}function fe(e,t,r){ce(e,t,le({error:t},r))}function Gt(e){return e===\"htmx:afterProcessNode\"}function R(e,t){oe(Fr(e),function(e){try{t(e)}catch(e){b(e)}})}function \nb(e){if(console.error){console.error(e)}else if(console.log){console.log(\"ERROR: \",e)}}function ce(e,t,r){e=g(e);if(r==null){r={}}r[\"elt\"]=e;var n=Wt(t,r);if(Q.logger&&!Gt(t)){Q.logger(e,t,r)}if(r.error){b(r.error);ce(e,\"htmx:error\",{errorInfo:r})}var i=e.dispatchEvent(n);var a=$t(t);if(i&&a!==t){var \no=Wt(a,n.detail);i=i&&e.dispatchEvent(o)}R(e,function(e){i=i&&(e.onEvent(t,n)!==false&&!n.defaultPrevented)});return i}var Jt=location.pathname+location.search;function Zt(){var e=re().querySelector(\"[hx-history-elt],[data-hx-history-elt]\");return e||re().body}function \nKt(e,t,r,n){if(!U()){return}if(Q.config.historyCacheSize<=0){localStorage.removeItem(\"htmx-history-cache\");return}e=B(e);var i=E(localStorage.getItem(\"htmx-history-cache\"))||[];for(var a=0;a<i.length;a++){if(i[a].url===e){i.splice(a,1);break}}var \no={url:e,content:t,title:r,scroll:n};ce(re().body,\"htmx:historyItemCreated\",{item:o,cache:i});i.push(o);while(i.length>Q.config.historyCacheSize){i.shift()}while(i.length>0){try{localStorage.setItem(\"htmx-history-cache\",JSON.stringify(i));break}catch(e){fe(re().body,\"htmx:historyCacheError\",{cause:e,cache:i});i.shift(\n)}}}function Yt(e){if(!U()){return null}e=B(e);var t=E(localStorage.getItem(\"htmx-history-cache\"))||[];for(var r=0;r<t.length;r++){if(t[r].url===e){return t[r]}}return null}function Qt(e){var t=Q.config.requestClass;var r=e.cloneNode(true);oe(f(r,\".\"+t),function(e){n(e,t)});return r.innerHTML}function er(){var \ne=Zt();var t=Jt||location.pathname+location.search;var r;try{r=re().querySelector('[hx-history=\"false\" i],[data-hx-history=\"false\" \ni]')}catch(e){r=re().querySelector('[hx-history=\"false\"],[data-hx-history=\"false\"]')}if(!r){ce(re().body,\"htmx:beforeHistorySave\",{path:t,historyElt:e});Kt(t,Qt(e),re().title,window.scrollY)}if(Q.config.historyEnabled)history.replaceState({htmx:true},re().title,window.location.href)}function \ntr(e){if(Q.config.getCacheBusterParam){e=e.replace(/org\\.htmx\\.cache-buster=[^&]*&?/,\"\");if(G(e,\"&\")||G(e,\"?\")){e=e.slice(0,-1)}}if(Q.config.historyEnabled){history.pushState({htmx:true},\"\",e)}Jt=e}function rr(e){if(Q.config.historyEnabled)history.replaceState({htmx:true},\"\",e);Jt=e}function \nnr(e){oe(e,function(e){e.call()})}function ir(a){var e=new XMLHttpRequest;var \no={path:a,xhr:e};ce(re().body,\"htmx:historyCacheMiss\",o);e.open(\"GET\",a,true);e.setRequestHeader(\"HX-Request\",\"true\");e.setRequestHeader(\"HX-History-Restore-Request\",\"true\");e.setRequestHeader(\"HX-Current-URL\",re().location.href);e.onload=function(){if(this.status>=200&&this.status<400){ce(re().body,\"htmx:historyCache\nMissLoad\",o);var e=l(this.response);e=e.querySelector(\"[hx-history-elt],[data-hx-history-elt]\")||e;var t=Zt();var r=T(t);var n=Ve(this.response);if(n){var \ni=C(\"title\");if(i){i.innerHTML=n}else{window.document.title=n}}Ue(t,e,r);nr(r.tasks);Jt=a;ce(re().body,\"htmx:historyRestore\",{path:a,cacheMiss:true,serverResponse:this.response})}else{fe(re().body,\"htmx:historyCacheMissLoadError\",o)}};e.send()}function ar(e){er();e=e||location.pathname+location.search;var \nt=Yt(e);if(t){var r=l(t.content);var n=Zt();var i=T(n);Ue(n,r,i);nr(i.tasks);document.title=t.title;setTimeout(function(){window.scrollTo(0,t.scroll)},0);Jt=e;ce(re().body,\"htmx:historyRestore\",{path:e,item:t})}else{if(Q.config.refreshOnHistoryMiss){window.location.reload(true)}else{ir(e)}}}function or(e){var \nt=pe(e,\"hx-indicator\");if(t==null){t=[e]}oe(t,function(e){var t=ae(e);t.requestCount=(t.requestCount||0)+1;e.classList[\"add\"].call(e.classList,Q.config.requestClass)});return t}function sr(e){var t=pe(e,\"hx-disabled-elt\");if(t==null){t=[]}oe(t,function(e){var \nt=ae(e);t.requestCount=(t.requestCount||0)+1;e.setAttribute(\"disabled\",\"\")});return t}function lr(e,t){oe(e,function(e){var t=ae(e);t.requestCount=(t.requestCount||0)-1;if(t.requestCount===0){e.classList[\"remove\"].call(e.classList,Q.config.requestClass)}});oe(t,function(e){var \nt=ae(e);t.requestCount=(t.requestCount||0)-1;if(t.requestCount===0){e.removeAttribute(\"disabled\")}})}function ur(e,t){for(var r=0;r<e.length;r++){var n=e[r];if(n.isSameNode(t)){return true}}return false}function fr(e){if(e.name===\"\"||e.name==null||e.disabled||v(e,\"fieldset[disabled]\")){return \nfalse}if(e.type===\"button\"||e.type===\"submit\"||e.tagName===\"image\"||e.tagName===\"reset\"||e.tagName===\"file\"){return false}if(e.type===\"checkbox\"||e.type===\"radio\"){return e.checked}return true}function cr(e,t,r){if(e!=null&&t!=null){var n=r[e];if(n===undefined){r[e]=t}else \nif(Array.isArray(n)){if(Array.isArray(t)){r[e]=n.concat(t)}else{n.push(t)}}else{if(Array.isArray(t)){r[e]=[n].concat(t)}else{r[e]=[n,t]}}}}function hr(t,r,n,e,i){if(e==null||ur(t,e)){return}else{t.push(e)}if(fr(e)){var a=ee(e,\"name\");var \no=e.value;if(e.multiple&&e.tagName===\"SELECT\"){o=M(e.querySelectorAll(\"option:checked\")).map(function(e){return e.value})}if(e.files){o=M(e.files)}cr(a,o,r);if(i){vr(e,n)}}if(h(e,\"form\")){var s=e.elements;oe(s,function(e){hr(t,r,n,e,i)})}}function \nvr(e,t){if(e.willValidate){ce(e,\"htmx:validation:validate\");if(!e.checkValidity()){t.push({elt:e,message:e.validationMessage,validity:e.validity});ce(e,\"htmx:validation:failed\",{message:e.validationMessage,validity:e.validity})}}}function dr(e,t){var r=[];var n={};var i={};var a=[];var \no=ae(e);if(o.lastButtonClicked&&!se(o.lastButtonClicked)){o.lastButtonClicked=null}var \ns=h(e,\"form\")&&e.noValidate!==true||te(e,\"hx-validate\")===\"true\";if(o.lastButtonClicked){s=s&&o.lastButtonClicked.formNoValidate!==true}if(t!==\"get\"){hr(r,i,a,v(e,\"form\"),s)}hr(r,n,a,e,s);if(o.lastButtonClicked||e.tagName===\"BUTTON\"||e.tagName===\"INPUT\"&&ee(e,\"type\")===\"submit\"){var l=o.lastButtonClicked||e;var \nu=ee(l,\"name\");cr(u,l.value,i)}var f=pe(e,\"hx-include\");oe(f,function(e){hr(r,n,a,e,s);if(!h(e,\"form\")){oe(e.querySelectorAll(rt),function(e){hr(r,n,a,e,s)})}});n=le(n,i);return{errors:a,values:n}}function gr(e,t,r){if(e!==\"\"){e+=\"&\"}if(String(r)===\"[object Object]\"){r=JSON.stringify(r)}var \nn=encodeURIComponent(r);e+=encodeURIComponent(t)+\"=\"+n;return e}function mr(e){var t=\"\";for(var r in e){if(e.hasOwnProperty(r)){var n=e[r];if(Array.isArray(n)){oe(n,function(e){t=gr(t,r,e)})}else{t=gr(t,r,n)}}}return t}function pr(e){var t=new FormData;for(var r in e){if(e.hasOwnProperty(r)){var \nn=e[r];if(Array.isArray(n)){oe(n,function(e){t.append(r,e)})}else{t.append(r,n)}}}return t}function xr(e,t,r){var \nn={\"HX-Request\":\"true\",\"HX-Trigger\":ee(e,\"id\"),\"HX-Trigger-Name\":ee(e,\"name\"),\"HX-Target\":te(t,\"id\"),\"HX-Current-URL\":re().location.href};Rr(e,\"hx-headers\",false,n);if(r!==undefined){n[\"HX-Prompt\"]=r}if(ae(e).boosted){n[\"HX-Boosted\"]=\"true\"}return n}function yr(t,e){var \nr=ne(e,\"hx-params\");if(r){if(r===\"none\"){return{}}else if(r===\"*\"){return t}else if(r.indexOf(\"not \")===0){oe(r.substr(4).split(\",\"),function(e){e=e.trim();delete t[e]});return t}else{var n={};oe(r.split(\",\"),function(e){e=e.trim();n[e]=t[e]});return n}}else{return t}}function br(e){return \nee(e,\"href\")&&ee(e,\"href\").indexOf(\"#\")>=0}function wr(e,t){var r=t?t:ne(e,\"hx-swap\");var n={swapStyle:ae(e).boosted?\"innerHTML\":Q.config.defaultSwapStyle,swapDelay:Q.config.defaultSwapDelay,settleDelay:Q.config.defaultSettleDelay};if(Q.config.scrollIntoViewOnBoost&&ae(e).boosted&&!br(e)){n[\"show\"]=\"top\"}if(r){var \ni=D(r);if(i.length>0){for(var a=0;a<i.length;a++){var o=i[a];if(o.indexOf(\"swap:\")===0){n[\"swapDelay\"]=d(o.substr(5))}else if(o.indexOf(\"settle:\")===0){n[\"settleDelay\"]=d(o.substr(7))}else if(o.indexOf(\"transition:\")===0){n[\"transition\"]=o.substr(11)===\"true\"}else \nif(o.indexOf(\"ignoreTitle:\")===0){n[\"ignoreTitle\"]=o.substr(12)===\"true\"}else if(o.indexOf(\"scroll:\")===0){var s=o.substr(7);var l=s.split(\":\");var u=l.pop();var f=l.length>0?l.join(\":\"):null;n[\"scroll\"]=u;n[\"scrollTarget\"]=f}else if(o.indexOf(\"show:\")===0){var c=o.substr(5);var l=c.split(\":\");var h=l.pop();var \nf=l.length>0?l.join(\":\"):null;n[\"show\"]=h;n[\"showTarget\"]=f}else if(o.indexOf(\"focus-scroll:\")===0){var v=o.substr(\"focus-scroll:\".length);n[\"focusScroll\"]=v==\"true\"}else if(a==0){n[\"swapStyle\"]=o}else{b(\"Unknown modifier in hx-swap: \"+o)}}}}return n}function Sr(e){return \nne(e,\"hx-encoding\")===\"multipart/form-data\"||h(e,\"form\")&&ee(e,\"enctype\")===\"multipart/form-data\"}function Er(t,r,n){var i=null;R(r,function(e){if(i==null){i=e.encodeParameters(t,n,r)}});if(i!=null){return i}else{if(Sr(r)){return pr(n)}else{return mr(n)}}}function T(e){return{tasks:[],elts:[e]}}function Cr(e,t){var \nr=e[0];var n=e[e.length-1];if(t.scroll){var i=null;if(t.scrollTarget){i=ue(r,t.scrollTarget)}if(t.scroll===\"top\"&&(r||i)){i=i||r;i.scrollTop=0}if(t.scroll===\"bottom\"&&(n||i)){i=i||n;i.scrollTop=i.scrollHeight}}if(t.show){var i=null;if(t.showTarget){var \na=t.showTarget;if(t.showTarget===\"window\"){a=\"body\"}i=ue(r,a)}if(t.show===\"top\"&&(r||i)){i=i||r;i.scrollIntoView({block:\"start\",behavior:Q.config.scrollBehavior})}if(t.show===\"bottom\"&&(n||i)){i=i||n;i.scrollIntoView({block:\"end\",behavior:Q.config.scrollBehavior})}}}function \nRr(e,t,r,n){if(n==null){n={}}if(e==null){return n}var i=te(e,t);if(i){var a=i.trim();var o=r;if(a===\"unset\"){return null}if(a.indexOf(\"javascript:\")===0){a=a.substr(11);o=true}else if(a.indexOf(\"js:\")===0){a=a.substr(3);o=true}if(a.indexOf(\"{\")!==0){a=\"{\"+a+\"}\"}var s;if(o){s=Tr(e,function(){return Function(\"return \n(\"+a+\")\")()},{})}else{s=E(a)}for(var l in s){if(s.hasOwnProperty(l)){if(n[l]==null){n[l]=s[l]}}}}return Rr(u(e),t,r,n)}function Tr(e,t,r){if(Q.config.allowEval){return t()}else{fe(e,\"htmx:evalDisallowedError\");return r}}function Or(e,t){return Rr(e,\"hx-vars\",true,t)}function qr(e,t){return \nRr(e,\"hx-vals\",false,t)}function Hr(e){return le(Or(e),qr(e))}function Lr(t,r,n){if(n!==null){try{t.setRequestHeader(r,n)}catch(e){t.setRequestHeader(r,encodeURIComponent(n));t.setRequestHeader(r+\"-URI-AutoEncoded\",\"true\")}}}function Ar(t){if(t.responseURL&&typeof URL!==\"undefined\"){try{var e=new \nURL(t.responseURL);return e.pathname+e.search}catch(e){fe(re().body,\"htmx:badResponseUrl\",{url:t.responseURL})}}}function O(e,t){return t.test(e.getAllResponseHeaders())}function Nr(e,t,r){e=e.toLowerCase();if(r){if(r instanceof Element||I(r,\"String\")){return \nhe(e,t,null,null,{targetOverride:g(r),returnPromise:true})}else{return he(e,t,g(r.source),r.event,{handler:r.handler,headers:r.headers,values:r.values,targetOverride:g(r.target),swapOverride:r.swap,select:r.select,returnPromise:true})}}else{return he(e,t,null,null,{returnPromise:true})}}function Ir(e){var \nt=[];while(e){t.push(e);e=e.parentElement}return t}function kr(e,t,r){var n;var i;if(typeof URL===\"function\"){i=new URL(t,document.location.href);var a=document.location.origin;n=a===i.origin}else{i=t;n=s(t,document.location.origin)}if(Q.config.selfRequestsOnly){if(!n){return false}}return \nce(e,\"htmx:validateUrl\",le({url:i,sameHost:n},r))}function he(t,r,n,i,a,e){var o=null;var s=null;a=a!=null?a:{};if(a.returnPromise&&typeof Promise!==\"undefined\"){var l=new Promise(function(e,t){o=e;s=t})}if(n==null){n=re().body}var M=a.handler||Mr;var X=a.select||null;if(!se(n)){ie(o);return l}var \nu=a.targetOverride||ye(n);if(u==null||u==me){fe(n,\"htmx:targetError\",{target:te(n,\"hx-target\")});ie(s);return l}var f=ae(n);var c=f.lastButtonClicked;if(c){var h=ee(c,\"formaction\");if(h!=null){r=h}var v=ee(c,\"formmethod\");if(v!=null){if(v.toLowerCase()!==\"dialog\"){t=v}}}var d=ne(n,\"hx-confirm\");if(e===undefined){var \nD=function(e){return he(t,r,n,i,a,!!e)};var U={target:u,elt:n,path:r,verb:t,triggeringEvent:i,etc:a,issueRequest:D,question:d};if(ce(n,\"htmx:confirm\",U)===false){ie(o);return l}}var g=n;var m=ne(n,\"hx-sync\");var p=null;var x=false;if(m){var B=m.split(\":\");var \nF=B[0].trim();if(F===\"this\"){g=xe(n,\"hx-sync\")}else{g=ue(n,F)}m=(B[1]||\"drop\").trim();f=ae(g);if(m===\"drop\"&&f.xhr&&f.abortable!==true){ie(o);return l}else if(m===\"abort\"){if(f.xhr){ie(o);return l}else{x=true}}else if(m===\"replace\"){ce(g,\"htmx:abort\")}else if(m.indexOf(\"queue\")===0){var V=m.split(\" \n\");p=(V[1]||\"last\").trim()}}if(f.xhr){if(f.abortable){ce(g,\"htmx:abort\")}else{if(p==null){if(i){var \ny=ae(i);if(y&&y.triggerSpec&&y.triggerSpec.queue){p=y.triggerSpec.queue}}if(p==null){p=\"last\"}}if(f.queuedRequests==null){f.queuedRequests=[]}if(p===\"first\"&&f.queuedRequests.length===0){f.queuedRequests.push(function(){he(t,r,n,i,a)})}else if(p===\"all\"){f.queuedRequests.push(function(){he(t,r,n,i,a)})}else \nif(p===\"last\"){f.queuedRequests=[];f.queuedRequests.push(function(){he(t,r,n,i,a)})}ie(o);return l}}var b=new XMLHttpRequest;f.xhr=b;f.abortable=x;var w=function(){f.xhr=null;f.abortable=false;if(f.queuedRequests!=null&&f.queuedRequests.length>0){var e=f.queuedRequests.shift();e()}};var j=ne(n,\"hx-prompt\");if(j){var \nS=prompt(j);if(S===null||!ce(n,\"htmx:prompt\",{prompt:S,target:u})){ie(o);w();return l}}if(d&&!e){if(!confirm(d)){ie(o);w();return l}}var E=xr(n,u,S);if(t!==\"get\"&&!Sr(n)){E[\"Content-Type\"]=\"application/x-www-form-urlencoded\"}if(a.headers){E=le(E,a.headers)}var _=dr(n,t);var C=_.errors;var \nR=_.values;if(a.values){R=le(R,a.values)}var z=Hr(n);var $=le(R,z);var T=yr($,n);if(Q.config.getCacheBusterParam&&t===\"get\"){T[\"org.htmx.cache-buster\"]=ee(u,\"id\")||\"true\"}if(r==null||r===\"\"){r=re().location.href}var O=Rr(n,\"hx-request\");var W=ae(n).boosted;var q=Q.config.methodsThatUseUrlParams.indexOf(t)>=0;var \nH={boosted:W,useUrlParams:q,parameters:T,unfilteredParameters:$,headers:E,target:u,verb:t,errors:C,withCredentials:a.credentials||O.credentials||Q.config.withCredentials,timeout:a.timeout||O.timeout||Q.config.timeout,path:r,triggeringEvent:i};if(!ce(n,\"htmx:configRequest\",H)){ie(o);w();return \nl}r=H.path;t=H.verb;E=H.headers;T=H.parameters;C=H.errors;q=H.useUrlParams;if(C&&C.length>0){ce(n,\"htmx:validation:halted\",H);ie(o);w();return l}var G=r.split(\"#\");var J=G[0];var L=G[1];var A=r;if(q){A=J;var \nZ=Object.keys(T).length!==0;if(Z){if(A.indexOf(\"?\")<0){A+=\"?\"}else{A+=\"&\"}A+=mr(T);if(L){A+=\"#\"+L}}}if(!kr(n,A,H)){fe(n,\"htmx:invalidPath\",H);ie(s);return l}b.open(t.toUpperCase(),A,true);b.overrideMimeType(\"text/html\");b.withCredentials=H.withCredentials;b.timeout=H.timeout;if(O.noHeaders){}else{for(var N in \nE){if(E.hasOwnProperty(N)){var K=E[N];Lr(b,N,K)}}}var I={xhr:b,target:u,requestConfig:H,etc:a,boosted:W,select:X,pathInfo:{requestPath:r,finalRequestPath:A,anchor:L}};b.onload=function(){try{var e=Ir(n);I.pathInfo.responsePath=Ar(b);M(n,I);lr(k,P);ce(n,\"htmx:afterRequest\",I);ce(n,\"htmx:afterOnLoad\",I);if(!se(n)){var \nt=null;while(e.length>0&&t==null){var r=e.shift();if(se(r)){t=r}}if(t){ce(t,\"htmx:afterRequest\",I);ce(t,\"htmx:afterOnLoad\",I)}}ie(o);w()}catch(e){fe(n,\"htmx:onLoadError\",le({error:e},I));throw \ne}};b.onerror=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:sendError\",I);ie(s);w()};b.onabort=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:sendAbort\",I);ie(s);w()};b.ontimeout=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:timeout\",I);ie(s);w()};if(!ce(n,\"htmx:beforeRequest\",I)){ie\n(o);w();return l}var k=or(n);var P=sr(n);oe([\"loadstart\",\"loadend\",\"progress\",\"abort\"],function(t){oe([b,b.upload],function(e){e.addEventListener(t,function(e){ce(n,\"htmx:xhr:\"+t,{lengthComputable:e.lengthComputable,loaded:e.loaded,total:e.total})})})});ce(n,\"htmx:beforeSend\",I);var Y=q?null:Er(b,n,T);b.send(Y);return \nl}function Pr(e,t){var r=t.xhr;var n=null;var i=null;if(O(r,/HX-Push:/i)){n=r.getResponseHeader(\"HX-Push\");i=\"push\"}else if(O(r,/HX-Push-Url:/i)){n=r.getResponseHeader(\"HX-Push-Url\");i=\"push\"}else \nif(O(r,/HX-Replace-Url:/i)){n=r.getResponseHeader(\"HX-Replace-Url\");i=\"replace\"}if(n){if(n===\"false\"){return{}}else{return{type:i,path:n}}}var a=t.pathInfo.finalRequestPath;var o=t.pathInfo.responsePath;var s=ne(e,\"hx-push-url\");var l=ne(e,\"hx-replace-url\");var u=ae(e).boosted;var f=null;var \nc=null;if(s){f=\"push\";c=s}else if(l){f=\"replace\";c=l}else if(u){f=\"push\";c=o||a}if(c){if(c===\"false\"){return{}}if(c===\"true\"){c=o||a}if(t.pathInfo.anchor&&c.indexOf(\"#\")===-1){c=c+\"#\"+t.pathInfo.anchor}return{type:f,path:c}}else{return{}}}function Mr(l,u){var f=u.xhr;var c=u.target;var e=u.etc;var \nt=u.requestConfig;var h=u.select;if(!ce(l,\"htmx:beforeOnLoad\",u))return;if(O(f,/HX-Trigger:/i)){_e(f,\"HX-Trigger\",l)}if(O(f,/HX-Location:/i)){er();var r=f.getResponseHeader(\"HX-Location\");var v;if(r.indexOf(\"{\")===0){v=E(r);r=v[\"path\"];delete v[\"path\"]}Nr(\"GET\",r,v).then(function(){tr(r)});return}var \nn=O(f,/HX-Refresh:/i)&&\"true\"===f.getResponseHeader(\"HX-Refresh\");if(O(f,/HX-Redirect:/i)){location.href=f.getResponseHeader(\"HX-Redirect\");n&&location.reload();return}if(n){location.reload();return}if(O(f,/HX-Retarget:/i)){if(f.getResponseHeader(\"HX-Retarget\")===\"this\"){u.target=l}else{u.target=ue(l,f.getResponseHead\ner(\"HX-Retarget\"))}}var d=Pr(l,u);var i=f.status>=200&&f.status<400&&f.status!==204;var g=f.response;var a=f.status>=400;var m=Q.config.ignoreTitle;var \no=le({shouldSwap:i,serverResponse:g,isError:a,ignoreTitle:m},u);if(!ce(c,\"htmx:beforeSwap\",o))return;c=o.target;g=o.serverResponse;a=o.isError;m=o.ignoreTitle;u.target=c;u.failed=a;u.successful=!a;if(o.shouldSwap){if(f.status===286){at(l)}R(l,function(e){g=e.transformResponse(g,f,l)});if(d.type){er()}var \ns=e.swapOverride;if(O(f,/HX-Reswap:/i)){s=f.getResponseHeader(\"HX-Reswap\")}var v=wr(l,s);if(v.hasOwnProperty(\"ignoreTitle\")){m=v.ignoreTitle}c.classList.add(Q.config.swappingClass);var p=null;var x=null;var y=function(){try{var e=document.activeElement;var \nt={};try{t={elt:e,start:e?e.selectionStart:null,end:e?e.selectionEnd:null}}catch(e){}var \nr;if(h){r=h}if(O(f,/HX-Reselect:/i)){r=f.getResponseHeader(\"HX-Reselect\")}if(d.type){ce(re().body,\"htmx:beforeHistoryUpdate\",le({history:d},u));if(d.type===\"push\"){tr(d.path);ce(re().body,\"htmx:pushedIntoHistory\",{path:d.path})}else{rr(d.path);ce(re().body,\"htmx:replacedInHistory\",{path:d.path})}}var \nn=T(c);je(v.swapStyle,c,l,g,n,r);if(t.elt&&!se(t.elt)&&ee(t.elt,\"id\")){var i=document.getElementById(ee(t.elt,\"id\"));var \na={preventScroll:v.focusScroll!==undefined?!v.focusScroll:!Q.config.defaultFocusScroll};if(i){if(t.start&&i.setSelectionRange){try{i.setSelectionRange(t.start,t.end)}catch(e){}}i.focus(a)}}c.classList.remove(Q.config.swappingClass);oe(n.elts,function(e){if(e.classList){e.classList.add(Q.config.settlingClass)}ce(e,\"htm\nx:afterSwap\",u)});if(O(f,/HX-Trigger-After-Swap:/i)){var o=l;if(!se(l)){o=re().body}_e(f,\"HX-Trigger-After-Swap\",o)}var s=function(){oe(n.tasks,function(e){e.call()});oe(n.elts,function(e){if(e.classList){e.classList.remove(Q.config.settlingClass)}ce(e,\"htmx:afterSettle\",u)});if(u.pathInfo.anchor){var \ne=re().getElementById(u.pathInfo.anchor);if(e){e.scrollIntoView({block:\"start\",behavior:\"auto\"})}}if(n.title&&!m){var t=C(\"title\");if(t){t.innerHTML=n.title}else{window.document.title=n.title}}Cr(n.elts,v);if(O(f,/HX-Trigger-After-Settle:/i)){var \nr=l;if(!se(l)){r=re().body}_e(f,\"HX-Trigger-After-Settle\",r)}ie(p)};if(v.settleDelay>0){setTimeout(s,v.settleDelay)}else{s()}}catch(e){fe(l,\"htmx:swapError\",u);ie(x);throw e}};var b=Q.config.globalViewTransitions;if(v.hasOwnProperty(\"transition\")){b=v.transition}if(b&&ce(l,\"htmx:beforeTransition\",u)&&typeof \nPromise!==\"undefined\"&&document.startViewTransition){var w=new Promise(function(e,t){p=e;x=t});var S=y;y=function(){document.startViewTransition(function(){S();return w})}}if(v.swapDelay>0){setTimeout(y,v.swapDelay)}else{y()}}if(a){fe(l,\"htmx:responseError\",le({error:\"Response Status Error Code \"+f.status+\" from \n\"+u.pathInfo.requestPath},u))}}var Xr={};function Dr(){return{init:function(e){return null},onEvent:function(e,t){return true},transformResponse:function(e,t,r){return e},isInlineSwap:function(e){return false},handleSwap:function(e,t,r,n){return false},encodeParameters:function(e,t,r){return null}}}function \nUr(e,t){if(t.init){t.init(r)}Xr[e]=le(Dr(),t)}function Br(e){delete Xr[e]}function Fr(e,r,n){if(e==undefined){return r}if(r==undefined){r=[]}if(n==undefined){n=[]}var t=te(e,\"hx-ext\");if(t){oe(t.split(\",\"),function(e){e=e.replace(/ /g,\"\");if(e.slice(0,7)==\"ignore:\"){n.push(e.slice(7));return}if(n.indexOf(e)<0){var \nt=Xr[e];if(t&&r.indexOf(t)<0){r.push(t)}}})}return Fr(u(e),r,n)}var Vr=false;re().addEventListener(\"DOMContentLoaded\",function(){Vr=true});function jr(e){if(Vr||re().readyState===\"complete\"){e()}else{re().addEventListener(\"DOMContentLoaded\",e)}}function \n_r(){if(Q.config.includeIndicatorStyles!==false){re().head.insertAdjacentHTML(\"beforeend\",\"<style>                      .\"+Q.config.indicatorClass+\"{opacity:0}                      .\"+Q.config.requestClass+\" .\"+Q.config.indicatorClass+\"{opacity:1; transition: opacity 200ms ease-in;}                      \n.\"+Q.config.requestClass+\".\"+Q.config.indicatorClass+\"{opacity:1; transition: opacity 200ms ease-in;}                    </style>\")}}function zr(){var e=re().querySelector('meta[name=\"htmx-config\"]');if(e){return E(e.content)}else{return null}}function $r(){var \ne=zr();if(e){Q.config=le(Q.config,e)}}jr(function(){$r();_r();var e=re().body;zt(e);var t=re().querySelectorAll(\"[hx-trigger='restored'],[data-hx-trigger='restored']\");e.addEventListener(\"htmx:abort\",function(e){var t=e.target;var r=ae(t);if(r&&r.xhr){r.xhr.abort()}});const \nr=window.onpopstate?window.onpopstate.bind(window):null;window.onpopstate=function(e){if(e.state&&e.state.htmx){ar();oe(t,function(e){ce(e,\"htmx:restored\",{document:re(),triggerEvent:ce})})}else{if(r){r(e)}}};setTimeout(function(){ce(e,\"htmx:load\",{});e=null},0)});return Q}()}); \n \nFix:  \n1.\u200b The following buttons do not work on panel 2. Generate Study Material: \na.\u200b Summary \nb.\u200b Key Terms \nc.\u200b Practice Questions \nd.\u200b External Resources \n \n",
        "added_at": "2025-11-30T01:09:19.473232"
      }
    ]
  },
  "a0b02959-e777-41bf-88a5-29f078a7d549": {
    "created_at": "2025-11-30T01:33:09.865610",
    "files": [
      {
        "name": "326 Proj.pdf",
        "type": "document",
        "text": "Python\ngit add .  \ngit commit -m \u201c   \u201c \ngit push -u origin main  \n \n \nmain.py:  \n \nimport os \nimport json \nimport uuid \nimport subprocess \nfrom datetime import datetime \nfrom typing import Dict, Any \n \nimport fitz  # PyMuPDF \nfrom pptx import Presentation \nfrom fastapi import FastAPI, Request, UploadFile, File, Form \nfrom fastapi.responses import HTMLResponse, PlainTextResponse \nfrom fastapi.staticfiles import StaticFiles \nfrom fastapi.templating import Jinja2Templates \nimport google.generativeai as genai \n \n# Ai model  \ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\")) \nGEM_MODEL = genai.GenerativeModel(\"gemini-1.5-flash\") \n \n# ----------------- CONFIG ----------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \nUPLOAD_DIR = os.path.join(BASE_DIR, \"upload\") \nAUDIO_DIR = os.path.join(UPLOAD_DIR, \"audio\") \nDOC_DIR = os.path.join(UPLOAD_DIR, \"docs\") \nSESSIONS_PATH = os.path.join(BASE_DIR, \"sessions.json\") \n \nos.makedirs(UPLOAD_DIR, exist_ok=True) \nos.makedirs(AUDIO_DIR, exist_ok=True) \nos.makedirs(DOC_DIR, exist_ok=True) \n \n#client = (api_key=os.getenv(\"GEMINI_API_KEY\")) \n \napp = FastAPI(title=\"Helektron Study Assistant\") \napp.mount(\"/static\", StaticFiles(directory=os.path.join(BASE_DIR, \"static\")), \nname=\"static\") \ntemplates = Jinja2Templates(directory=os.path.join(BASE_DIR, \"templates\")) \n \n\n# ----------------- SESSION STORAGE ----------------- \ndef load_sessions() -> Dict[str, Any]: \n    if not os.path.exists(SESSIONS_PATH): \n        return {} \n    with open(SESSIONS_PATH, \"r\") as f: \n        try: \n            return json.load(f) \n        except json.JSONDecodeError: \n            return {} \n \ndef save_sessions(sessions: Dict[str, Any]) -> None: \n    with open(SESSIONS_PATH, \"w\") as f: \n        json.dump(sessions, f, indent=2) \n \nsessions: Dict[str, Any] = load_sessions() \n \ndef get_or_create_session(session_id: str | None) -> str: \n    global sessions \n    if session_id and session_id in sessions: \n        return session_id \n    new_id = str(uuid.uuid4()) \n    sessions[new_id] = { \n        \"created_at\": datetime.utcnow().isoformat(), \n        \"files\": [],   # list of {\"name\",\"type\",\"text\"} \n    } \n    save_sessions(sessions) \n    return new_id \n \ndef add_text_to_session(session_id: str, filename: str, file_type: str, text: \nstr) -> None: \n    global sessions \n    if session_id not in sessions: \n        session_id = get_or_create_session(None) \n    sessions[session_id][\"files\"].append({ \n        \"name\": filename, \n        \"type\": file_type, \n        \"text\": text, \n        \"added_at\": datetime.utcnow().isoformat() \n    }) \n    save_sessions(sessions) \n \ndef get_session_text(session_id: str) -> str: \n    session = sessions.get(session_id) \n    if not session: \n\n        return \"\" \n    blocks = [f\"--- {f['name']} ({f['type']}) ---\\n{f['text']}\" for f in \nsession[\"files\"] if f.get(\"text\")] \n    return \"\\n\\n\".join(blocks) \n \n# ----------------- HELPERS: FILE TEXT EXTRACTION ----------------- \ndef extract_txt(path: str) -> str: \n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        return f.read() \n \ndef extract_pdf(path: str) -> str: \n    doc = fitz.open(path) \n    texts = [] \n    for page in doc: \n        texts.append(page.get_text()) \n    doc.close() \n    return \"\\n\".join(texts) \n \ndef extract_pptx(path: str) -> str: \n    prs = Presentation(path) \n    texts = [] \n    for slide in prs.slides: \n        for shape in slide.shapes: \n            if hasattr(shape, \"text\"): \n                texts.append(shape.text) \n    return \"\\n\".join(texts) \n \ndef transcribe_audio(path: str) -> str: \n    # Find whisper binary \n    candidates = [ \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"build\", \"bin\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper-main\", \"build\", \"bin\", \"whisper-cli\"), \n    ] \n    whisper_bin = next((c for c in candidates if os.path.exists(c)), None) \n    if not whisper_bin: \n        raise RuntimeError(\"Whisper binary not found. Build whisper.cpp \nfirst.\") \n \n    wav_path = path + \".wav\" \n    subprocess.run( \n        [\"ffmpeg\", \"-y\", \"-i\", path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \n\"-ac\", \"1\", wav_path], \n        check=True \n\n    ) \n \n    out_prefix = path + \"_out\" \n    subprocess.run( \n        [whisper_bin, \"-m\", os.path.join(BASE_DIR, \"whisper.cpp\", \"models\", \n\"ggml-base.en.bin\"), \n         \"-f\", wav_path, \"--output-txt\", \"--output-file\", out_prefix], \n        check=True \n    ) \n \n    txt_path = out_prefix + \".txt\" \n    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        transcript = f.read() \n \n    # cleanup \n    for p in [wav_path, txt_path]: \n        if os.path.exists(p): \n            os.remove(p) \n \n    return transcript \n \n# ----------------- GPT PROMPTS ----------------- \ndef get_summary_prompt(transcript: str) -> str: \n    return f\"\"\" \nBased on the following combined study materials (lectures, slides, PDFs, notes, \ntranscripts), \ncreate a **structured, detailed, and academically accurate study summary**. \n \nOrganize the output into the following clearly labeled sections: \n \n- **Overview**: Briefly describe the overall topic and goals. \n- **Key Concepts Introduced**: List and explain major ideas, theories, \nformulas, or processes. \n- **Detailed Topic Breakdown**: Group related ideas and summarize explanations, \nreasoning steps, and relationships. \n- **Important Definitions**: Provide concise definitions for technical terms or \ndomain-specific vocabulary. \n- **Examples or Applications**: Summarize any examples, demonstrations, or \nreal-world applications. \n- **Main Takeaways**: What a student should remember after studying this \nmaterial. \n \nExpectations: \n- Use clear, concise bullet points. \n\n- Prioritize conceptual correctness. \n- Use simple academic language suitable for undergraduate study. \n- If information is unclear or missing, label it as `TBD`. \n \n---BEGIN MATERIAL--- \n{transcript} \n---END MATERIAL--- \n\"\"\" \n \ndef get_keyterms_prompt(text: str) -> str: \n    return f\"\"\" \nFrom the combined study materials below, extract **10\u201320 key terms** with short \ndefinitions. \n \nFor each term, include: \n- The term \n- A 1\u20132 sentence definition \n- (Optional) A quick note on why it matters in this context. \n \nFormat as a bulleted list. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_questions_prompt(text: str) -> str: \n    return f\"\"\" \nCreate **8\u201312 practice questions** based on the combined study materials below. \n \nInclude a mix of: \n- Conceptual understanding questions \n- Short-answer questions \n- Application/problem-style questions (where possible) \n \nDo NOT provide answers, only questions. \nGroup them into sections if appropriate (e.g., 'Conceptual', 'Application'). \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_resources_prompt(text: str) -> str: \n\n    return f\"\"\" \nBased on the topics and concepts in the combined study materials below, \nrecommend **3\u20137 high-quality external resources** for further study. \n \nEach resource should include: \n- Title \n- Type (e.g., textbook chapter, .edu article, video lecture) \n- Source (e.g., university, well-known platform) \n- 1\u20132 sentences on why it is relevant. \n \nPrefer: \n- .edu domains \n- Reputable textbooks \n- Well-known educational channels \n \nYou do NOT need to provide URLs, just clear, identifiable references. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef call_gemini(prompt: str) -> str: \n    try: \n        response = GEM_MODEL.generate_content(prompt) \n        return response.text.strip() \n    except Exception as e: \n        return f\"[Gemini API Error: {e}]\" \n \n# ----------------- ROUTES ----------------- \n@app.get(\"/\", response_class=HTMLResponse) \ndef index(request: Request): \n    return templates.TemplateResponse(\"index.html\", {\"request\": request}) \n \n# Upload any material (txt, pdf, pptx, mp4, m4a) \n@app.post(\"/upload\", response_class=HTMLResponse) \nasync def upload_material( \n    request: Request, \n    file: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    global sessions \n \n    session_id = get_or_create_session(session_id) \n\n    filename = file.filename \n    ext = filename.split(\".\")[-1].lower() \n \n    # Save file \n    if ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n        save_dir = AUDIO_DIR \n        kind = \"audio\" \n    else: \n        save_dir = DOC_DIR \n        kind = \"document\" \n \n    os.makedirs(save_dir, exist_ok=True) \n    saved_path = os.path.join(save_dir, f\"{uuid.uuid4().hex}_{filename}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await file.read()) \n \n    # Extract text \n    try: \n        if ext == \"txt\": \n            text = extract_txt(saved_path) \n        elif ext == \"pdf\": \n            text = extract_pdf(saved_path) \n        elif ext in [\"pptx\"]: \n            text = extract_pptx(saved_path) \n        elif ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n            text = transcribe_audio(saved_path) \n        else: \n            text = f\"[Unsupported file type: {ext}]\" \n    except Exception as e: \n        text = f\"[Error processing file {filename}: {e}]\" \n \n    add_text_to_session(session_id, filename, kind, text) \n \n    # Render updated materials list (fragment) \n    session = sessions[session_id] \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n\n \n# Live transcription: audio blob from the browser \n@app.post(\"/upload_live_audio\", response_class=HTMLResponse) \nasync def upload_live_audio( \n    request: Request, \n    audio: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    session_id = get_or_create_session(session_id) \n    filename = audio.filename or \"live_recording.webm\" \n    ext = filename.split(\".\")[-1].lower() \n \n    os.makedirs(AUDIO_DIR, exist_ok=True) \n    saved_path = os.path.join(AUDIO_DIR, f\"live_{uuid.uuid4().hex}.{ext}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await audio.read()) \n \n    try: \n        text = transcribe_audio(saved_path) \n    except Exception as e: \n        text = f\"[Error transcribing live audio: {e}]\" \n \n    add_text_to_session(session_id, filename, \"audio-live\", text) \n    session = sessions[session_id] \n \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# ---- Study tools (summary, key terms, questions, resources) ---- \n \n@app.get(\"/summary/{session_id}\", response_class=PlainTextResponse) \ndef generate_summary(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_summary_prompt(text) \n\nPython\n        content = call_gemini(prompt) \n    # Return plain text directly to HTMX target \n    return content \n \n \n@app.get(\"/keyterms/{session_id}\", response_class=PlainTextResponse) \ndef generate_keyterms(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_keyterms_prompt(text) \n        content = call_gemini(prompt) \n    return content \n \n \n@app.get(\"/questions/{session_id}\", response_class=PlainTextResponse) \ndef generate_questions_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_questions_prompt(text) \n        content = call_gemini(prompt) \n    return content \n \n \n@app.get(\"/resources/{session_id}\", response_class=PlainTextResponse) \ndef generate_resources_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_resources_prompt(text) \n        content = call_gemini(prompt) \n    return content \nRag Folder:  \n1.\u200b build_vector_store.py  \n \nimport os \nimport json \n\nimport faiss \nfrom langchain_community.vectorstores import FAISS \nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain_core.documents import Document \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter \n \n# -------------------------------------------- \n# Correct project paths \n# -------------------------------------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \n \n# Uploaded document text is stored here by your FastAPI app \nDOC_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"docs\") \n \n# Vector store output directory \nVS_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"vs\") \n \nos.makedirs(VS_DIR, exist_ok=True) \n \n \n# -------------------------------------------- \n# Load documents from upload/docs/ \n# -------------------------------------------- \ndef load_documents(): \n    docs = [] \n    if not os.path.exists(DOC_DIR): \n        print(f\"Missing directory {DOC_DIR}. No documents found.\") \n        return docs \n \n    for filename in os.listdir(DOC_DIR): \n        path = os.path.join(DOC_DIR, filename) \n        if not os.path.isfile(path): \n            continue \n \n        try: \n            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n                text = f.read().strip() \n                if text: \n                    docs.append(Document(page_content=text, metadata={\"source\": \nfilename})) \n        except Exception as e: \n            print(f\"Error loading {filename}: {e}\") \n \n    return docs \n\n \n \n# -------------------------------------------- \n# Build vector store \n# -------------------------------------------- \ndef build_vector_store(): \n    print(f\"Building vector store from {DOC_DIR}...\") \n \n    docs = load_documents() \n \n    if not docs: \n        print(\"No documents found in upload/docs/. Upload files in the web app \nfirst.\") \n        return \n \n    print(f\"Loaded {len(docs)} raw documents.\") \n \n    # ------------------------------- \n    # Chunk documents \n    # ------------------------------- \n    splitter = RecursiveCharacterTextSplitter( \n        chunk_size=1000, # ~750 tokens \n        chunk_overlap=200, # contextual overlap \n        length_function=len \n    ) \n \n    split_docs = [] \n    for doc in docs: \n        chunks = splitter.split_text(doc.page_content) \n        for chunk in chunks: \n            split_docs.append( \n                Document( \n                    page_content=chunk, \n                    metadata=doc.metadata  # keep source info \n                ) \n            ) \n \n    print(f\"Created {len(split_docs)} total chunks.\") \n \n    # ------------------------------- \n    # Generate embeddings \n    # ------------------------------- \n    embeddings = HuggingFaceEmbeddings() \n \n\nPython\n    print(\"Generating embeddings...\") \n    vectorstore = FAISS.from_documents(split_docs, embeddings) \n \n    # ------------------------------- \n    # Save FAISS index + metadata \n    # ------------------------------- \n    print(\"Saving FAISS index and metadata...\") \n \n    faiss.write_index(vectorstore.index, os.path.join(VS_DIR, \"vs.faiss\")) \n \n    with open(os.path.join(VS_DIR, \"docs.json\"), \"w\") as f: \n        json.dump([d.dict() for d in split_docs], f, indent=2) \n \n    print(\"\\n\ud83c\udf89 Vector store built successfully!\") \n    print(f\"Saved to: {VS_DIR}\") \n \n \nif __name__ == \"__main__\": \n    build_vector_store() \n \n2.\u200b rag_utils.py \n \nimport os \nimport json \nimport numpy as np \nfrom google import genai \n \n# Load Gemini client \nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\")) \n \n \n# ----------------------------- \n# Helper: Load vector store \n# ----------------------------- \ndef load_store(path): \n    if not os.path.exists(path): \n        return [] \n    with open(path, \"r\") as f: \n        return json.load(f) \n \n \n\n# ----------------------------- \n# Helper: Save vector store \n# ----------------------------- \ndef save_store(path, data): \n    with open(path, \"w\") as f: \n        json.dump(data, f, indent=4) \n \n \n# ----------------------------- \n# Embed text using Gemini \n# ----------------------------- \ndef embed_text(text: str): \n    \"\"\" \n    Uses Gemini embeddings. \n    Returns list[float]. \n    \"\"\" \n \n    result = client.models.embed_content( \n        model=\"text-embedding-004\", \n        contents=text, \n    ) \n \n    # Gemini wraps embeddings inside .output_embedding \n    return result.output_embedding \n \n \n# ----------------------------- \n# Add text to class vector store \n# ----------------------------- \ndef add_to_vector_store(class_id: str, text: str): \n    \"\"\" \n    Creates or appends to the vector store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    store = load_store(store_path) \n \n    embedding = embed_text(text) \n \n    store.append({ \n        \"text\": text, \n        \"embedding\": embedding, \n\n    }) \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Build store from scratch (optional) \n# ----------------------------- \ndef build_vector_store(class_id: str, text: str): \n    \"\"\" \n    Clears and rebuilds the entire store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    embedding = embed_text(text) \n \n    store = [ \n        { \n            \"text\": text, \n            \"embedding\": embedding, \n        } \n    ] \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Search RAG store using cosine similarity \n# ----------------------------- \ndef search_vector_store(class_id: str, query: str): \n    \"\"\" \n    Returns the most relevant text chunk for the given query. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    store = load_store(store_path) \n \n    if not store: \n        return None \n \n    query_embedding = embed_text(query) \n    query_vec = np.array(query_embedding) \n\nHTML\n \n    best_score = -999 \n    best_text = None \n \n    for item in store: \n        vec = np.array(item[\"embedding\"]) \n \n        # cosine similarity \n        score = np.dot(query_vec, vec) / ( \n            np.linalg.norm(query_vec) * np.linalg.norm(vec) \n        ) \n \n        if score > best_score: \n            best_score = score \n            best_text = item[\"text\"] \n \n    return best_text \n \n \nTemplates \n1.\u200b templates/index.html  \n \n{% extends \"base.html\" %} \n{% block content %} \n \n<h1>Helektron Study Assistant</h1> \n<p class=\"subtitle\"> \n    Upload lecture materials (PDF, slides, audio, notes), then generate \nsummaries, key terms, questions, and more. \n</p> \n \n<div class=\"layout\"> \n \n    <!-- LEFT PANEL --> \n    <div class=\"card\"> \n        <h2>1. Upload Materials</h2> \n \n        <div class=\"upload-box\"> \n            <p><strong>Upload files:</strong> .txt, .pdf, .pptx, .mp4, .m4a</p> \n \n            <form \n\n                hx-post=\"/upload\" \n                hx-target=\"#materials-panel\" \n                hx-swap=\"innerHTML\" \n                hx-indicator=\"#upload-loading\" \n                enctype=\"multipart/form-data\"> \n \n                <input type=\"file\" name=\"file\" required> \n                <input type=\"hidden\" name=\"session_id\" id=\"session_id_input\"> \n                <br><br> \n                <button class=\"primary\" type=\"submit\">Upload</button> \n            </form> \n        </div> \n \n        <div class=\"upload-box\"> \n            <p><strong>Live Transcription:</strong> record audio and transcribe \nwith Whisper.</p> \n            <button class=\"secondary\" id=\"record-btn\">\ud83c\udf99 Start \nRecording</button> \n            <p id=\"record-status\" style=\"font-size: 12px; color:#555; \nmargin-top:6px;\"></p> \n        </div> \n \n        <div id=\"upload-loading\" class=\"htmx-indicator\"> \n            Uploading/processing material... \n        </div> \n \n        <!-- \ud83d\udd25 THIS GETS UPDATED BY upload_status.html --> \n        <div id=\"materials-panel\"> \n            <em>No materials uploaded yet.</em> \n        </div> \n \n    </div> \n \n    <!-- RIGHT PANEL --> \n    <div class=\"card\"> \n        <h2>2. Generate Study Material</h2> \n \n        <p style=\"font-size:13px; color:#555;\">Choose what you want the AI to \ngenerate.</p> \n \n        <div style=\"display:flex; gap:8px; flex-wrap:wrap; \nmargin-bottom:10px;\"> \n            <button class=\"primary\" \n                    id=\"btn-summary\" \n\n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Summary \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-keyterms\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Key Terms \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-questions\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Practice Questions \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-resources\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                External Resources \n            </button> \n        </div> \n \n        <div id=\"generate-loading\" class=\"htmx-indicator\"> \n            Generating material... \n        </div> \n \n        <div id=\"results-panel\"> \n            <em>Your generated study content will appear here.</em> \n        </div> \n \n    </div> \n\nHTML\nHTML\n</div> \n \n<script src=\"/static/app.js\"></script> \n \n{% endblock %} \n \n2.\u200b templates/base.html  \n \n<!DOCTYPE html> \n<html lang=\"en\"> \n<head> \n    <meta charset=\"UTF-8\"> \n    <title>Helektron Study Assistant</title> \n    <link rel=\"stylesheet\" href=\"/static/styles.css\"> \n</head> \n \n<body> \n<div class=\"app-shell\"> \n    {% block content %}{% endblock %} \n</div> \n \n<script src=\"https://unpkg.com/htmx.org@1.9.10\"></script> \n</body> \n</html> \n \n3.\u200b Fragments folder \na.\u200b templates/fragments/class_list.html  \n \n<ul> \n    {% for cls in classes %} \n    <li> \n        <button  \n            hx-post=\"/set_active_class\"  \n            hx-vals='{\"class_id\": \"{{ cls.id }}\"}' \n            hx-target=\"#upload-list\" \n            hx-swap=\"innerHTML\" \n            class=\"{% if cls.id == active_class_id %}active{% endif %}\" \n\nHTML\nHTML\nHTML\n        > \n            {{ cls.name }} \n        </button> \n    </li> \n    {% endfor %} \n</ul> \n \nb.\u200b templates/fragments/upload_status.html  \n \n{% if session_id %} \n<div data-session-id=\"{{ session_id }}\"></div> \n{% endif %} \n \n{% if files %} \n<ul> \n    {% for f in files %} \n    <li><strong>{{ f.name }}</strong> ({{ f.type }})</li> \n    {% endfor %} \n</ul> \n{% else %} \n<em>No materials uploaded yet.</em> \n{% endif %} \n \nc.\u200b templates/fragments/summary.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n \nd.\u200b templates/fragments/resources.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n\nHTML\nHTML\nCSS\ne.\u200b templates/fragments/questions.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \nf.\u200b\ntemplates/fragments/keyterms.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n \n \nStatic:  \n1.\u200b static/styles.css \n \nbody { \n    margin: 0; \n    font-family: system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", \nsans-serif; \n    background: #f4f4fb; \n} \n \n.app-shell { \n    max-width: 1100px; \n    margin: 0 auto; \n    padding: 24px; \n} \n \nh1 { \n    font-size: 28px; \n    margin-bottom: 8px; \n} \n \n.subtitle { \n    color: #555; \n    margin-bottom: 24px; \n} \n\n \n.layout { \n    display: grid; \n    grid-template-columns: 1.2fr 1.8fr; \n    gap: 20px; \n} \n \n.card { \n    background: white; \n    border-radius: 12px; \n    padding: 16px 20px; \n    box-shadow: 0 1px 3px rgba(15,23,42,0.08); \n    border: 1px solid #e2e8f0; \n} \n \n.card h2 { \n    font-size: 18px; \n    margin-top: 0; \n    margin-bottom: 12px; \n} \n \n.upload-box { \n    border: 2px dashed #cbd5f5; \n    border-radius: 10px; \n    padding: 16px; \n    text-align: center; \n    background: #f9fafb; \n    margin-bottom: 12px; \n} \n \n.upload-box input[type=\"file\"] { \n    margin-top: 8px; \n} \n \nbutton.primary { \n    background: #4f46e5; \n    color: white; \n    border: none; \n    border-radius: 8px; \n    padding: 10px 16px; \n    font-weight: 600; \n    cursor: pointer; \n    font-size: 14px; \n} \n\n \nbutton.primary:disabled { \n    opacity: 0.6; \n    cursor: not-allowed; \n} \n \nbutton.primary:hover:not(:disabled) { \n    background: #4338ca; \n} \n \nbutton.secondary { \n    background: white; \n    color: #4f46e5; \n    border: 1px solid #cbd5f5; \n    border-radius: 8px; \n    padding: 8px 14px; \n    font-weight: 500; \n    cursor: pointer; \n    font-size: 13px; \n} \n \nbutton.secondary:hover { \n    background: #eef2ff; \n} \n \n#materials-list { \n    font-size: 14px; \n    max-height: 200px; \n    overflow-y: auto; \n} \n \n#materials-list ul { \n    padding-left: 18px; \n} \n \n#results-panel { \n    min-height: 250px; \n    font-size: 14px; \n    white-space: pre-wrap; \n} \n \n/* htmx loading indicator */ \n.htmx-indicator { \n    display: none; \n\nJavaScript\n    margin-bottom: 8px; \n    font-size: 13px; \n    color: #6b21a8; \n} \n \n.htmx-request .htmx-indicator { \n    display: block; \n} \n2.\u200b static/app.js \n \nlet currentSessionId = null; \n \nfunction setSessionId(id) { \n    currentSessionId = id; \n    const hidden = document.getElementById(\"session_id_input\"); \n    if (hidden) hidden.value = id; \n    enableStudyButtons(); \n} \n \nfunction enableStudyButtons() { \n    if (!currentSessionId) return; \n \n    const summaryBtn = document.getElementById(\"btn-summary\"); \n    const keyBtn     = document.getElementById(\"btn-keyterms\"); \n    const qBtn       = document.getElementById(\"btn-questions\"); \n    const rBtn       = document.getElementById(\"btn-resources\"); \n \n    [summaryBtn, keyBtn, qBtn, rBtn].forEach(btn => btn.disabled = false); \n \n    summaryBtn.setAttribute(\"hx-get\", `/summary/${currentSessionId}`); \n    keyBtn.setAttribute(\"hx-get\", `/keyterms/${currentSessionId}`); \n    qBtn.setAttribute(\"hx-get\", `/questions/${currentSessionId}`); \n    rBtn.setAttribute(\"hx-get\", `/resources/${currentSessionId}`); \n} \n \ndocument.body.addEventListener(\"htmx:afterSettle\", function () { \n    // Look for the element *inside* #materials-panel that has data-session-id \n    const sidElem = document.querySelector(\"#materials-panel \n[data-session-id]\"); \n    const sid = sidElem ? sidElem.getAttribute(\"data-session-id\") : null; \n \n\n    if (sid && sid !== currentSessionId) { \n        setSessionId(sid); \n    } \n}); \n \n// recording JS: \nlet mediaRecorder = null; \nlet recordedChunks = []; \n \nconst recordBtn = document.getElementById(\"record-btn\"); \nconst recordStatus = document.getElementById(\"record-status\"); \n \nrecordBtn.addEventListener(\"click\", async () => { \n    if (!mediaRecorder || mediaRecorder.state === \"inactive\") { \n        try { \n            const stream = await navigator.mediaDevices.getUserMedia({ audio: \ntrue }); \n            mediaRecorder = new MediaRecorder(stream); \n            recordedChunks = []; \n \n            mediaRecorder.ondataavailable = (e) => { \n                if (e.data.size > 0) recordedChunks.push(e.data); \n            }; \n \n            mediaRecorder.onstop = async () => { \n                const blob = new Blob(recordedChunks, { type: \"audio/webm\" }); \n                await uploadLiveAudio(blob); \n                stream.getTracks().forEach(t => t.stop()); \n                recordStatus.textContent = \"Recording finished. \nTranscribing...\"; \n            }; \n \n            mediaRecorder.start(); \n            recordBtn.textContent = \"\u23f9 Stop Recording\"; \n            recordStatus.textContent = \"Recording...\"; \n        } catch (err) { \n            console.error(err); \n            alert(\"Could not access microphone.\"); \n        } \n    } else if (mediaRecorder.state === \"recording\") { \n        mediaRecorder.stop(); \n        recordBtn.textContent = \"\ud83c\udf99 Start Recording\"; \n    } \n}); \n\nJavaScript\n \nasync function uploadLiveAudio(blob) { \n    const formData = new FormData(); \n    formData.append(\"audio\", blob, \"live_recording.webm\"); \n    if (currentSessionId) { \n        formData.append(\"session_id\", currentSessionId); \n    } \n \n    const resp = await fetch(\"/upload_live_audio\", { \n        method: \"POST\", \n        body: formData \n    }); \n \n    const html = await resp.text(); \n    document.getElementById(\"materials-panel\").innerHTML = html; \n    recordStatus.textContent = \"Live audio processed.\"; \n} \n3.\u200b static/htmx.min.js \n \n(function(e,t){if(typeof define===\"function\"&&define.amd){define([],t)}else if(typeof module===\"object\"&&module.exports){module.exports=t()}else{e.htmx=e.htmx||t()}})(typeof self!==\"undefined\"?self:this,function(){return function(){\"use strict\";var \nQ={onLoad:F,process:zt,on:de,off:ge,trigger:ce,ajax:Nr,find:C,findAll:f,closest:v,values:function(e,t){var r=dr(e,t||\"post\");return \nr.values},remove:_,addClass:z,removeClass:n,toggleClass:$,takeClass:W,defineExtension:Ur,removeExtension:Br,logAll:V,logNone:j,logger:null,config:{historyEnabled:true,historyCacheSize:10,refreshOnHistoryMiss:false,defaultSwapStyle:\"innerHTML\",defaultSwapDelay:0,defaultSettleDelay:20,includeIndicatorStyles:true,indicat\norClass:\"htmx-indicator\",requestClass:\"htmx-request\",addedClass:\"htmx-added\",settlingClass:\"htmx-settling\",swappingClass:\"htmx-swapping\",allowEval:true,allowScriptTags:true,inlineScriptNonce:\"\",attributesToSettle:[\"class\",\"style\",\"width\",\"height\"],withCredentials:false,timeout:0,wsReconnectDelay:\"full-jitter\",wsBinary\nType:\"blob\",disableSelector:\"[hx-disable], \n[data-hx-disable]\",useTemplateFragments:false,scrollBehavior:\"smooth\",defaultFocusScroll:false,getCacheBusterParam:false,globalViewTransitions:false,methodsThatUseUrlParams:[\"get\"],selfRequestsOnly:false,ignoreTitle:false,scrollIntoViewOnBoost:true,triggerSpecsCache:null},parseInterval:d,_:t,createEventSource:function\n(e){return new EventSource(e,{withCredentials:true})},createWebSocket:function(e){var t=new WebSocket(e,[]);t.binaryType=Q.config.wsBinaryType;return t},version:\"1.9.10\"};var \nr={addTriggerHandler:Lt,bodyContains:se,canAccessLocalStorage:U,findThisElement:xe,filterValues:yr,hasAttribute:o,getAttributeValue:te,getClosestAttributeValue:ne,getClosestMatch:c,getExpressionVars:Hr,getHeaders:xr,getInputValues:dr,getInternalData:ae,getSwapSpecification:wr,getTriggerSpecs:it,getTarget:ye,makeFragme\nnt:l,mergeObjects:le,makeSettleInfo:T,oobSwap:Ee,querySelectorExt:ue,selectAndSwap:je,settleImmediately:nr,shouldCancel:ut,triggerEvent:ce,triggerErrorEvent:fe,withExtensions:R};var w=[\"get\",\"post\",\"put\",\"delete\",\"patch\"];var i=w.map(function(e){return\"[hx-\"+e+\"], [data-hx-\"+e+\"]\"}).join(\", \");var \nS=e(\"head\"),q=e(\"title\"),H=e(\"svg\",true);function e(e,t=false){return new RegExp(`<${e}(\\\\s[^>]*>|>)([\\\\s\\\\S]*?)<\\\\/${e}>`,t?\"gim\":\"im\")}function d(e){if(e==undefined){return undefined}let t=NaN;if(e.slice(-2)==\"ms\"){t=parseFloat(e.slice(0,-2))}else if(e.slice(-1)==\"s\"){t=parseFloat(e.slice(0,-1))*1e3}else \nif(e.slice(-1)==\"m\"){t=parseFloat(e.slice(0,-1))*1e3*60}else{t=parseFloat(e)}return isNaN(t)?undefined:t}function ee(e,t){return e.getAttribute&&e.getAttribute(t)}function o(e,t){return e.hasAttribute&&(e.hasAttribute(t)||e.hasAttribute(\"data-\"+t))}function te(e,t){return ee(e,t)||ee(e,\"data-\"+t)}function u(e){return \ne.parentElement}function re(){return document}function c(e,t){while(e&&!t(e)){e=u(e)}return e?e:null}function L(e,t,r){var n=te(t,r);var i=te(t,\"hx-disinherit\");if(e!==t&&i&&(i===\"*\"||i.split(\" \").indexOf(r)>=0)){return\"unset\"}else{return n}}function ne(t,r){var n=null;c(t,function(e){return \nn=L(t,e,r)});if(n!==\"unset\"){return n}}function h(e,t){var r=e.matches||e.matchesSelector||e.msMatchesSelector||e.mozMatchesSelector||e.webkitMatchesSelector||e.oMatchesSelector;return r&&r.call(e,t)}function A(e){var t=/<([a-z][^\\/\\0>\\x20\\t\\r\\n\\f]*)/i;var r=t.exec(e);if(r){return \nr[1].toLowerCase()}else{return\"\"}}function a(e,t){var r=new DOMParser;var n=r.parseFromString(e,\"text/html\");var i=n.body;while(t>0){t--;i=i.firstChild}if(i==null){i=re().createDocumentFragment()}return i}function N(e){return/<body/.test(e)}function l(e){var t=!N(e);var r=A(e);var \nn=e;if(r===\"head\"){n=n.replace(S,\"\")}if(Q.config.useTemplateFragments&&t){var i=a(\"<body><template>\"+n+\"</template></body>\",0);return i.querySelector(\"template\").content}switch(r){case\"thead\":case\"tbody\":case\"tfoot\":case\"colgroup\":case\"caption\":return a(\"<table>\"+n+\"</table>\",1);case\"col\":return \na(\"<table><colgroup>\"+n+\"</colgroup></table>\",2);case\"tr\":return a(\"<table><tbody>\"+n+\"</tbody></table>\",2);case\"td\":case\"th\":return a(\"<table><tbody><tr>\"+n+\"</tr></tbody></table>\",3);case\"script\":case\"style\":return a(\"<div>\"+n+\"</div>\",1);default:return a(n,0)}}function ie(e){if(e){e()}}function I(e,t){return \nObject.prototype.toString.call(e)===\"[object \"+t+\"]\"}function k(e){return I(e,\"Function\")}function P(e){return I(e,\"Object\")}function ae(e){var t=\"htmx-internal-data\";var r=e[t];if(!r){r=e[t]={}}return r}function M(e){var t=[];if(e){for(var r=0;r<e.length;r++){t.push(e[r])}}return t}function oe(e,t){if(e){for(var \nr=0;r<e.length;r++){t(e[r])}}}function X(e){var t=e.getBoundingClientRect();var r=t.top;var n=t.bottom;return r<window.innerHeight&&n>=0}function se(e){if(e.getRootNode&&e.getRootNode()instanceof window.ShadowRoot){return re().body.contains(e.getRootNode().host)}else{return re().body.contains(e)}}function D(e){return \ne.trim().split(/\\s+/)}function le(e,t){for(var r in t){if(t.hasOwnProperty(r)){e[r]=t[r]}}return e}function E(e){try{return JSON.parse(e)}catch(e){b(e);return null}}function U(){var e=\"htmx:localStorageTest\";try{localStorage.setItem(e,e);localStorage.removeItem(e);return true}catch(e){return false}}function \nB(t){try{var e=new URL(t);if(e){t=e.pathname+e.search}if(!/^\\/$/.test(t)){t=t.replace(/\\/+$/,\"\")}return t}catch(e){return t}}function t(e){return Tr(re().body,function(){return eval(e)})}function F(t){var e=Q.on(\"htmx:load\",function(e){t(e.detail.elt)});return e}function \nV(){Q.logger=function(e,t,r){if(console){console.log(t,e,r)}}}function j(){Q.logger=null}function C(e,t){if(t){return e.querySelector(t)}else{return C(re(),e)}}function f(e,t){if(t){return e.querySelectorAll(t)}else{return f(re(),e)}}function \n_(e,t){e=g(e);if(t){setTimeout(function(){_(e);e=null},t)}else{e.parentElement.removeChild(e)}}function z(e,t,r){e=g(e);if(r){setTimeout(function(){z(e,t);e=null},r)}else{e.classList&&e.classList.add(t)}}function \nn(e,t,r){e=g(e);if(r){setTimeout(function(){n(e,t);e=null},r)}else{if(e.classList){e.classList.remove(t);if(e.classList.length===0){e.removeAttribute(\"class\")}}}}function $(e,t){e=g(e);e.classList.toggle(t)}function W(e,t){e=g(e);oe(e.parentElement.children,function(e){n(e,t)});z(e,t)}function \nv(e,t){e=g(e);if(e.closest){return e.closest(t)}else{do{if(e==null||h(e,t)){return e}}while(e=e&&u(e));return null}}function s(e,t){return e.substring(0,t.length)===t}function G(e,t){return e.substring(e.length-t.length)===t}function J(e){var t=e.trim();if(s(t,\"<\")&&G(t,\"/>\")){return \nt.substring(1,t.length-2)}else{return t}}function Z(e,t){if(t.indexOf(\"closest \")===0){return[v(e,J(t.substr(8)))]}else if(t.indexOf(\"find \")===0){return[C(e,J(t.substr(5)))]}else if(t===\"next\"){return[e.nextElementSibling]}else if(t.indexOf(\"next \")===0){return[K(e,J(t.substr(5)))]}else \nif(t===\"previous\"){return[e.previousElementSibling]}else if(t.indexOf(\"previous \")===0){return[Y(e,J(t.substr(9)))]}else if(t===\"document\"){return[document]}else if(t===\"window\"){return[window]}else if(t===\"body\"){return[document.body]}else{return re().querySelectorAll(J(t))}}var K=function(e,t){var \nr=re().querySelectorAll(t);for(var n=0;n<r.length;n++){var i=r[n];if(i.compareDocumentPosition(e)===Node.DOCUMENT_POSITION_PRECEDING){return i}}};var Y=function(e,t){var r=re().querySelectorAll(t);for(var n=r.length-1;n>=0;n--){var i=r[n];if(i.compareDocumentPosition(e)===Node.DOCUMENT_POSITION_FOLLOWING){return \ni}}};function ue(e,t){if(t){return Z(e,t)[0]}else{return Z(re().body,e)[0]}}function g(e){if(I(e,\"String\")){return C(e)}else{return e}}function ve(e,t,r){if(k(t)){return{target:re().body,event:e,listener:t}}else{return{target:g(e),event:t,listener:r}}}function de(t,r,n){jr(function(){var \ne=ve(t,r,n);e.target.addEventListener(e.event,e.listener)});var e=k(r);return e?r:n}function ge(t,r,n){jr(function(){var e=ve(t,r,n);e.target.removeEventListener(e.event,e.listener)});return k(r)?r:n}var me=re().createElement(\"output\");function pe(e,t){var r=ne(e,t);if(r){if(r===\"this\"){return[xe(e,t)]}else{var \nn=Z(e,r);if(n.length===0){b('The selector \"'+r+'\" on '+t+\" returned no matches!\");return[me]}else{return n}}}}function xe(e,t){return c(e,function(e){return te(e,t)!=null})}function ye(e){var t=ne(e,\"hx-target\");if(t){if(t===\"this\"){return xe(e,\"hx-target\")}else{return ue(e,t)}}else{var r=ae(e);if(r.boosted){return \nre().body}else{return e}}}function be(e){var t=Q.config.attributesToSettle;for(var r=0;r<t.length;r++){if(e===t[r]){return true}}return false}function \nwe(t,r){oe(t.attributes,function(e){if(!r.hasAttribute(e.name)&&be(e.name)){t.removeAttribute(e.name)}});oe(r.attributes,function(e){if(be(e.name)){t.setAttribute(e.name,e.value)}})}function Se(e,t){var r=Fr(t);for(var n=0;n<r.length;n++){var i=r[n];try{if(i.isInlineSwap(e)){return true}}catch(e){b(e)}}return \ne===\"outerHTML\"}function Ee(e,i,a){var t=\"#\"+ee(i,\"id\");var o=\"outerHTML\";if(e===\"true\"){}else if(e.indexOf(\":\")>0){o=e.substr(0,e.indexOf(\":\"));t=e.substr(e.indexOf(\":\")+1,e.length)}else{o=e}var r=re().querySelectorAll(t);if(r){oe(r,function(e){var t;var \nr=i.cloneNode(true);t=re().createDocumentFragment();t.appendChild(r);if(!Se(o,e)){t=r}var \nn={shouldSwap:true,target:e,fragment:t};if(!ce(e,\"htmx:oobBeforeSwap\",n))return;e=n.target;if(n[\"shouldSwap\"]){Fe(o,e,e,t,a)}oe(a.elts,function(e){ce(e,\"htmx:oobAfterSwap\",n)})});i.parentNode.removeChild(i)}else{i.parentNode.removeChild(i);fe(re().body,\"htmx:oobErrorNoTarget\",{content:i})}return e}function \nCe(e,t,r){var n=ne(e,\"hx-select-oob\");if(n){var i=n.split(\",\");for(var a=0;a<i.length;a++){var o=i[a].split(\":\",2);var s=o[0].trim();if(s.indexOf(\"#\")===0){s=s.substring(1)}var l=o[1]||\"true\";var u=t.querySelector(\"#\"+s);if(u){Ee(l,u,r)}}}oe(f(t,\"[hx-swap-oob], [data-hx-swap-oob]\"),function(e){var \nt=te(e,\"hx-swap-oob\");if(t!=null){Ee(t,e,r)}})}function Re(e){oe(f(e,\"[hx-preserve], [data-hx-preserve]\"),function(e){var t=te(e,\"id\");var r=re().getElementById(t);if(r!=null){e.parentNode.replaceChild(r,e)}})}function Te(o,e,s){oe(e.querySelectorAll(\"[id]\"),function(e){var t=ee(e,\"id\");if(t&&t.length>0){var \nr=t.replace(\"'\",\"\\\\'\");var n=e.tagName.replace(\":\",\"\\\\:\");var i=o.querySelector(n+\"[id='\"+r+\"']\");if(i&&i!==o){var a=e.cloneNode();we(e,i);s.tasks.push(function(){we(e,a)})}}})}function Oe(e){return function(){n(e,Q.config.addedClass);zt(e);Nt(e);qe(e);ce(e,\"htmx:load\")}}function qe(e){var t=\"[autofocus]\";var \nr=h(e,t)?e:e.querySelector(t);if(r!=null){r.focus()}}function m(e,t,r,n){Te(e,r,n);while(r.childNodes.length>0){var i=r.firstChild;z(i,Q.config.addedClass);e.insertBefore(i,t);if(i.nodeType!==Node.TEXT_NODE&&i.nodeType!==Node.COMMENT_NODE){n.tasks.push(Oe(i))}}}function He(e,t){var \nr=0;while(r<e.length){t=(t<<5)-t+e.charCodeAt(r++)|0}return t}function Le(e){var t=0;if(e.attributes){for(var r=0;r<e.attributes.length;r++){var n=e.attributes[r];if(n.value){t=He(n.name,t);t=He(n.value,t)}}}return t}function Ae(e){var t=ae(e);if(t.onHandlers){for(var r=0;r<t.onHandlers.length;r++){const \nn=t.onHandlers[r];e.removeEventListener(n.event,n.listener)}delete t.onHandlers}}function Ne(e){var \nt=ae(e);if(t.timeout){clearTimeout(t.timeout)}if(t.webSocket){t.webSocket.close()}if(t.sseEventSource){t.sseEventSource.close()}if(t.listenerInfos){oe(t.listenerInfos,function(e){if(e.on){e.on.removeEventListener(e.trigger,e.listener)}})}Ae(e);oe(Object.keys(t),function(e){delete t[e]})}function \np(e){ce(e,\"htmx:beforeCleanupElement\");Ne(e);if(e.children){oe(e.children,function(e){p(e)})}}function Ie(t,e,r){if(t.tagName===\"BODY\"){return Ue(t,e,r)}else{var n;var i=t.previousSibling;m(u(t),t,e,r);if(i==null){n=u(t).firstChild}else{n=i.nextSibling}r.elts=r.elts.filter(function(e){return \ne!=t});while(n&&n!==t){if(n.nodeType===Node.ELEMENT_NODE){r.elts.push(n)}n=n.nextElementSibling}p(t);u(t).removeChild(t)}}function ke(e,t,r){return m(e,e.firstChild,t,r)}function Pe(e,t,r){return m(u(e),e,t,r)}function Me(e,t,r){return m(e,null,t,r)}function Xe(e,t,r){return m(u(e),e.nextSibling,t,r)}function \nDe(e,t,r){p(e);return u(e).removeChild(e)}function Ue(e,t,r){var n=e.firstChild;m(e,n,t,r);if(n){while(n.nextSibling){p(n.nextSibling);e.removeChild(n.nextSibling)}p(n);e.removeChild(n)}}function Be(e,t,r){var n=r||ne(e,\"hx-select\");if(n){var \ni=re().createDocumentFragment();oe(t.querySelectorAll(n),function(e){i.appendChild(e)});t=i}return t}function \nFe(e,t,r,n,i){switch(e){case\"none\":return;case\"outerHTML\":Ie(r,n,i);return;case\"afterbegin\":ke(r,n,i);return;case\"beforebegin\":Pe(r,n,i);return;case\"beforeend\":Me(r,n,i);return;case\"afterend\":Xe(r,n,i);return;case\"delete\":De(r,n,i);return;default:var a=Fr(t);for(var o=0;o<a.length;o++){var s=a[o];try{var \nl=s.handleSwap(e,r,n,i);if(l){if(typeof l.length!==\"undefined\"){for(var u=0;u<l.length;u++){var f=l[u];if(f.nodeType!==Node.TEXT_NODE&&f.nodeType!==Node.COMMENT_NODE){i.tasks.push(Oe(f))}}}return}}catch(e){b(e)}}if(e===\"innerHTML\"){Ue(r,n,i)}else{Fe(Q.config.defaultSwapStyle,t,r,n,i)}}}function \nVe(e){if(e.indexOf(\"<title\")>-1){var t=e.replace(H,\"\");var r=t.match(q);if(r){return r[2]}}}function je(e,t,r,n,i,a){i.title=Ve(n);var o=l(n);if(o){Ce(r,o,i);o=Be(r,o,a);Re(o);return Fe(e,r,t,o,i)}}function _e(e,t,r){var n=e.getResponseHeader(t);if(n.indexOf(\"{\")===0){var i=E(n);for(var a in \ni){if(i.hasOwnProperty(a)){var o=i[a];if(!P(o)){o={value:o}}ce(r,a,o)}}}else{var s=n.split(\",\");for(var l=0;l<s.length;l++){ce(r,s[l].trim(),[])}}}var ze=/\\s/;var x=/[\\s,]/;var $e=/[_$a-zA-Z]/;var We=/[_$a-zA-Z0-9]/;var Ge=['\"',\"'\",\"/\"];var Je=/[^\\s]/;var Ze=/[{(]/;var Ke=/[})]/;function Ye(e){var t=[];var \nr=0;while(r<e.length){if($e.exec(e.charAt(r))){var n=r;while(We.exec(e.charAt(r+1))){r++}t.push(e.substr(n,r-n+1))}else if(Ge.indexOf(e.charAt(r))!==-1){var i=e.charAt(r);var n=r;r++;while(r<e.length&&e.charAt(r)!==i){if(e.charAt(r)===\"\\\\\"){r++}r++}t.push(e.substr(n,r-n+1))}else{var a=e.charAt(r);t.push(a)}r++}return \nt}function Qe(e,t,r){return $e.exec(e.charAt(0))&&e!==\"true\"&&e!==\"false\"&&e!==\"this\"&&e!==r&&t!==\".\"}function et(e,t,r){if(t[0]===\"[\"){t.shift();var n=1;var i=\" return (function(\"+r+\"){ return (\";var a=null;while(t.length>0){var o=t[0];if(o===\"]\"){n--;if(n===0){if(a===null){i=i+\"true\"}t.shift();i+=\")})\";try{var \ns=Tr(e,function(){return Function(i)()},function(){return true});s.source=i;return s}catch(e){fe(re().body,\"htmx:syntax:error\",{error:e,source:i});return null}}}else if(o===\"[\"){n++}if(Qe(o,a,r)){i+=\"((\"+r+\".\"+o+\") ? (\"+r+\".\"+o+\") : (window.\"+o+\"))\"}else{i=i+o}a=t.shift()}}}function y(e,t){var \nr=\"\";while(e.length>0&&!t.test(e[0])){r+=e.shift()}return r}function tt(e){var t;if(e.length>0&&Ze.test(e[0])){e.shift();t=y(e,Ke).trim();e.shift()}else{t=y(e,x)}return t}var rt=\"input, textarea, select\";function nt(e,t,r){var n=[];var i=Ye(t);do{y(i,Je);var a=i.length;var \no=y(i,/[,\\[\\s]/);if(o!==\"\"){if(o===\"every\"){var s={trigger:\"every\"};y(i,Je);s.pollInterval=d(y(i,/[,\\[\\s]/));y(i,Je);var l=et(e,i,\"event\");if(l){s.eventFilter=l}n.push(s)}else if(o.indexOf(\"sse:\")===0){n.push({trigger:\"sse\",sseEvent:o.substr(4)})}else{var u={trigger:o};var \nl=et(e,i,\"event\");if(l){u.eventFilter=l}while(i.length>0&&i[0]!==\",\"){y(i,Je);var f=i.shift();if(f===\"changed\"){u.changed=true}else if(f===\"once\"){u.once=true}else if(f===\"consume\"){u.consume=true}else if(f===\"delay\"&&i[0]===\":\"){i.shift();u.delay=d(y(i,x))}else \nif(f===\"from\"&&i[0]===\":\"){i.shift();if(Ze.test(i[0])){var c=tt(i)}else{var c=y(i,x);if(c===\"closest\"||c===\"find\"||c===\"next\"||c===\"previous\"){i.shift();var h=tt(i);if(h.length>0){c+=\" \"+h}}}u.from=c}else if(f===\"target\"&&i[0]===\":\"){i.shift();u.target=tt(i)}else \nif(f===\"throttle\"&&i[0]===\":\"){i.shift();u.throttle=d(y(i,x))}else if(f===\"queue\"&&i[0]===\":\"){i.shift();u.queue=y(i,x)}else if(f===\"root\"&&i[0]===\":\"){i.shift();u[f]=tt(i)}else \nif(f===\"threshold\"&&i[0]===\":\"){i.shift();u[f]=y(i,x)}else{fe(e,\"htmx:syntax:error\",{token:i.shift()})}}n.push(u)}}if(i.length===a){fe(e,\"htmx:syntax:error\",{token:i.shift()})}y(i,Je)}while(i[0]===\",\"&&i.shift());if(r){r[t]=n}return n}function it(e){var t=te(e,\"hx-trigger\");var r=[];if(t){var \nn=Q.config.triggerSpecsCache;r=n&&n[t]||nt(e,t,n)}if(r.length>0){return r}else if(h(e,\"form\")){return[{trigger:\"submit\"}]}else if(h(e,'input[type=\"button\"], input[type=\"submit\"]')){return[{trigger:\"click\"}]}else if(h(e,rt)){return[{trigger:\"change\"}]}else{return[{trigger:\"click\"}]}}function \nat(e){ae(e).cancelled=true}function ot(e,t,r){var n=ae(e);n.timeout=setTimeout(function(){if(se(e)&&n.cancelled!==true){if(!ct(r,e,Wt(\"hx:poll:trigger\",{triggerSpec:r,target:e}))){t(e)}ot(e,t,r)}},r.pollInterval)}function st(e){return location.hostname===e.hostname&&ee(e,\"href\")&&ee(e,\"href\").indexOf(\"#\")!==0}function \nlt(t,r,e){if(t.tagName===\"A\"&&st(t)&&(t.target===\"\"||t.target===\"_self\")||t.tagName===\"FORM\"){r.boosted=true;var n,i;if(t.tagName===\"A\"){n=\"get\";i=ee(t,\"href\")}else{var \na=ee(t,\"method\");n=a?a.toLowerCase():\"get\";if(n===\"get\"){}i=ee(t,\"action\")}e.forEach(function(e){ht(t,function(e,t){if(v(e,Q.config.disableSelector)){p(e);return}he(n,i,e,t)},r,e,true)})}}function ut(e,t){if(e.type===\"submit\"||e.type===\"click\"){if(t.tagName===\"FORM\"){return true}if(h(t,'input[type=\"submit\"], \nbutton')&&v(t,\"form\")!==null){return true}if(t.tagName===\"A\"&&t.href&&(t.getAttribute(\"href\")===\"#\"||t.getAttribute(\"href\").indexOf(\"#\")!==0)){return true}}return false}function ft(e,t){return ae(e).boosted&&e.tagName===\"A\"&&t.type===\"click\"&&(t.ctrlKey||t.metaKey)}function ct(e,t,r){var \nn=e.eventFilter;if(n){try{return n.call(t,r)!==true}catch(e){fe(re().body,\"htmx:eventFilter:error\",{error:e,source:n.source});return true}}return false}function ht(a,o,e,s,l){var u=ae(a);var t;if(s.from){t=Z(a,s.from)}else{t=[a]}if(s.changed){t.forEach(function(e){var t=ae(e);t.lastValue=e.value})}oe(t,function(n){var \ni=function(e){if(!se(a)){n.removeEventListener(s.trigger,i);return}if(ft(a,e)){return}if(l||ut(e,a)){e.preventDefault()}if(ct(s,a,e)){return}var \nt=ae(e);t.triggerSpec=s;if(t.handledFor==null){t.handledFor=[]}if(t.handledFor.indexOf(a)<0){t.handledFor.push(a);if(s.consume){e.stopPropagation()}if(s.target&&e.target){if(!h(e.target,s.target)){return}}if(s.once){if(u.triggeredOnce){return}else{u.triggeredOnce=true}}if(s.changed){var \nr=ae(n);if(r.lastValue===n.value){return}r.lastValue=n.value}if(u.delayed){clearTimeout(u.delayed)}if(u.throttle){return}if(s.throttle>0){if(!u.throttle){o(a,e);u.throttle=setTimeout(function(){u.throttle=null},s.throttle)}}else \nif(s.delay>0){u.delayed=setTimeout(function(){o(a,e)},s.delay)}else{ce(a,\"htmx:trigger\");o(a,e)}}};if(e.listenerInfos==null){e.listenerInfos=[]}e.listenerInfos.push({trigger:s.trigger,listener:i,on:n});n.addEventListener(s.trigger,i)})}var vt=false;var dt=null;function \ngt(){if(!dt){dt=function(){vt=true};window.addEventListener(\"scroll\",dt);setInterval(function(){if(vt){vt=false;oe(re().querySelectorAll(\"[hx-trigger='revealed'],[data-hx-trigger='revealed']\"),function(e){mt(e)})}},200)}}function mt(t){if(!o(t,\"data-hx-revealed\")&&X(t)){t.setAttribute(\"data-hx-revealed\",\"true\");var \ne=ae(t);if(e.initHash){ce(t,\"revealed\")}else{t.addEventListener(\"htmx:afterProcessNode\",function(e){ce(t,\"revealed\")},{once:true})}}}function pt(e,t,r){var n=D(r);for(var i=0;i<n.length;i++){var a=n[i].split(/:(.+)/);if(a[0]===\"connect\"){xt(e,a[1],0)}if(a[0]===\"send\"){bt(e)}}}function \nxt(s,r,n){if(!se(s)){return}if(r.indexOf(\"/\")==0){var e=location.hostname+(location.port?\":\"+location.port:\"\");if(location.protocol==\"https:\"){r=\"wss://\"+e+r}else if(location.protocol==\"http:\"){r=\"ws://\"+e+r}}var \nt=Q.createWebSocket(r);t.onerror=function(e){fe(s,\"htmx:wsError\",{error:e,socket:t});yt(s)};t.onclose=function(e){if([1006,1012,1013].indexOf(e.code)>=0){var t=wt(n);setTimeout(function(){xt(s,r,n+1)},t)}};t.onopen=function(e){n=0};ae(s).webSocket=t;t.addEventListener(\"message\",function(e){if(yt(s)){return}var \nt=e.data;R(s,function(e){t=e.transformResponse(t,null,s)});var r=T(s);var n=l(t);var i=M(n.children);for(var a=0;a<i.length;a++){var o=i[a];Ee(te(o,\"hx-swap-oob\")||\"true\",o,r)}nr(r.tasks)})}function yt(e){if(!se(e)){ae(e).webSocket.close();return true}}function bt(u){var f=c(u,function(e){return \nae(e).webSocket!=null});if(f){u.addEventListener(it(u)[0].trigger,function(e){var t=ae(f).webSocket;var r=xr(u,f);var n=dr(u,\"post\");var i=n.errors;var a=n.values;var o=Hr(u);var s=le(a,o);var \nl=yr(s,u);l[\"HEADERS\"]=r;if(i&&i.length>0){ce(u,\"htmx:validation:halted\",i);return}t.send(JSON.stringify(l));if(ut(e,u)){e.preventDefault()}})}else{fe(u,\"htmx:noWebSocketSourceError\")}}function wt(e){var t=Q.config.wsReconnectDelay;if(typeof t===\"function\"){return t(e)}if(t===\"full-jitter\"){var r=Math.min(e,6);var \nn=1e3*Math.pow(2,r);return n*Math.random()}b('htmx.config.wsReconnectDelay must either be a function or the string \"full-jitter\"')}function St(e,t,r){var n=D(r);for(var i=0;i<n.length;i++){var a=n[i].split(/:(.+)/);if(a[0]===\"connect\"){Et(e,a[1])}if(a[0]===\"swap\"){Ct(e,a[1])}}}function Et(t,e){var \nr=Q.createEventSource(e);r.onerror=function(e){fe(t,\"htmx:sseError\",{error:e,source:r});Tt(t)};ae(t).sseEventSource=r}function Ct(a,o){var s=c(a,Ot);if(s){var l=ae(s).sseEventSource;var u=function(e){if(Tt(s)){return}if(!se(a)){l.removeEventListener(o,u);return}var \nt=e.data;R(a,function(e){t=e.transformResponse(t,null,a)});var r=wr(a);var n=ye(a);var i=T(a);je(r.swapStyle,n,a,t,i);nr(i.tasks);ce(a,\"htmx:sseMessage\",e)};ae(a).sseListener=u;l.addEventListener(o,u)}else{fe(a,\"htmx:noSSESourceError\")}}function Rt(e,t,r){var n=c(e,Ot);if(n){var i=ae(n).sseEventSource;var \na=function(){if(!Tt(n)){if(se(e)){t(e)}else{i.removeEventListener(r,a)}}};ae(e).sseListener=a;i.addEventListener(r,a)}else{fe(e,\"htmx:noSSESourceError\")}}function Tt(e){if(!se(e)){ae(e).sseEventSource.close();return true}}function Ot(e){return ae(e).sseEventSource!=null}function qt(e,t,r,n){var \ni=function(){if(!r.loaded){r.loaded=true;t(e)}};if(n>0){setTimeout(i,n)}else{i()}}function Ht(t,i,e){var a=false;oe(w,function(r){if(o(t,\"hx-\"+r)){var n=te(t,\"hx-\"+r);a=true;i.path=n;i.verb=r;e.forEach(function(e){Lt(t,e,i,function(e,t){if(v(e,Q.config.disableSelector)){p(e);return}he(r,n,e,t)})})}});return a}function \nLt(n,e,t,r){if(e.sseEvent){Rt(n,r,e.sseEvent)}else if(e.trigger===\"revealed\"){gt();ht(n,r,t,e);mt(n)}else if(e.trigger===\"intersect\"){var i={};if(e.root){i.root=ue(n,e.root)}if(e.threshold){i.threshold=parseFloat(e.threshold)}var a=new IntersectionObserver(function(e){for(var t=0;t<e.length;t++){var \nr=e[t];if(r.isIntersecting){ce(n,\"intersect\");break}}},i);a.observe(n);ht(n,r,t,e)}else if(e.trigger===\"load\"){if(!ct(e,n,Wt(\"load\",{elt:n}))){qt(n,r,t,e.delay)}}else if(e.pollInterval>0){t.polling=true;ot(n,r,e)}else{ht(n,r,t,e)}}function \nAt(e){if(Q.config.allowScriptTags&&(e.type===\"text/javascript\"||e.type===\"module\"||e.type===\"\")){var t=re().createElement(\"script\");oe(e.attributes,function(e){t.setAttribute(e.name,e.value)});t.textContent=e.textContent;t.async=false;if(Q.config.inlineScriptNonce){t.nonce=Q.config.inlineScriptNonce}var \nr=e.parentElement;try{r.insertBefore(t,e)}catch(e){b(e)}finally{if(e.parentElement){e.parentElement.removeChild(e)}}}}function Nt(e){if(h(e,\"script\")){At(e)}oe(f(e,\"script\"),function(e){At(e)})}function It(e){var t=e.attributes;for(var r=0;r<t.length;r++){var \nn=t[r].name;if(s(n,\"hx-on:\")||s(n,\"data-hx-on:\")||s(n,\"hx-on-\")||s(n,\"data-hx-on-\")){return true}}return false}function kt(e){var t=null;var r=[];if(It(e)){r.push(e)}if(document.evaluate){var n=document.evaluate('.//*[@*[ starts-with(name(), \"hx-on:\") or starts-with(name(), \"data-hx-on:\") or'+' starts-with(name(), \n\"hx-on-\") or starts-with(name(), \"data-hx-on-\") ]]',e);while(t=n.iterateNext())r.push(t)}else{var i=e.getElementsByTagName(\"*\");for(var a=0;a<i.length;a++){if(It(i[a])){r.push(i[a])}}}return r}function Pt(e){if(e.querySelectorAll){var t=\", [hx-boost] a, [data-hx-boost] a, a[hx-boost], a[data-hx-boost]\";var \nr=e.querySelectorAll(i+t+\", form, [type='submit'], [hx-sse], [data-hx-sse], [hx-ws],\"+\" [data-hx-ws], [hx-ext], [data-hx-ext], [hx-trigger], [data-hx-trigger], [hx-on], [data-hx-on]\");return r}else{return[]}}function Mt(e){var t=v(e.target,\"button, input[type='submit']\");var \n\nr=Dt(e);if(r){r.lastButtonClicked=t}}function Xt(e){var t=Dt(e);if(t){t.lastButtonClicked=null}}function Dt(e){var t=v(e.target,\"button, input[type='submit']\");if(!t){return}var r=g(\"#\"+ee(t,\"form\"))||v(t,\"form\");if(!r){return}return ae(r)}function \nUt(e){e.addEventListener(\"click\",Mt);e.addEventListener(\"focusin\",Mt);e.addEventListener(\"focusout\",Xt)}function Bt(e){var t=Ye(e);var r=0;for(var n=0;n<t.length;n++){const i=t[n];if(i===\"{\"){r++}else if(i===\"}\"){r--}}return r}function Ft(t,e,r){var n=ae(t);if(!Array.isArray(n.onHandlers)){n.onHandlers=[]}var i;var \na=function(e){return Tr(t,function(){if(!i){i=new Function(\"event\",r)}i.call(t,e)})};t.addEventListener(e,a);n.onHandlers.push({event:e,listener:a})}function Vt(e){var t=te(e,\"hx-on\");if(t){var r={};var n=t.split(\"\\n\");var i=null;var a=0;while(n.length>0){var o=n.shift();var \ns=o.match(/^\\s*([a-zA-Z:\\-\\.]+:)(.*)/);if(a===0&&s){o.split(\":\");i=s[1].slice(0,-1);r[i]=s[2]}else{r[i]+=o}a+=Bt(o)}for(var l in r){Ft(e,l,r[l])}}}function jt(e){Ae(e);for(var t=0;t<e.attributes.length;t++){var r=e.attributes[t].name;var n=e.attributes[t].value;if(s(r,\"hx-on\")||s(r,\"data-hx-on\")){var \ni=r.indexOf(\"-on\")+3;var a=r.slice(i,i+1);if(a===\"-\"||a===\":\"){var o=r.slice(i+1);if(s(o,\":\")){o=\"htmx\"+o}else if(s(o,\"-\")){o=\"htmx:\"+o.slice(1)}else if(s(o,\"htmx-\")){o=\"htmx:\"+o.slice(5)}Ft(e,o,n)}}}}function _t(t){if(v(t,Q.config.disableSelector)){p(t);return}var \nr=ae(t);if(r.initHash!==Le(t)){Ne(t);r.initHash=Le(t);Vt(t);ce(t,\"htmx:beforeProcessNode\");if(t.value){r.lastValue=t.value}var e=it(t);var n=Ht(t,r,e);if(!n){if(ne(t,\"hx-boost\")===\"true\"){lt(t,r,e)}else \nif(o(t,\"hx-trigger\")){e.forEach(function(e){Lt(t,e,r,function(){})})}}if(t.tagName===\"FORM\"||ee(t,\"type\")===\"submit\"&&o(t,\"form\")){Ut(t)}var i=te(t,\"hx-sse\");if(i){St(t,r,i)}var a=te(t,\"hx-ws\");if(a){pt(t,r,a)}ce(t,\"htmx:afterProcessNode\")}}function \nzt(e){e=g(e);if(v(e,Q.config.disableSelector)){p(e);return}_t(e);oe(Pt(e),function(e){_t(e)});oe(kt(e),jt)}function $t(e){return e.replace(/([a-z0-9])([A-Z])/g,\"$1-$2\").toLowerCase()}function Wt(e,t){var r;if(window.CustomEvent&&typeof window.CustomEvent===\"function\"){r=new \nCustomEvent(e,{bubbles:true,cancelable:true,detail:t})}else{r=re().createEvent(\"CustomEvent\");r.initCustomEvent(e,true,true,t)}return r}function fe(e,t,r){ce(e,t,le({error:t},r))}function Gt(e){return e===\"htmx:afterProcessNode\"}function R(e,t){oe(Fr(e),function(e){try{t(e)}catch(e){b(e)}})}function \nb(e){if(console.error){console.error(e)}else if(console.log){console.log(\"ERROR: \",e)}}function ce(e,t,r){e=g(e);if(r==null){r={}}r[\"elt\"]=e;var n=Wt(t,r);if(Q.logger&&!Gt(t)){Q.logger(e,t,r)}if(r.error){b(r.error);ce(e,\"htmx:error\",{errorInfo:r})}var i=e.dispatchEvent(n);var a=$t(t);if(i&&a!==t){var \no=Wt(a,n.detail);i=i&&e.dispatchEvent(o)}R(e,function(e){i=i&&(e.onEvent(t,n)!==false&&!n.defaultPrevented)});return i}var Jt=location.pathname+location.search;function Zt(){var e=re().querySelector(\"[hx-history-elt],[data-hx-history-elt]\");return e||re().body}function \nKt(e,t,r,n){if(!U()){return}if(Q.config.historyCacheSize<=0){localStorage.removeItem(\"htmx-history-cache\");return}e=B(e);var i=E(localStorage.getItem(\"htmx-history-cache\"))||[];for(var a=0;a<i.length;a++){if(i[a].url===e){i.splice(a,1);break}}var \no={url:e,content:t,title:r,scroll:n};ce(re().body,\"htmx:historyItemCreated\",{item:o,cache:i});i.push(o);while(i.length>Q.config.historyCacheSize){i.shift()}while(i.length>0){try{localStorage.setItem(\"htmx-history-cache\",JSON.stringify(i));break}catch(e){fe(re().body,\"htmx:historyCacheError\",{cause:e,cache:i});i.shift(\n)}}}function Yt(e){if(!U()){return null}e=B(e);var t=E(localStorage.getItem(\"htmx-history-cache\"))||[];for(var r=0;r<t.length;r++){if(t[r].url===e){return t[r]}}return null}function Qt(e){var t=Q.config.requestClass;var r=e.cloneNode(true);oe(f(r,\".\"+t),function(e){n(e,t)});return r.innerHTML}function er(){var \ne=Zt();var t=Jt||location.pathname+location.search;var r;try{r=re().querySelector('[hx-history=\"false\" i],[data-hx-history=\"false\" \ni]')}catch(e){r=re().querySelector('[hx-history=\"false\"],[data-hx-history=\"false\"]')}if(!r){ce(re().body,\"htmx:beforeHistorySave\",{path:t,historyElt:e});Kt(t,Qt(e),re().title,window.scrollY)}if(Q.config.historyEnabled)history.replaceState({htmx:true},re().title,window.location.href)}function \ntr(e){if(Q.config.getCacheBusterParam){e=e.replace(/org\\.htmx\\.cache-buster=[^&]*&?/,\"\");if(G(e,\"&\")||G(e,\"?\")){e=e.slice(0,-1)}}if(Q.config.historyEnabled){history.pushState({htmx:true},\"\",e)}Jt=e}function rr(e){if(Q.config.historyEnabled)history.replaceState({htmx:true},\"\",e);Jt=e}function \nnr(e){oe(e,function(e){e.call()})}function ir(a){var e=new XMLHttpRequest;var \no={path:a,xhr:e};ce(re().body,\"htmx:historyCacheMiss\",o);e.open(\"GET\",a,true);e.setRequestHeader(\"HX-Request\",\"true\");e.setRequestHeader(\"HX-History-Restore-Request\",\"true\");e.setRequestHeader(\"HX-Current-URL\",re().location.href);e.onload=function(){if(this.status>=200&&this.status<400){ce(re().body,\"htmx:historyCache\nMissLoad\",o);var e=l(this.response);e=e.querySelector(\"[hx-history-elt],[data-hx-history-elt]\")||e;var t=Zt();var r=T(t);var n=Ve(this.response);if(n){var \ni=C(\"title\");if(i){i.innerHTML=n}else{window.document.title=n}}Ue(t,e,r);nr(r.tasks);Jt=a;ce(re().body,\"htmx:historyRestore\",{path:a,cacheMiss:true,serverResponse:this.response})}else{fe(re().body,\"htmx:historyCacheMissLoadError\",o)}};e.send()}function ar(e){er();e=e||location.pathname+location.search;var \nt=Yt(e);if(t){var r=l(t.content);var n=Zt();var i=T(n);Ue(n,r,i);nr(i.tasks);document.title=t.title;setTimeout(function(){window.scrollTo(0,t.scroll)},0);Jt=e;ce(re().body,\"htmx:historyRestore\",{path:e,item:t})}else{if(Q.config.refreshOnHistoryMiss){window.location.reload(true)}else{ir(e)}}}function or(e){var \nt=pe(e,\"hx-indicator\");if(t==null){t=[e]}oe(t,function(e){var t=ae(e);t.requestCount=(t.requestCount||0)+1;e.classList[\"add\"].call(e.classList,Q.config.requestClass)});return t}function sr(e){var t=pe(e,\"hx-disabled-elt\");if(t==null){t=[]}oe(t,function(e){var \nt=ae(e);t.requestCount=(t.requestCount||0)+1;e.setAttribute(\"disabled\",\"\")});return t}function lr(e,t){oe(e,function(e){var t=ae(e);t.requestCount=(t.requestCount||0)-1;if(t.requestCount===0){e.classList[\"remove\"].call(e.classList,Q.config.requestClass)}});oe(t,function(e){var \nt=ae(e);t.requestCount=(t.requestCount||0)-1;if(t.requestCount===0){e.removeAttribute(\"disabled\")}})}function ur(e,t){for(var r=0;r<e.length;r++){var n=e[r];if(n.isSameNode(t)){return true}}return false}function fr(e){if(e.name===\"\"||e.name==null||e.disabled||v(e,\"fieldset[disabled]\")){return \nfalse}if(e.type===\"button\"||e.type===\"submit\"||e.tagName===\"image\"||e.tagName===\"reset\"||e.tagName===\"file\"){return false}if(e.type===\"checkbox\"||e.type===\"radio\"){return e.checked}return true}function cr(e,t,r){if(e!=null&&t!=null){var n=r[e];if(n===undefined){r[e]=t}else \nif(Array.isArray(n)){if(Array.isArray(t)){r[e]=n.concat(t)}else{n.push(t)}}else{if(Array.isArray(t)){r[e]=[n].concat(t)}else{r[e]=[n,t]}}}}function hr(t,r,n,e,i){if(e==null||ur(t,e)){return}else{t.push(e)}if(fr(e)){var a=ee(e,\"name\");var \no=e.value;if(e.multiple&&e.tagName===\"SELECT\"){o=M(e.querySelectorAll(\"option:checked\")).map(function(e){return e.value})}if(e.files){o=M(e.files)}cr(a,o,r);if(i){vr(e,n)}}if(h(e,\"form\")){var s=e.elements;oe(s,function(e){hr(t,r,n,e,i)})}}function \nvr(e,t){if(e.willValidate){ce(e,\"htmx:validation:validate\");if(!e.checkValidity()){t.push({elt:e,message:e.validationMessage,validity:e.validity});ce(e,\"htmx:validation:failed\",{message:e.validationMessage,validity:e.validity})}}}function dr(e,t){var r=[];var n={};var i={};var a=[];var \no=ae(e);if(o.lastButtonClicked&&!se(o.lastButtonClicked)){o.lastButtonClicked=null}var \ns=h(e,\"form\")&&e.noValidate!==true||te(e,\"hx-validate\")===\"true\";if(o.lastButtonClicked){s=s&&o.lastButtonClicked.formNoValidate!==true}if(t!==\"get\"){hr(r,i,a,v(e,\"form\"),s)}hr(r,n,a,e,s);if(o.lastButtonClicked||e.tagName===\"BUTTON\"||e.tagName===\"INPUT\"&&ee(e,\"type\")===\"submit\"){var l=o.lastButtonClicked||e;var \nu=ee(l,\"name\");cr(u,l.value,i)}var f=pe(e,\"hx-include\");oe(f,function(e){hr(r,n,a,e,s);if(!h(e,\"form\")){oe(e.querySelectorAll(rt),function(e){hr(r,n,a,e,s)})}});n=le(n,i);return{errors:a,values:n}}function gr(e,t,r){if(e!==\"\"){e+=\"&\"}if(String(r)===\"[object Object]\"){r=JSON.stringify(r)}var \nn=encodeURIComponent(r);e+=encodeURIComponent(t)+\"=\"+n;return e}function mr(e){var t=\"\";for(var r in e){if(e.hasOwnProperty(r)){var n=e[r];if(Array.isArray(n)){oe(n,function(e){t=gr(t,r,e)})}else{t=gr(t,r,n)}}}return t}function pr(e){var t=new FormData;for(var r in e){if(e.hasOwnProperty(r)){var \nn=e[r];if(Array.isArray(n)){oe(n,function(e){t.append(r,e)})}else{t.append(r,n)}}}return t}function xr(e,t,r){var \nn={\"HX-Request\":\"true\",\"HX-Trigger\":ee(e,\"id\"),\"HX-Trigger-Name\":ee(e,\"name\"),\"HX-Target\":te(t,\"id\"),\"HX-Current-URL\":re().location.href};Rr(e,\"hx-headers\",false,n);if(r!==undefined){n[\"HX-Prompt\"]=r}if(ae(e).boosted){n[\"HX-Boosted\"]=\"true\"}return n}function yr(t,e){var \nr=ne(e,\"hx-params\");if(r){if(r===\"none\"){return{}}else if(r===\"*\"){return t}else if(r.indexOf(\"not \")===0){oe(r.substr(4).split(\",\"),function(e){e=e.trim();delete t[e]});return t}else{var n={};oe(r.split(\",\"),function(e){e=e.trim();n[e]=t[e]});return n}}else{return t}}function br(e){return \nee(e,\"href\")&&ee(e,\"href\").indexOf(\"#\")>=0}function wr(e,t){var r=t?t:ne(e,\"hx-swap\");var n={swapStyle:ae(e).boosted?\"innerHTML\":Q.config.defaultSwapStyle,swapDelay:Q.config.defaultSwapDelay,settleDelay:Q.config.defaultSettleDelay};if(Q.config.scrollIntoViewOnBoost&&ae(e).boosted&&!br(e)){n[\"show\"]=\"top\"}if(r){var \ni=D(r);if(i.length>0){for(var a=0;a<i.length;a++){var o=i[a];if(o.indexOf(\"swap:\")===0){n[\"swapDelay\"]=d(o.substr(5))}else if(o.indexOf(\"settle:\")===0){n[\"settleDelay\"]=d(o.substr(7))}else if(o.indexOf(\"transition:\")===0){n[\"transition\"]=o.substr(11)===\"true\"}else \nif(o.indexOf(\"ignoreTitle:\")===0){n[\"ignoreTitle\"]=o.substr(12)===\"true\"}else if(o.indexOf(\"scroll:\")===0){var s=o.substr(7);var l=s.split(\":\");var u=l.pop();var f=l.length>0?l.join(\":\"):null;n[\"scroll\"]=u;n[\"scrollTarget\"]=f}else if(o.indexOf(\"show:\")===0){var c=o.substr(5);var l=c.split(\":\");var h=l.pop();var \nf=l.length>0?l.join(\":\"):null;n[\"show\"]=h;n[\"showTarget\"]=f}else if(o.indexOf(\"focus-scroll:\")===0){var v=o.substr(\"focus-scroll:\".length);n[\"focusScroll\"]=v==\"true\"}else if(a==0){n[\"swapStyle\"]=o}else{b(\"Unknown modifier in hx-swap: \"+o)}}}}return n}function Sr(e){return \nne(e,\"hx-encoding\")===\"multipart/form-data\"||h(e,\"form\")&&ee(e,\"enctype\")===\"multipart/form-data\"}function Er(t,r,n){var i=null;R(r,function(e){if(i==null){i=e.encodeParameters(t,n,r)}});if(i!=null){return i}else{if(Sr(r)){return pr(n)}else{return mr(n)}}}function T(e){return{tasks:[],elts:[e]}}function Cr(e,t){var \nr=e[0];var n=e[e.length-1];if(t.scroll){var i=null;if(t.scrollTarget){i=ue(r,t.scrollTarget)}if(t.scroll===\"top\"&&(r||i)){i=i||r;i.scrollTop=0}if(t.scroll===\"bottom\"&&(n||i)){i=i||n;i.scrollTop=i.scrollHeight}}if(t.show){var i=null;if(t.showTarget){var \na=t.showTarget;if(t.showTarget===\"window\"){a=\"body\"}i=ue(r,a)}if(t.show===\"top\"&&(r||i)){i=i||r;i.scrollIntoView({block:\"start\",behavior:Q.config.scrollBehavior})}if(t.show===\"bottom\"&&(n||i)){i=i||n;i.scrollIntoView({block:\"end\",behavior:Q.config.scrollBehavior})}}}function \nRr(e,t,r,n){if(n==null){n={}}if(e==null){return n}var i=te(e,t);if(i){var a=i.trim();var o=r;if(a===\"unset\"){return null}if(a.indexOf(\"javascript:\")===0){a=a.substr(11);o=true}else if(a.indexOf(\"js:\")===0){a=a.substr(3);o=true}if(a.indexOf(\"{\")!==0){a=\"{\"+a+\"}\"}var s;if(o){s=Tr(e,function(){return Function(\"return \n(\"+a+\")\")()},{})}else{s=E(a)}for(var l in s){if(s.hasOwnProperty(l)){if(n[l]==null){n[l]=s[l]}}}}return Rr(u(e),t,r,n)}function Tr(e,t,r){if(Q.config.allowEval){return t()}else{fe(e,\"htmx:evalDisallowedError\");return r}}function Or(e,t){return Rr(e,\"hx-vars\",true,t)}function qr(e,t){return \nRr(e,\"hx-vals\",false,t)}function Hr(e){return le(Or(e),qr(e))}function Lr(t,r,n){if(n!==null){try{t.setRequestHeader(r,n)}catch(e){t.setRequestHeader(r,encodeURIComponent(n));t.setRequestHeader(r+\"-URI-AutoEncoded\",\"true\")}}}function Ar(t){if(t.responseURL&&typeof URL!==\"undefined\"){try{var e=new \nURL(t.responseURL);return e.pathname+e.search}catch(e){fe(re().body,\"htmx:badResponseUrl\",{url:t.responseURL})}}}function O(e,t){return t.test(e.getAllResponseHeaders())}function Nr(e,t,r){e=e.toLowerCase();if(r){if(r instanceof Element||I(r,\"String\")){return \nhe(e,t,null,null,{targetOverride:g(r),returnPromise:true})}else{return he(e,t,g(r.source),r.event,{handler:r.handler,headers:r.headers,values:r.values,targetOverride:g(r.target),swapOverride:r.swap,select:r.select,returnPromise:true})}}else{return he(e,t,null,null,{returnPromise:true})}}function Ir(e){var \nt=[];while(e){t.push(e);e=e.parentElement}return t}function kr(e,t,r){var n;var i;if(typeof URL===\"function\"){i=new URL(t,document.location.href);var a=document.location.origin;n=a===i.origin}else{i=t;n=s(t,document.location.origin)}if(Q.config.selfRequestsOnly){if(!n){return false}}return \nce(e,\"htmx:validateUrl\",le({url:i,sameHost:n},r))}function he(t,r,n,i,a,e){var o=null;var s=null;a=a!=null?a:{};if(a.returnPromise&&typeof Promise!==\"undefined\"){var l=new Promise(function(e,t){o=e;s=t})}if(n==null){n=re().body}var M=a.handler||Mr;var X=a.select||null;if(!se(n)){ie(o);return l}var \nu=a.targetOverride||ye(n);if(u==null||u==me){fe(n,\"htmx:targetError\",{target:te(n,\"hx-target\")});ie(s);return l}var f=ae(n);var c=f.lastButtonClicked;if(c){var h=ee(c,\"formaction\");if(h!=null){r=h}var v=ee(c,\"formmethod\");if(v!=null){if(v.toLowerCase()!==\"dialog\"){t=v}}}var d=ne(n,\"hx-confirm\");if(e===undefined){var \nD=function(e){return he(t,r,n,i,a,!!e)};var U={target:u,elt:n,path:r,verb:t,triggeringEvent:i,etc:a,issueRequest:D,question:d};if(ce(n,\"htmx:confirm\",U)===false){ie(o);return l}}var g=n;var m=ne(n,\"hx-sync\");var p=null;var x=false;if(m){var B=m.split(\":\");var \nF=B[0].trim();if(F===\"this\"){g=xe(n,\"hx-sync\")}else{g=ue(n,F)}m=(B[1]||\"drop\").trim();f=ae(g);if(m===\"drop\"&&f.xhr&&f.abortable!==true){ie(o);return l}else if(m===\"abort\"){if(f.xhr){ie(o);return l}else{x=true}}else if(m===\"replace\"){ce(g,\"htmx:abort\")}else if(m.indexOf(\"queue\")===0){var V=m.split(\" \n\");p=(V[1]||\"last\").trim()}}if(f.xhr){if(f.abortable){ce(g,\"htmx:abort\")}else{if(p==null){if(i){var \ny=ae(i);if(y&&y.triggerSpec&&y.triggerSpec.queue){p=y.triggerSpec.queue}}if(p==null){p=\"last\"}}if(f.queuedRequests==null){f.queuedRequests=[]}if(p===\"first\"&&f.queuedRequests.length===0){f.queuedRequests.push(function(){he(t,r,n,i,a)})}else if(p===\"all\"){f.queuedRequests.push(function(){he(t,r,n,i,a)})}else \nif(p===\"last\"){f.queuedRequests=[];f.queuedRequests.push(function(){he(t,r,n,i,a)})}ie(o);return l}}var b=new XMLHttpRequest;f.xhr=b;f.abortable=x;var w=function(){f.xhr=null;f.abortable=false;if(f.queuedRequests!=null&&f.queuedRequests.length>0){var e=f.queuedRequests.shift();e()}};var j=ne(n,\"hx-prompt\");if(j){var \nS=prompt(j);if(S===null||!ce(n,\"htmx:prompt\",{prompt:S,target:u})){ie(o);w();return l}}if(d&&!e){if(!confirm(d)){ie(o);w();return l}}var E=xr(n,u,S);if(t!==\"get\"&&!Sr(n)){E[\"Content-Type\"]=\"application/x-www-form-urlencoded\"}if(a.headers){E=le(E,a.headers)}var _=dr(n,t);var C=_.errors;var \nR=_.values;if(a.values){R=le(R,a.values)}var z=Hr(n);var $=le(R,z);var T=yr($,n);if(Q.config.getCacheBusterParam&&t===\"get\"){T[\"org.htmx.cache-buster\"]=ee(u,\"id\")||\"true\"}if(r==null||r===\"\"){r=re().location.href}var O=Rr(n,\"hx-request\");var W=ae(n).boosted;var q=Q.config.methodsThatUseUrlParams.indexOf(t)>=0;var \nH={boosted:W,useUrlParams:q,parameters:T,unfilteredParameters:$,headers:E,target:u,verb:t,errors:C,withCredentials:a.credentials||O.credentials||Q.config.withCredentials,timeout:a.timeout||O.timeout||Q.config.timeout,path:r,triggeringEvent:i};if(!ce(n,\"htmx:configRequest\",H)){ie(o);w();return \nl}r=H.path;t=H.verb;E=H.headers;T=H.parameters;C=H.errors;q=H.useUrlParams;if(C&&C.length>0){ce(n,\"htmx:validation:halted\",H);ie(o);w();return l}var G=r.split(\"#\");var J=G[0];var L=G[1];var A=r;if(q){A=J;var \nZ=Object.keys(T).length!==0;if(Z){if(A.indexOf(\"?\")<0){A+=\"?\"}else{A+=\"&\"}A+=mr(T);if(L){A+=\"#\"+L}}}if(!kr(n,A,H)){fe(n,\"htmx:invalidPath\",H);ie(s);return l}b.open(t.toUpperCase(),A,true);b.overrideMimeType(\"text/html\");b.withCredentials=H.withCredentials;b.timeout=H.timeout;if(O.noHeaders){}else{for(var N in \nE){if(E.hasOwnProperty(N)){var K=E[N];Lr(b,N,K)}}}var I={xhr:b,target:u,requestConfig:H,etc:a,boosted:W,select:X,pathInfo:{requestPath:r,finalRequestPath:A,anchor:L}};b.onload=function(){try{var e=Ir(n);I.pathInfo.responsePath=Ar(b);M(n,I);lr(k,P);ce(n,\"htmx:afterRequest\",I);ce(n,\"htmx:afterOnLoad\",I);if(!se(n)){var \nt=null;while(e.length>0&&t==null){var r=e.shift();if(se(r)){t=r}}if(t){ce(t,\"htmx:afterRequest\",I);ce(t,\"htmx:afterOnLoad\",I)}}ie(o);w()}catch(e){fe(n,\"htmx:onLoadError\",le({error:e},I));throw \ne}};b.onerror=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:sendError\",I);ie(s);w()};b.onabort=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:sendAbort\",I);ie(s);w()};b.ontimeout=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:timeout\",I);ie(s);w()};if(!ce(n,\"htmx:beforeRequest\",I)){ie\n(o);w();return l}var k=or(n);var P=sr(n);oe([\"loadstart\",\"loadend\",\"progress\",\"abort\"],function(t){oe([b,b.upload],function(e){e.addEventListener(t,function(e){ce(n,\"htmx:xhr:\"+t,{lengthComputable:e.lengthComputable,loaded:e.loaded,total:e.total})})})});ce(n,\"htmx:beforeSend\",I);var Y=q?null:Er(b,n,T);b.send(Y);return \nl}function Pr(e,t){var r=t.xhr;var n=null;var i=null;if(O(r,/HX-Push:/i)){n=r.getResponseHeader(\"HX-Push\");i=\"push\"}else if(O(r,/HX-Push-Url:/i)){n=r.getResponseHeader(\"HX-Push-Url\");i=\"push\"}else \nif(O(r,/HX-Replace-Url:/i)){n=r.getResponseHeader(\"HX-Replace-Url\");i=\"replace\"}if(n){if(n===\"false\"){return{}}else{return{type:i,path:n}}}var a=t.pathInfo.finalRequestPath;var o=t.pathInfo.responsePath;var s=ne(e,\"hx-push-url\");var l=ne(e,\"hx-replace-url\");var u=ae(e).boosted;var f=null;var \nc=null;if(s){f=\"push\";c=s}else if(l){f=\"replace\";c=l}else if(u){f=\"push\";c=o||a}if(c){if(c===\"false\"){return{}}if(c===\"true\"){c=o||a}if(t.pathInfo.anchor&&c.indexOf(\"#\")===-1){c=c+\"#\"+t.pathInfo.anchor}return{type:f,path:c}}else{return{}}}function Mr(l,u){var f=u.xhr;var c=u.target;var e=u.etc;var \nt=u.requestConfig;var h=u.select;if(!ce(l,\"htmx:beforeOnLoad\",u))return;if(O(f,/HX-Trigger:/i)){_e(f,\"HX-Trigger\",l)}if(O(f,/HX-Location:/i)){er();var r=f.getResponseHeader(\"HX-Location\");var v;if(r.indexOf(\"{\")===0){v=E(r);r=v[\"path\"];delete v[\"path\"]}Nr(\"GET\",r,v).then(function(){tr(r)});return}var \nn=O(f,/HX-Refresh:/i)&&\"true\"===f.getResponseHeader(\"HX-Refresh\");if(O(f,/HX-Redirect:/i)){location.href=f.getResponseHeader(\"HX-Redirect\");n&&location.reload();return}if(n){location.reload();return}if(O(f,/HX-Retarget:/i)){if(f.getResponseHeader(\"HX-Retarget\")===\"this\"){u.target=l}else{u.target=ue(l,f.getResponseHead\ner(\"HX-Retarget\"))}}var d=Pr(l,u);var i=f.status>=200&&f.status<400&&f.status!==204;var g=f.response;var a=f.status>=400;var m=Q.config.ignoreTitle;var \no=le({shouldSwap:i,serverResponse:g,isError:a,ignoreTitle:m},u);if(!ce(c,\"htmx:beforeSwap\",o))return;c=o.target;g=o.serverResponse;a=o.isError;m=o.ignoreTitle;u.target=c;u.failed=a;u.successful=!a;if(o.shouldSwap){if(f.status===286){at(l)}R(l,function(e){g=e.transformResponse(g,f,l)});if(d.type){er()}var \ns=e.swapOverride;if(O(f,/HX-Reswap:/i)){s=f.getResponseHeader(\"HX-Reswap\")}var v=wr(l,s);if(v.hasOwnProperty(\"ignoreTitle\")){m=v.ignoreTitle}c.classList.add(Q.config.swappingClass);var p=null;var x=null;var y=function(){try{var e=document.activeElement;var \nt={};try{t={elt:e,start:e?e.selectionStart:null,end:e?e.selectionEnd:null}}catch(e){}var \nr;if(h){r=h}if(O(f,/HX-Reselect:/i)){r=f.getResponseHeader(\"HX-Reselect\")}if(d.type){ce(re().body,\"htmx:beforeHistoryUpdate\",le({history:d},u));if(d.type===\"push\"){tr(d.path);ce(re().body,\"htmx:pushedIntoHistory\",{path:d.path})}else{rr(d.path);ce(re().body,\"htmx:replacedInHistory\",{path:d.path})}}var \nn=T(c);je(v.swapStyle,c,l,g,n,r);if(t.elt&&!se(t.elt)&&ee(t.elt,\"id\")){var i=document.getElementById(ee(t.elt,\"id\"));var \na={preventScroll:v.focusScroll!==undefined?!v.focusScroll:!Q.config.defaultFocusScroll};if(i){if(t.start&&i.setSelectionRange){try{i.setSelectionRange(t.start,t.end)}catch(e){}}i.focus(a)}}c.classList.remove(Q.config.swappingClass);oe(n.elts,function(e){if(e.classList){e.classList.add(Q.config.settlingClass)}ce(e,\"htm\nx:afterSwap\",u)});if(O(f,/HX-Trigger-After-Swap:/i)){var o=l;if(!se(l)){o=re().body}_e(f,\"HX-Trigger-After-Swap\",o)}var s=function(){oe(n.tasks,function(e){e.call()});oe(n.elts,function(e){if(e.classList){e.classList.remove(Q.config.settlingClass)}ce(e,\"htmx:afterSettle\",u)});if(u.pathInfo.anchor){var \ne=re().getElementById(u.pathInfo.anchor);if(e){e.scrollIntoView({block:\"start\",behavior:\"auto\"})}}if(n.title&&!m){var t=C(\"title\");if(t){t.innerHTML=n.title}else{window.document.title=n.title}}Cr(n.elts,v);if(O(f,/HX-Trigger-After-Settle:/i)){var \nr=l;if(!se(l)){r=re().body}_e(f,\"HX-Trigger-After-Settle\",r)}ie(p)};if(v.settleDelay>0){setTimeout(s,v.settleDelay)}else{s()}}catch(e){fe(l,\"htmx:swapError\",u);ie(x);throw e}};var b=Q.config.globalViewTransitions;if(v.hasOwnProperty(\"transition\")){b=v.transition}if(b&&ce(l,\"htmx:beforeTransition\",u)&&typeof \nPromise!==\"undefined\"&&document.startViewTransition){var w=new Promise(function(e,t){p=e;x=t});var S=y;y=function(){document.startViewTransition(function(){S();return w})}}if(v.swapDelay>0){setTimeout(y,v.swapDelay)}else{y()}}if(a){fe(l,\"htmx:responseError\",le({error:\"Response Status Error Code \"+f.status+\" from \n\"+u.pathInfo.requestPath},u))}}var Xr={};function Dr(){return{init:function(e){return null},onEvent:function(e,t){return true},transformResponse:function(e,t,r){return e},isInlineSwap:function(e){return false},handleSwap:function(e,t,r,n){return false},encodeParameters:function(e,t,r){return null}}}function \nUr(e,t){if(t.init){t.init(r)}Xr[e]=le(Dr(),t)}function Br(e){delete Xr[e]}function Fr(e,r,n){if(e==undefined){return r}if(r==undefined){r=[]}if(n==undefined){n=[]}var t=te(e,\"hx-ext\");if(t){oe(t.split(\",\"),function(e){e=e.replace(/ /g,\"\");if(e.slice(0,7)==\"ignore:\"){n.push(e.slice(7));return}if(n.indexOf(e)<0){var \nt=Xr[e];if(t&&r.indexOf(t)<0){r.push(t)}}})}return Fr(u(e),r,n)}var Vr=false;re().addEventListener(\"DOMContentLoaded\",function(){Vr=true});function jr(e){if(Vr||re().readyState===\"complete\"){e()}else{re().addEventListener(\"DOMContentLoaded\",e)}}function \n_r(){if(Q.config.includeIndicatorStyles!==false){re().head.insertAdjacentHTML(\"beforeend\",\"<style>                      .\"+Q.config.indicatorClass+\"{opacity:0}                      .\"+Q.config.requestClass+\" .\"+Q.config.indicatorClass+\"{opacity:1; transition: opacity 200ms ease-in;}                      \n.\"+Q.config.requestClass+\".\"+Q.config.indicatorClass+\"{opacity:1; transition: opacity 200ms ease-in;}                    </style>\")}}function zr(){var e=re().querySelector('meta[name=\"htmx-config\"]');if(e){return E(e.content)}else{return null}}function $r(){var \ne=zr();if(e){Q.config=le(Q.config,e)}}jr(function(){$r();_r();var e=re().body;zt(e);var t=re().querySelectorAll(\"[hx-trigger='restored'],[data-hx-trigger='restored']\");e.addEventListener(\"htmx:abort\",function(e){var t=e.target;var r=ae(t);if(r&&r.xhr){r.xhr.abort()}});const \nr=window.onpopstate?window.onpopstate.bind(window):null;window.onpopstate=function(e){if(e.state&&e.state.htmx){ar();oe(t,function(e){ce(e,\"htmx:restored\",{document:re(),triggerEvent:ce})})}else{if(r){r(e)}}};setTimeout(function(){ce(e,\"htmx:load\",{});e=null},0)});return Q}()}); \n \nFix:  \n1.\u200b The following buttons do not work on panel 2. Generate Study Material: \na.\u200b Summary \nb.\u200b Key Terms \nc.\u200b Practice Questions \nd.\u200b External Resources \n \n",
        "added_at": "2025-11-30T01:33:09.971203"
      },
      {
        "name": "326 Proj.pdf",
        "type": "document",
        "text": "Python\ngit add .  \ngit commit -m \u201c   \u201c \ngit push -u origin main  \n \n \nmain.py:  \n \nimport os \nimport json \nimport uuid \nimport subprocess \nfrom datetime import datetime \nfrom typing import Dict, Any \n \nimport fitz  # PyMuPDF \nfrom pptx import Presentation \nfrom fastapi import FastAPI, Request, UploadFile, File, Form \nfrom fastapi.responses import HTMLResponse, PlainTextResponse \nfrom fastapi.staticfiles import StaticFiles \nfrom fastapi.templating import Jinja2Templates \nimport google.generativeai as genai \n \n# Ai model  \ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\")) \nGEM_MODEL = genai.GenerativeModel(\"gemini-1.5-flash\") \n \n# ----------------- CONFIG ----------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \nUPLOAD_DIR = os.path.join(BASE_DIR, \"upload\") \nAUDIO_DIR = os.path.join(UPLOAD_DIR, \"audio\") \nDOC_DIR = os.path.join(UPLOAD_DIR, \"docs\") \nSESSIONS_PATH = os.path.join(BASE_DIR, \"sessions.json\") \n \nos.makedirs(UPLOAD_DIR, exist_ok=True) \nos.makedirs(AUDIO_DIR, exist_ok=True) \nos.makedirs(DOC_DIR, exist_ok=True) \n \n#client = (api_key=os.getenv(\"GEMINI_API_KEY\")) \n \napp = FastAPI(title=\"Helektron Study Assistant\") \napp.mount(\"/static\", StaticFiles(directory=os.path.join(BASE_DIR, \"static\")), \nname=\"static\") \ntemplates = Jinja2Templates(directory=os.path.join(BASE_DIR, \"templates\")) \n \n\n# ----------------- SESSION STORAGE ----------------- \ndef load_sessions() -> Dict[str, Any]: \n    if not os.path.exists(SESSIONS_PATH): \n        return {} \n    with open(SESSIONS_PATH, \"r\") as f: \n        try: \n            return json.load(f) \n        except json.JSONDecodeError: \n            return {} \n \ndef save_sessions(sessions: Dict[str, Any]) -> None: \n    with open(SESSIONS_PATH, \"w\") as f: \n        json.dump(sessions, f, indent=2) \n \nsessions: Dict[str, Any] = load_sessions() \n \ndef get_or_create_session(session_id: str | None) -> str: \n    global sessions \n    if session_id and session_id in sessions: \n        return session_id \n    new_id = str(uuid.uuid4()) \n    sessions[new_id] = { \n        \"created_at\": datetime.utcnow().isoformat(), \n        \"files\": [],   # list of {\"name\",\"type\",\"text\"} \n    } \n    save_sessions(sessions) \n    return new_id \n \ndef add_text_to_session(session_id: str, filename: str, file_type: str, text: \nstr) -> None: \n    global sessions \n    if session_id not in sessions: \n        session_id = get_or_create_session(None) \n    sessions[session_id][\"files\"].append({ \n        \"name\": filename, \n        \"type\": file_type, \n        \"text\": text, \n        \"added_at\": datetime.utcnow().isoformat() \n    }) \n    save_sessions(sessions) \n \ndef get_session_text(session_id: str) -> str: \n    session = sessions.get(session_id) \n    if not session: \n\n        return \"\" \n    blocks = [f\"--- {f['name']} ({f['type']}) ---\\n{f['text']}\" for f in \nsession[\"files\"] if f.get(\"text\")] \n    return \"\\n\\n\".join(blocks) \n \n# ----------------- HELPERS: FILE TEXT EXTRACTION ----------------- \ndef extract_txt(path: str) -> str: \n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        return f.read() \n \ndef extract_pdf(path: str) -> str: \n    doc = fitz.open(path) \n    texts = [] \n    for page in doc: \n        texts.append(page.get_text()) \n    doc.close() \n    return \"\\n\".join(texts) \n \ndef extract_pptx(path: str) -> str: \n    prs = Presentation(path) \n    texts = [] \n    for slide in prs.slides: \n        for shape in slide.shapes: \n            if hasattr(shape, \"text\"): \n                texts.append(shape.text) \n    return \"\\n\".join(texts) \n \ndef transcribe_audio(path: str) -> str: \n    # Find whisper binary \n    candidates = [ \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"build\", \"bin\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper-main\", \"build\", \"bin\", \"whisper-cli\"), \n    ] \n    whisper_bin = next((c for c in candidates if os.path.exists(c)), None) \n    if not whisper_bin: \n        raise RuntimeError(\"Whisper binary not found. Build whisper.cpp \nfirst.\") \n \n    wav_path = path + \".wav\" \n    subprocess.run( \n        [\"ffmpeg\", \"-y\", \"-i\", path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \n\"-ac\", \"1\", wav_path], \n        check=True \n\n    ) \n \n    out_prefix = path + \"_out\" \n    subprocess.run( \n        [whisper_bin, \"-m\", os.path.join(BASE_DIR, \"whisper.cpp\", \"models\", \n\"ggml-base.en.bin\"), \n         \"-f\", wav_path, \"--output-txt\", \"--output-file\", out_prefix], \n        check=True \n    ) \n \n    txt_path = out_prefix + \".txt\" \n    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        transcript = f.read() \n \n    # cleanup \n    for p in [wav_path, txt_path]: \n        if os.path.exists(p): \n            os.remove(p) \n \n    return transcript \n \n# ----------------- GPT PROMPTS ----------------- \ndef get_summary_prompt(transcript: str) -> str: \n    return f\"\"\" \nBased on the following combined study materials (lectures, slides, PDFs, notes, \ntranscripts), \ncreate a **structured, detailed, and academically accurate study summary**. \n \nOrganize the output into the following clearly labeled sections: \n \n- **Overview**: Briefly describe the overall topic and goals. \n- **Key Concepts Introduced**: List and explain major ideas, theories, \nformulas, or processes. \n- **Detailed Topic Breakdown**: Group related ideas and summarize explanations, \nreasoning steps, and relationships. \n- **Important Definitions**: Provide concise definitions for technical terms or \ndomain-specific vocabulary. \n- **Examples or Applications**: Summarize any examples, demonstrations, or \nreal-world applications. \n- **Main Takeaways**: What a student should remember after studying this \nmaterial. \n \nExpectations: \n- Use clear, concise bullet points. \n\n- Prioritize conceptual correctness. \n- Use simple academic language suitable for undergraduate study. \n- If information is unclear or missing, label it as `TBD`. \n \n---BEGIN MATERIAL--- \n{transcript} \n---END MATERIAL--- \n\"\"\" \n \ndef get_keyterms_prompt(text: str) -> str: \n    return f\"\"\" \nFrom the combined study materials below, extract **10\u201320 key terms** with short \ndefinitions. \n \nFor each term, include: \n- The term \n- A 1\u20132 sentence definition \n- (Optional) A quick note on why it matters in this context. \n \nFormat as a bulleted list. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_questions_prompt(text: str) -> str: \n    return f\"\"\" \nCreate **8\u201312 practice questions** based on the combined study materials below. \n \nInclude a mix of: \n- Conceptual understanding questions \n- Short-answer questions \n- Application/problem-style questions (where possible) \n \nDo NOT provide answers, only questions. \nGroup them into sections if appropriate (e.g., 'Conceptual', 'Application'). \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_resources_prompt(text: str) -> str: \n\n    return f\"\"\" \nBased on the topics and concepts in the combined study materials below, \nrecommend **3\u20137 high-quality external resources** for further study. \n \nEach resource should include: \n- Title \n- Type (e.g., textbook chapter, .edu article, video lecture) \n- Source (e.g., university, well-known platform) \n- 1\u20132 sentences on why it is relevant. \n \nPrefer: \n- .edu domains \n- Reputable textbooks \n- Well-known educational channels \n \nYou do NOT need to provide URLs, just clear, identifiable references. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef call_gemini(prompt: str) -> str: \n    try: \n        response = GEM_MODEL.generate_content(prompt) \n        return response.text.strip() \n    except Exception as e: \n        return f\"[Gemini API Error: {e}]\" \n \n# ----------------- ROUTES ----------------- \n@app.get(\"/\", response_class=HTMLResponse) \ndef index(request: Request): \n    return templates.TemplateResponse(\"index.html\", {\"request\": request}) \n \n# Upload any material (txt, pdf, pptx, mp4, m4a) \n@app.post(\"/upload\", response_class=HTMLResponse) \nasync def upload_material( \n    request: Request, \n    file: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    global sessions \n \n    session_id = get_or_create_session(session_id) \n\n    filename = file.filename \n    ext = filename.split(\".\")[-1].lower() \n \n    # Save file \n    if ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n        save_dir = AUDIO_DIR \n        kind = \"audio\" \n    else: \n        save_dir = DOC_DIR \n        kind = \"document\" \n \n    os.makedirs(save_dir, exist_ok=True) \n    saved_path = os.path.join(save_dir, f\"{uuid.uuid4().hex}_{filename}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await file.read()) \n \n    # Extract text \n    try: \n        if ext == \"txt\": \n            text = extract_txt(saved_path) \n        elif ext == \"pdf\": \n            text = extract_pdf(saved_path) \n        elif ext in [\"pptx\"]: \n            text = extract_pptx(saved_path) \n        elif ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n            text = transcribe_audio(saved_path) \n        else: \n            text = f\"[Unsupported file type: {ext}]\" \n    except Exception as e: \n        text = f\"[Error processing file {filename}: {e}]\" \n \n    add_text_to_session(session_id, filename, kind, text) \n \n    # Render updated materials list (fragment) \n    session = sessions[session_id] \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n\n \n# Live transcription: audio blob from the browser \n@app.post(\"/upload_live_audio\", response_class=HTMLResponse) \nasync def upload_live_audio( \n    request: Request, \n    audio: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    session_id = get_or_create_session(session_id) \n    filename = audio.filename or \"live_recording.webm\" \n    ext = filename.split(\".\")[-1].lower() \n \n    os.makedirs(AUDIO_DIR, exist_ok=True) \n    saved_path = os.path.join(AUDIO_DIR, f\"live_{uuid.uuid4().hex}.{ext}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await audio.read()) \n \n    try: \n        text = transcribe_audio(saved_path) \n    except Exception as e: \n        text = f\"[Error transcribing live audio: {e}]\" \n \n    add_text_to_session(session_id, filename, \"audio-live\", text) \n    session = sessions[session_id] \n \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# ---- Study tools (summary, key terms, questions, resources) ---- \n \n@app.get(\"/summary/{session_id}\", response_class=PlainTextResponse) \ndef generate_summary(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_summary_prompt(text) \n\nPython\n        content = call_gemini(prompt) \n    # Return plain text directly to HTMX target \n    return content \n \n \n@app.get(\"/keyterms/{session_id}\", response_class=PlainTextResponse) \ndef generate_keyterms(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_keyterms_prompt(text) \n        content = call_gemini(prompt) \n    return content \n \n \n@app.get(\"/questions/{session_id}\", response_class=PlainTextResponse) \ndef generate_questions_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_questions_prompt(text) \n        content = call_gemini(prompt) \n    return content \n \n \n@app.get(\"/resources/{session_id}\", response_class=PlainTextResponse) \ndef generate_resources_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_resources_prompt(text) \n        content = call_gemini(prompt) \n    return content \nRag Folder:  \n1.\u200b build_vector_store.py  \n \nimport os \nimport json \n\nimport faiss \nfrom langchain_community.vectorstores import FAISS \nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain_core.documents import Document \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter \n \n# -------------------------------------------- \n# Correct project paths \n# -------------------------------------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \n \n# Uploaded document text is stored here by your FastAPI app \nDOC_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"docs\") \n \n# Vector store output directory \nVS_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"vs\") \n \nos.makedirs(VS_DIR, exist_ok=True) \n \n \n# -------------------------------------------- \n# Load documents from upload/docs/ \n# -------------------------------------------- \ndef load_documents(): \n    docs = [] \n    if not os.path.exists(DOC_DIR): \n        print(f\"Missing directory {DOC_DIR}. No documents found.\") \n        return docs \n \n    for filename in os.listdir(DOC_DIR): \n        path = os.path.join(DOC_DIR, filename) \n        if not os.path.isfile(path): \n            continue \n \n        try: \n            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n                text = f.read().strip() \n                if text: \n                    docs.append(Document(page_content=text, metadata={\"source\": \nfilename})) \n        except Exception as e: \n            print(f\"Error loading {filename}: {e}\") \n \n    return docs \n\n \n \n# -------------------------------------------- \n# Build vector store \n# -------------------------------------------- \ndef build_vector_store(): \n    print(f\"Building vector store from {DOC_DIR}...\") \n \n    docs = load_documents() \n \n    if not docs: \n        print(\"No documents found in upload/docs/. Upload files in the web app \nfirst.\") \n        return \n \n    print(f\"Loaded {len(docs)} raw documents.\") \n \n    # ------------------------------- \n    # Chunk documents \n    # ------------------------------- \n    splitter = RecursiveCharacterTextSplitter( \n        chunk_size=1000, # ~750 tokens \n        chunk_overlap=200, # contextual overlap \n        length_function=len \n    ) \n \n    split_docs = [] \n    for doc in docs: \n        chunks = splitter.split_text(doc.page_content) \n        for chunk in chunks: \n            split_docs.append( \n                Document( \n                    page_content=chunk, \n                    metadata=doc.metadata  # keep source info \n                ) \n            ) \n \n    print(f\"Created {len(split_docs)} total chunks.\") \n \n    # ------------------------------- \n    # Generate embeddings \n    # ------------------------------- \n    embeddings = HuggingFaceEmbeddings() \n \n\nPython\n    print(\"Generating embeddings...\") \n    vectorstore = FAISS.from_documents(split_docs, embeddings) \n \n    # ------------------------------- \n    # Save FAISS index + metadata \n    # ------------------------------- \n    print(\"Saving FAISS index and metadata...\") \n \n    faiss.write_index(vectorstore.index, os.path.join(VS_DIR, \"vs.faiss\")) \n \n    with open(os.path.join(VS_DIR, \"docs.json\"), \"w\") as f: \n        json.dump([d.dict() for d in split_docs], f, indent=2) \n \n    print(\"\\n\ud83c\udf89 Vector store built successfully!\") \n    print(f\"Saved to: {VS_DIR}\") \n \n \nif __name__ == \"__main__\": \n    build_vector_store() \n \n2.\u200b rag_utils.py \n \nimport os \nimport json \nimport numpy as np \nfrom google import genai \n \n# Load Gemini client \nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\")) \n \n \n# ----------------------------- \n# Helper: Load vector store \n# ----------------------------- \ndef load_store(path): \n    if not os.path.exists(path): \n        return [] \n    with open(path, \"r\") as f: \n        return json.load(f) \n \n \n\n# ----------------------------- \n# Helper: Save vector store \n# ----------------------------- \ndef save_store(path, data): \n    with open(path, \"w\") as f: \n        json.dump(data, f, indent=4) \n \n \n# ----------------------------- \n# Embed text using Gemini \n# ----------------------------- \ndef embed_text(text: str): \n    \"\"\" \n    Uses Gemini embeddings. \n    Returns list[float]. \n    \"\"\" \n \n    result = client.models.embed_content( \n        model=\"text-embedding-004\", \n        contents=text, \n    ) \n \n    # Gemini wraps embeddings inside .output_embedding \n    return result.output_embedding \n \n \n# ----------------------------- \n# Add text to class vector store \n# ----------------------------- \ndef add_to_vector_store(class_id: str, text: str): \n    \"\"\" \n    Creates or appends to the vector store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    store = load_store(store_path) \n \n    embedding = embed_text(text) \n \n    store.append({ \n        \"text\": text, \n        \"embedding\": embedding, \n\n    }) \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Build store from scratch (optional) \n# ----------------------------- \ndef build_vector_store(class_id: str, text: str): \n    \"\"\" \n    Clears and rebuilds the entire store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    embedding = embed_text(text) \n \n    store = [ \n        { \n            \"text\": text, \n            \"embedding\": embedding, \n        } \n    ] \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Search RAG store using cosine similarity \n# ----------------------------- \ndef search_vector_store(class_id: str, query: str): \n    \"\"\" \n    Returns the most relevant text chunk for the given query. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    store = load_store(store_path) \n \n    if not store: \n        return None \n \n    query_embedding = embed_text(query) \n    query_vec = np.array(query_embedding) \n\nHTML\n \n    best_score = -999 \n    best_text = None \n \n    for item in store: \n        vec = np.array(item[\"embedding\"]) \n \n        # cosine similarity \n        score = np.dot(query_vec, vec) / ( \n            np.linalg.norm(query_vec) * np.linalg.norm(vec) \n        ) \n \n        if score > best_score: \n            best_score = score \n            best_text = item[\"text\"] \n \n    return best_text \n \n \nTemplates \n1.\u200b templates/index.html  \n \n{% extends \"base.html\" %} \n{% block content %} \n \n<h1>Helektron Study Assistant</h1> \n<p class=\"subtitle\"> \n    Upload lecture materials (PDF, slides, audio, notes), then generate \nsummaries, key terms, questions, and more. \n</p> \n \n<div class=\"layout\"> \n \n    <!-- LEFT PANEL --> \n    <div class=\"card\"> \n        <h2>1. Upload Materials</h2> \n \n        <div class=\"upload-box\"> \n            <p><strong>Upload files:</strong> .txt, .pdf, .pptx, .mp4, .m4a</p> \n \n            <form \n\n                hx-post=\"/upload\" \n                hx-target=\"#materials-panel\" \n                hx-swap=\"innerHTML\" \n                hx-indicator=\"#upload-loading\" \n                enctype=\"multipart/form-data\"> \n \n                <input type=\"file\" name=\"file\" required> \n                <input type=\"hidden\" name=\"session_id\" id=\"session_id_input\"> \n                <br><br> \n                <button class=\"primary\" type=\"submit\">Upload</button> \n            </form> \n        </div> \n \n        <div class=\"upload-box\"> \n            <p><strong>Live Transcription:</strong> record audio and transcribe \nwith Whisper.</p> \n            <button class=\"secondary\" id=\"record-btn\">\ud83c\udf99 Start \nRecording</button> \n            <p id=\"record-status\" style=\"font-size: 12px; color:#555; \nmargin-top:6px;\"></p> \n        </div> \n \n        <div id=\"upload-loading\" class=\"htmx-indicator\"> \n            Uploading/processing material... \n        </div> \n \n        <!-- \ud83d\udd25 THIS GETS UPDATED BY upload_status.html --> \n        <div id=\"materials-panel\"> \n            <em>No materials uploaded yet.</em> \n        </div> \n \n    </div> \n \n    <!-- RIGHT PANEL --> \n    <div class=\"card\"> \n        <h2>2. Generate Study Material</h2> \n \n        <p style=\"font-size:13px; color:#555;\">Choose what you want the AI to \ngenerate.</p> \n \n        <div style=\"display:flex; gap:8px; flex-wrap:wrap; \nmargin-bottom:10px;\"> \n            <button class=\"primary\" \n                    id=\"btn-summary\" \n\n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Summary \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-keyterms\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Key Terms \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-questions\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Practice Questions \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-resources\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                External Resources \n            </button> \n        </div> \n \n        <div id=\"generate-loading\" class=\"htmx-indicator\"> \n            Generating material... \n        </div> \n \n        <div id=\"results-panel\"> \n            <em>Your generated study content will appear here.</em> \n        </div> \n \n    </div> \n\nHTML\nHTML\n</div> \n \n<script src=\"/static/app.js\"></script> \n \n{% endblock %} \n \n2.\u200b templates/base.html  \n \n<!DOCTYPE html> \n<html lang=\"en\"> \n<head> \n    <meta charset=\"UTF-8\"> \n    <title>Helektron Study Assistant</title> \n    <link rel=\"stylesheet\" href=\"/static/styles.css\"> \n</head> \n \n<body> \n<div class=\"app-shell\"> \n    {% block content %}{% endblock %} \n</div> \n \n<script src=\"https://unpkg.com/htmx.org@1.9.10\"></script> \n</body> \n</html> \n \n3.\u200b Fragments folder \na.\u200b templates/fragments/class_list.html  \n \n<ul> \n    {% for cls in classes %} \n    <li> \n        <button  \n            hx-post=\"/set_active_class\"  \n            hx-vals='{\"class_id\": \"{{ cls.id }}\"}' \n            hx-target=\"#upload-list\" \n            hx-swap=\"innerHTML\" \n            class=\"{% if cls.id == active_class_id %}active{% endif %}\" \n\nHTML\nHTML\nHTML\n        > \n            {{ cls.name }} \n        </button> \n    </li> \n    {% endfor %} \n</ul> \n \nb.\u200b templates/fragments/upload_status.html  \n \n{% if session_id %} \n<div data-session-id=\"{{ session_id }}\"></div> \n{% endif %} \n \n{% if files %} \n<ul> \n    {% for f in files %} \n    <li><strong>{{ f.name }}</strong> ({{ f.type }})</li> \n    {% endfor %} \n</ul> \n{% else %} \n<em>No materials uploaded yet.</em> \n{% endif %} \n \nc.\u200b templates/fragments/summary.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n \nd.\u200b templates/fragments/resources.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n\nHTML\nHTML\nCSS\ne.\u200b templates/fragments/questions.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \nf.\u200b\ntemplates/fragments/keyterms.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n \n \nStatic:  \n1.\u200b static/styles.css \n \nbody { \n    margin: 0; \n    font-family: system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", \nsans-serif; \n    background: #f4f4fb; \n} \n \n.app-shell { \n    max-width: 1100px; \n    margin: 0 auto; \n    padding: 24px; \n} \n \nh1 { \n    font-size: 28px; \n    margin-bottom: 8px; \n} \n \n.subtitle { \n    color: #555; \n    margin-bottom: 24px; \n} \n\n \n.layout { \n    display: grid; \n    grid-template-columns: 1.2fr 1.8fr; \n    gap: 20px; \n} \n \n.card { \n    background: white; \n    border-radius: 12px; \n    padding: 16px 20px; \n    box-shadow: 0 1px 3px rgba(15,23,42,0.08); \n    border: 1px solid #e2e8f0; \n} \n \n.card h2 { \n    font-size: 18px; \n    margin-top: 0; \n    margin-bottom: 12px; \n} \n \n.upload-box { \n    border: 2px dashed #cbd5f5; \n    border-radius: 10px; \n    padding: 16px; \n    text-align: center; \n    background: #f9fafb; \n    margin-bottom: 12px; \n} \n \n.upload-box input[type=\"file\"] { \n    margin-top: 8px; \n} \n \nbutton.primary { \n    background: #4f46e5; \n    color: white; \n    border: none; \n    border-radius: 8px; \n    padding: 10px 16px; \n    font-weight: 600; \n    cursor: pointer; \n    font-size: 14px; \n} \n\n \nbutton.primary:disabled { \n    opacity: 0.6; \n    cursor: not-allowed; \n} \n \nbutton.primary:hover:not(:disabled) { \n    background: #4338ca; \n} \n \nbutton.secondary { \n    background: white; \n    color: #4f46e5; \n    border: 1px solid #cbd5f5; \n    border-radius: 8px; \n    padding: 8px 14px; \n    font-weight: 500; \n    cursor: pointer; \n    font-size: 13px; \n} \n \nbutton.secondary:hover { \n    background: #eef2ff; \n} \n \n#materials-list { \n    font-size: 14px; \n    max-height: 200px; \n    overflow-y: auto; \n} \n \n#materials-list ul { \n    padding-left: 18px; \n} \n \n#results-panel { \n    min-height: 250px; \n    font-size: 14px; \n    white-space: pre-wrap; \n} \n \n/* htmx loading indicator */ \n.htmx-indicator { \n    display: none; \n\nJavaScript\n    margin-bottom: 8px; \n    font-size: 13px; \n    color: #6b21a8; \n} \n \n.htmx-request .htmx-indicator { \n    display: block; \n} \n2.\u200b static/app.js \n \nlet currentSessionId = null; \n \nfunction setSessionId(id) { \n    currentSessionId = id; \n    const hidden = document.getElementById(\"session_id_input\"); \n    if (hidden) hidden.value = id; \n    enableStudyButtons(); \n} \n \nfunction enableStudyButtons() { \n    if (!currentSessionId) return; \n \n    const summaryBtn = document.getElementById(\"btn-summary\"); \n    const keyBtn     = document.getElementById(\"btn-keyterms\"); \n    const qBtn       = document.getElementById(\"btn-questions\"); \n    const rBtn       = document.getElementById(\"btn-resources\"); \n \n    [summaryBtn, keyBtn, qBtn, rBtn].forEach(btn => btn.disabled = false); \n \n    summaryBtn.setAttribute(\"hx-get\", `/summary/${currentSessionId}`); \n    keyBtn.setAttribute(\"hx-get\", `/keyterms/${currentSessionId}`); \n    qBtn.setAttribute(\"hx-get\", `/questions/${currentSessionId}`); \n    rBtn.setAttribute(\"hx-get\", `/resources/${currentSessionId}`); \n} \n \ndocument.body.addEventListener(\"htmx:afterSettle\", function () { \n    // Look for the element *inside* #materials-panel that has data-session-id \n    const sidElem = document.querySelector(\"#materials-panel \n[data-session-id]\"); \n    const sid = sidElem ? sidElem.getAttribute(\"data-session-id\") : null; \n \n\n    if (sid && sid !== currentSessionId) { \n        setSessionId(sid); \n    } \n}); \n \n// recording JS: \nlet mediaRecorder = null; \nlet recordedChunks = []; \n \nconst recordBtn = document.getElementById(\"record-btn\"); \nconst recordStatus = document.getElementById(\"record-status\"); \n \nrecordBtn.addEventListener(\"click\", async () => { \n    if (!mediaRecorder || mediaRecorder.state === \"inactive\") { \n        try { \n            const stream = await navigator.mediaDevices.getUserMedia({ audio: \ntrue }); \n            mediaRecorder = new MediaRecorder(stream); \n            recordedChunks = []; \n \n            mediaRecorder.ondataavailable = (e) => { \n                if (e.data.size > 0) recordedChunks.push(e.data); \n            }; \n \n            mediaRecorder.onstop = async () => { \n                const blob = new Blob(recordedChunks, { type: \"audio/webm\" }); \n                await uploadLiveAudio(blob); \n                stream.getTracks().forEach(t => t.stop()); \n                recordStatus.textContent = \"Recording finished. \nTranscribing...\"; \n            }; \n \n            mediaRecorder.start(); \n            recordBtn.textContent = \"\u23f9 Stop Recording\"; \n            recordStatus.textContent = \"Recording...\"; \n        } catch (err) { \n            console.error(err); \n            alert(\"Could not access microphone.\"); \n        } \n    } else if (mediaRecorder.state === \"recording\") { \n        mediaRecorder.stop(); \n        recordBtn.textContent = \"\ud83c\udf99 Start Recording\"; \n    } \n}); \n\nJavaScript\n \nasync function uploadLiveAudio(blob) { \n    const formData = new FormData(); \n    formData.append(\"audio\", blob, \"live_recording.webm\"); \n    if (currentSessionId) { \n        formData.append(\"session_id\", currentSessionId); \n    } \n \n    const resp = await fetch(\"/upload_live_audio\", { \n        method: \"POST\", \n        body: formData \n    }); \n \n    const html = await resp.text(); \n    document.getElementById(\"materials-panel\").innerHTML = html; \n    recordStatus.textContent = \"Live audio processed.\"; \n} \n3.\u200b static/htmx.min.js \n \n(function(e,t){if(typeof define===\"function\"&&define.amd){define([],t)}else if(typeof module===\"object\"&&module.exports){module.exports=t()}else{e.htmx=e.htmx||t()}})(typeof self!==\"undefined\"?self:this,function(){return function(){\"use strict\";var \nQ={onLoad:F,process:zt,on:de,off:ge,trigger:ce,ajax:Nr,find:C,findAll:f,closest:v,values:function(e,t){var r=dr(e,t||\"post\");return \nr.values},remove:_,addClass:z,removeClass:n,toggleClass:$,takeClass:W,defineExtension:Ur,removeExtension:Br,logAll:V,logNone:j,logger:null,config:{historyEnabled:true,historyCacheSize:10,refreshOnHistoryMiss:false,defaultSwapStyle:\"innerHTML\",defaultSwapDelay:0,defaultSettleDelay:20,includeIndicatorStyles:true,indicat\norClass:\"htmx-indicator\",requestClass:\"htmx-request\",addedClass:\"htmx-added\",settlingClass:\"htmx-settling\",swappingClass:\"htmx-swapping\",allowEval:true,allowScriptTags:true,inlineScriptNonce:\"\",attributesToSettle:[\"class\",\"style\",\"width\",\"height\"],withCredentials:false,timeout:0,wsReconnectDelay:\"full-jitter\",wsBinary\nType:\"blob\",disableSelector:\"[hx-disable], \n[data-hx-disable]\",useTemplateFragments:false,scrollBehavior:\"smooth\",defaultFocusScroll:false,getCacheBusterParam:false,globalViewTransitions:false,methodsThatUseUrlParams:[\"get\"],selfRequestsOnly:false,ignoreTitle:false,scrollIntoViewOnBoost:true,triggerSpecsCache:null},parseInterval:d,_:t,createEventSource:function\n(e){return new EventSource(e,{withCredentials:true})},createWebSocket:function(e){var t=new WebSocket(e,[]);t.binaryType=Q.config.wsBinaryType;return t},version:\"1.9.10\"};var \nr={addTriggerHandler:Lt,bodyContains:se,canAccessLocalStorage:U,findThisElement:xe,filterValues:yr,hasAttribute:o,getAttributeValue:te,getClosestAttributeValue:ne,getClosestMatch:c,getExpressionVars:Hr,getHeaders:xr,getInputValues:dr,getInternalData:ae,getSwapSpecification:wr,getTriggerSpecs:it,getTarget:ye,makeFragme\nnt:l,mergeObjects:le,makeSettleInfo:T,oobSwap:Ee,querySelectorExt:ue,selectAndSwap:je,settleImmediately:nr,shouldCancel:ut,triggerEvent:ce,triggerErrorEvent:fe,withExtensions:R};var w=[\"get\",\"post\",\"put\",\"delete\",\"patch\"];var i=w.map(function(e){return\"[hx-\"+e+\"], [data-hx-\"+e+\"]\"}).join(\", \");var \nS=e(\"head\"),q=e(\"title\"),H=e(\"svg\",true);function e(e,t=false){return new RegExp(`<${e}(\\\\s[^>]*>|>)([\\\\s\\\\S]*?)<\\\\/${e}>`,t?\"gim\":\"im\")}function d(e){if(e==undefined){return undefined}let t=NaN;if(e.slice(-2)==\"ms\"){t=parseFloat(e.slice(0,-2))}else if(e.slice(-1)==\"s\"){t=parseFloat(e.slice(0,-1))*1e3}else \nif(e.slice(-1)==\"m\"){t=parseFloat(e.slice(0,-1))*1e3*60}else{t=parseFloat(e)}return isNaN(t)?undefined:t}function ee(e,t){return e.getAttribute&&e.getAttribute(t)}function o(e,t){return e.hasAttribute&&(e.hasAttribute(t)||e.hasAttribute(\"data-\"+t))}function te(e,t){return ee(e,t)||ee(e,\"data-\"+t)}function u(e){return \ne.parentElement}function re(){return document}function c(e,t){while(e&&!t(e)){e=u(e)}return e?e:null}function L(e,t,r){var n=te(t,r);var i=te(t,\"hx-disinherit\");if(e!==t&&i&&(i===\"*\"||i.split(\" \").indexOf(r)>=0)){return\"unset\"}else{return n}}function ne(t,r){var n=null;c(t,function(e){return \nn=L(t,e,r)});if(n!==\"unset\"){return n}}function h(e,t){var r=e.matches||e.matchesSelector||e.msMatchesSelector||e.mozMatchesSelector||e.webkitMatchesSelector||e.oMatchesSelector;return r&&r.call(e,t)}function A(e){var t=/<([a-z][^\\/\\0>\\x20\\t\\r\\n\\f]*)/i;var r=t.exec(e);if(r){return \nr[1].toLowerCase()}else{return\"\"}}function a(e,t){var r=new DOMParser;var n=r.parseFromString(e,\"text/html\");var i=n.body;while(t>0){t--;i=i.firstChild}if(i==null){i=re().createDocumentFragment()}return i}function N(e){return/<body/.test(e)}function l(e){var t=!N(e);var r=A(e);var \nn=e;if(r===\"head\"){n=n.replace(S,\"\")}if(Q.config.useTemplateFragments&&t){var i=a(\"<body><template>\"+n+\"</template></body>\",0);return i.querySelector(\"template\").content}switch(r){case\"thead\":case\"tbody\":case\"tfoot\":case\"colgroup\":case\"caption\":return a(\"<table>\"+n+\"</table>\",1);case\"col\":return \na(\"<table><colgroup>\"+n+\"</colgroup></table>\",2);case\"tr\":return a(\"<table><tbody>\"+n+\"</tbody></table>\",2);case\"td\":case\"th\":return a(\"<table><tbody><tr>\"+n+\"</tr></tbody></table>\",3);case\"script\":case\"style\":return a(\"<div>\"+n+\"</div>\",1);default:return a(n,0)}}function ie(e){if(e){e()}}function I(e,t){return \nObject.prototype.toString.call(e)===\"[object \"+t+\"]\"}function k(e){return I(e,\"Function\")}function P(e){return I(e,\"Object\")}function ae(e){var t=\"htmx-internal-data\";var r=e[t];if(!r){r=e[t]={}}return r}function M(e){var t=[];if(e){for(var r=0;r<e.length;r++){t.push(e[r])}}return t}function oe(e,t){if(e){for(var \nr=0;r<e.length;r++){t(e[r])}}}function X(e){var t=e.getBoundingClientRect();var r=t.top;var n=t.bottom;return r<window.innerHeight&&n>=0}function se(e){if(e.getRootNode&&e.getRootNode()instanceof window.ShadowRoot){return re().body.contains(e.getRootNode().host)}else{return re().body.contains(e)}}function D(e){return \ne.trim().split(/\\s+/)}function le(e,t){for(var r in t){if(t.hasOwnProperty(r)){e[r]=t[r]}}return e}function E(e){try{return JSON.parse(e)}catch(e){b(e);return null}}function U(){var e=\"htmx:localStorageTest\";try{localStorage.setItem(e,e);localStorage.removeItem(e);return true}catch(e){return false}}function \nB(t){try{var e=new URL(t);if(e){t=e.pathname+e.search}if(!/^\\/$/.test(t)){t=t.replace(/\\/+$/,\"\")}return t}catch(e){return t}}function t(e){return Tr(re().body,function(){return eval(e)})}function F(t){var e=Q.on(\"htmx:load\",function(e){t(e.detail.elt)});return e}function \nV(){Q.logger=function(e,t,r){if(console){console.log(t,e,r)}}}function j(){Q.logger=null}function C(e,t){if(t){return e.querySelector(t)}else{return C(re(),e)}}function f(e,t){if(t){return e.querySelectorAll(t)}else{return f(re(),e)}}function \n_(e,t){e=g(e);if(t){setTimeout(function(){_(e);e=null},t)}else{e.parentElement.removeChild(e)}}function z(e,t,r){e=g(e);if(r){setTimeout(function(){z(e,t);e=null},r)}else{e.classList&&e.classList.add(t)}}function \nn(e,t,r){e=g(e);if(r){setTimeout(function(){n(e,t);e=null},r)}else{if(e.classList){e.classList.remove(t);if(e.classList.length===0){e.removeAttribute(\"class\")}}}}function $(e,t){e=g(e);e.classList.toggle(t)}function W(e,t){e=g(e);oe(e.parentElement.children,function(e){n(e,t)});z(e,t)}function \nv(e,t){e=g(e);if(e.closest){return e.closest(t)}else{do{if(e==null||h(e,t)){return e}}while(e=e&&u(e));return null}}function s(e,t){return e.substring(0,t.length)===t}function G(e,t){return e.substring(e.length-t.length)===t}function J(e){var t=e.trim();if(s(t,\"<\")&&G(t,\"/>\")){return \nt.substring(1,t.length-2)}else{return t}}function Z(e,t){if(t.indexOf(\"closest \")===0){return[v(e,J(t.substr(8)))]}else if(t.indexOf(\"find \")===0){return[C(e,J(t.substr(5)))]}else if(t===\"next\"){return[e.nextElementSibling]}else if(t.indexOf(\"next \")===0){return[K(e,J(t.substr(5)))]}else \nif(t===\"previous\"){return[e.previousElementSibling]}else if(t.indexOf(\"previous \")===0){return[Y(e,J(t.substr(9)))]}else if(t===\"document\"){return[document]}else if(t===\"window\"){return[window]}else if(t===\"body\"){return[document.body]}else{return re().querySelectorAll(J(t))}}var K=function(e,t){var \nr=re().querySelectorAll(t);for(var n=0;n<r.length;n++){var i=r[n];if(i.compareDocumentPosition(e)===Node.DOCUMENT_POSITION_PRECEDING){return i}}};var Y=function(e,t){var r=re().querySelectorAll(t);for(var n=r.length-1;n>=0;n--){var i=r[n];if(i.compareDocumentPosition(e)===Node.DOCUMENT_POSITION_FOLLOWING){return \ni}}};function ue(e,t){if(t){return Z(e,t)[0]}else{return Z(re().body,e)[0]}}function g(e){if(I(e,\"String\")){return C(e)}else{return e}}function ve(e,t,r){if(k(t)){return{target:re().body,event:e,listener:t}}else{return{target:g(e),event:t,listener:r}}}function de(t,r,n){jr(function(){var \ne=ve(t,r,n);e.target.addEventListener(e.event,e.listener)});var e=k(r);return e?r:n}function ge(t,r,n){jr(function(){var e=ve(t,r,n);e.target.removeEventListener(e.event,e.listener)});return k(r)?r:n}var me=re().createElement(\"output\");function pe(e,t){var r=ne(e,t);if(r){if(r===\"this\"){return[xe(e,t)]}else{var \nn=Z(e,r);if(n.length===0){b('The selector \"'+r+'\" on '+t+\" returned no matches!\");return[me]}else{return n}}}}function xe(e,t){return c(e,function(e){return te(e,t)!=null})}function ye(e){var t=ne(e,\"hx-target\");if(t){if(t===\"this\"){return xe(e,\"hx-target\")}else{return ue(e,t)}}else{var r=ae(e);if(r.boosted){return \nre().body}else{return e}}}function be(e){var t=Q.config.attributesToSettle;for(var r=0;r<t.length;r++){if(e===t[r]){return true}}return false}function \nwe(t,r){oe(t.attributes,function(e){if(!r.hasAttribute(e.name)&&be(e.name)){t.removeAttribute(e.name)}});oe(r.attributes,function(e){if(be(e.name)){t.setAttribute(e.name,e.value)}})}function Se(e,t){var r=Fr(t);for(var n=0;n<r.length;n++){var i=r[n];try{if(i.isInlineSwap(e)){return true}}catch(e){b(e)}}return \ne===\"outerHTML\"}function Ee(e,i,a){var t=\"#\"+ee(i,\"id\");var o=\"outerHTML\";if(e===\"true\"){}else if(e.indexOf(\":\")>0){o=e.substr(0,e.indexOf(\":\"));t=e.substr(e.indexOf(\":\")+1,e.length)}else{o=e}var r=re().querySelectorAll(t);if(r){oe(r,function(e){var t;var \nr=i.cloneNode(true);t=re().createDocumentFragment();t.appendChild(r);if(!Se(o,e)){t=r}var \nn={shouldSwap:true,target:e,fragment:t};if(!ce(e,\"htmx:oobBeforeSwap\",n))return;e=n.target;if(n[\"shouldSwap\"]){Fe(o,e,e,t,a)}oe(a.elts,function(e){ce(e,\"htmx:oobAfterSwap\",n)})});i.parentNode.removeChild(i)}else{i.parentNode.removeChild(i);fe(re().body,\"htmx:oobErrorNoTarget\",{content:i})}return e}function \nCe(e,t,r){var n=ne(e,\"hx-select-oob\");if(n){var i=n.split(\",\");for(var a=0;a<i.length;a++){var o=i[a].split(\":\",2);var s=o[0].trim();if(s.indexOf(\"#\")===0){s=s.substring(1)}var l=o[1]||\"true\";var u=t.querySelector(\"#\"+s);if(u){Ee(l,u,r)}}}oe(f(t,\"[hx-swap-oob], [data-hx-swap-oob]\"),function(e){var \nt=te(e,\"hx-swap-oob\");if(t!=null){Ee(t,e,r)}})}function Re(e){oe(f(e,\"[hx-preserve], [data-hx-preserve]\"),function(e){var t=te(e,\"id\");var r=re().getElementById(t);if(r!=null){e.parentNode.replaceChild(r,e)}})}function Te(o,e,s){oe(e.querySelectorAll(\"[id]\"),function(e){var t=ee(e,\"id\");if(t&&t.length>0){var \nr=t.replace(\"'\",\"\\\\'\");var n=e.tagName.replace(\":\",\"\\\\:\");var i=o.querySelector(n+\"[id='\"+r+\"']\");if(i&&i!==o){var a=e.cloneNode();we(e,i);s.tasks.push(function(){we(e,a)})}}})}function Oe(e){return function(){n(e,Q.config.addedClass);zt(e);Nt(e);qe(e);ce(e,\"htmx:load\")}}function qe(e){var t=\"[autofocus]\";var \nr=h(e,t)?e:e.querySelector(t);if(r!=null){r.focus()}}function m(e,t,r,n){Te(e,r,n);while(r.childNodes.length>0){var i=r.firstChild;z(i,Q.config.addedClass);e.insertBefore(i,t);if(i.nodeType!==Node.TEXT_NODE&&i.nodeType!==Node.COMMENT_NODE){n.tasks.push(Oe(i))}}}function He(e,t){var \nr=0;while(r<e.length){t=(t<<5)-t+e.charCodeAt(r++)|0}return t}function Le(e){var t=0;if(e.attributes){for(var r=0;r<e.attributes.length;r++){var n=e.attributes[r];if(n.value){t=He(n.name,t);t=He(n.value,t)}}}return t}function Ae(e){var t=ae(e);if(t.onHandlers){for(var r=0;r<t.onHandlers.length;r++){const \nn=t.onHandlers[r];e.removeEventListener(n.event,n.listener)}delete t.onHandlers}}function Ne(e){var \nt=ae(e);if(t.timeout){clearTimeout(t.timeout)}if(t.webSocket){t.webSocket.close()}if(t.sseEventSource){t.sseEventSource.close()}if(t.listenerInfos){oe(t.listenerInfos,function(e){if(e.on){e.on.removeEventListener(e.trigger,e.listener)}})}Ae(e);oe(Object.keys(t),function(e){delete t[e]})}function \np(e){ce(e,\"htmx:beforeCleanupElement\");Ne(e);if(e.children){oe(e.children,function(e){p(e)})}}function Ie(t,e,r){if(t.tagName===\"BODY\"){return Ue(t,e,r)}else{var n;var i=t.previousSibling;m(u(t),t,e,r);if(i==null){n=u(t).firstChild}else{n=i.nextSibling}r.elts=r.elts.filter(function(e){return \ne!=t});while(n&&n!==t){if(n.nodeType===Node.ELEMENT_NODE){r.elts.push(n)}n=n.nextElementSibling}p(t);u(t).removeChild(t)}}function ke(e,t,r){return m(e,e.firstChild,t,r)}function Pe(e,t,r){return m(u(e),e,t,r)}function Me(e,t,r){return m(e,null,t,r)}function Xe(e,t,r){return m(u(e),e.nextSibling,t,r)}function \nDe(e,t,r){p(e);return u(e).removeChild(e)}function Ue(e,t,r){var n=e.firstChild;m(e,n,t,r);if(n){while(n.nextSibling){p(n.nextSibling);e.removeChild(n.nextSibling)}p(n);e.removeChild(n)}}function Be(e,t,r){var n=r||ne(e,\"hx-select\");if(n){var \ni=re().createDocumentFragment();oe(t.querySelectorAll(n),function(e){i.appendChild(e)});t=i}return t}function \nFe(e,t,r,n,i){switch(e){case\"none\":return;case\"outerHTML\":Ie(r,n,i);return;case\"afterbegin\":ke(r,n,i);return;case\"beforebegin\":Pe(r,n,i);return;case\"beforeend\":Me(r,n,i);return;case\"afterend\":Xe(r,n,i);return;case\"delete\":De(r,n,i);return;default:var a=Fr(t);for(var o=0;o<a.length;o++){var s=a[o];try{var \nl=s.handleSwap(e,r,n,i);if(l){if(typeof l.length!==\"undefined\"){for(var u=0;u<l.length;u++){var f=l[u];if(f.nodeType!==Node.TEXT_NODE&&f.nodeType!==Node.COMMENT_NODE){i.tasks.push(Oe(f))}}}return}}catch(e){b(e)}}if(e===\"innerHTML\"){Ue(r,n,i)}else{Fe(Q.config.defaultSwapStyle,t,r,n,i)}}}function \nVe(e){if(e.indexOf(\"<title\")>-1){var t=e.replace(H,\"\");var r=t.match(q);if(r){return r[2]}}}function je(e,t,r,n,i,a){i.title=Ve(n);var o=l(n);if(o){Ce(r,o,i);o=Be(r,o,a);Re(o);return Fe(e,r,t,o,i)}}function _e(e,t,r){var n=e.getResponseHeader(t);if(n.indexOf(\"{\")===0){var i=E(n);for(var a in \ni){if(i.hasOwnProperty(a)){var o=i[a];if(!P(o)){o={value:o}}ce(r,a,o)}}}else{var s=n.split(\",\");for(var l=0;l<s.length;l++){ce(r,s[l].trim(),[])}}}var ze=/\\s/;var x=/[\\s,]/;var $e=/[_$a-zA-Z]/;var We=/[_$a-zA-Z0-9]/;var Ge=['\"',\"'\",\"/\"];var Je=/[^\\s]/;var Ze=/[{(]/;var Ke=/[})]/;function Ye(e){var t=[];var \nr=0;while(r<e.length){if($e.exec(e.charAt(r))){var n=r;while(We.exec(e.charAt(r+1))){r++}t.push(e.substr(n,r-n+1))}else if(Ge.indexOf(e.charAt(r))!==-1){var i=e.charAt(r);var n=r;r++;while(r<e.length&&e.charAt(r)!==i){if(e.charAt(r)===\"\\\\\"){r++}r++}t.push(e.substr(n,r-n+1))}else{var a=e.charAt(r);t.push(a)}r++}return \nt}function Qe(e,t,r){return $e.exec(e.charAt(0))&&e!==\"true\"&&e!==\"false\"&&e!==\"this\"&&e!==r&&t!==\".\"}function et(e,t,r){if(t[0]===\"[\"){t.shift();var n=1;var i=\" return (function(\"+r+\"){ return (\";var a=null;while(t.length>0){var o=t[0];if(o===\"]\"){n--;if(n===0){if(a===null){i=i+\"true\"}t.shift();i+=\")})\";try{var \ns=Tr(e,function(){return Function(i)()},function(){return true});s.source=i;return s}catch(e){fe(re().body,\"htmx:syntax:error\",{error:e,source:i});return null}}}else if(o===\"[\"){n++}if(Qe(o,a,r)){i+=\"((\"+r+\".\"+o+\") ? (\"+r+\".\"+o+\") : (window.\"+o+\"))\"}else{i=i+o}a=t.shift()}}}function y(e,t){var \nr=\"\";while(e.length>0&&!t.test(e[0])){r+=e.shift()}return r}function tt(e){var t;if(e.length>0&&Ze.test(e[0])){e.shift();t=y(e,Ke).trim();e.shift()}else{t=y(e,x)}return t}var rt=\"input, textarea, select\";function nt(e,t,r){var n=[];var i=Ye(t);do{y(i,Je);var a=i.length;var \no=y(i,/[,\\[\\s]/);if(o!==\"\"){if(o===\"every\"){var s={trigger:\"every\"};y(i,Je);s.pollInterval=d(y(i,/[,\\[\\s]/));y(i,Je);var l=et(e,i,\"event\");if(l){s.eventFilter=l}n.push(s)}else if(o.indexOf(\"sse:\")===0){n.push({trigger:\"sse\",sseEvent:o.substr(4)})}else{var u={trigger:o};var \nl=et(e,i,\"event\");if(l){u.eventFilter=l}while(i.length>0&&i[0]!==\",\"){y(i,Je);var f=i.shift();if(f===\"changed\"){u.changed=true}else if(f===\"once\"){u.once=true}else if(f===\"consume\"){u.consume=true}else if(f===\"delay\"&&i[0]===\":\"){i.shift();u.delay=d(y(i,x))}else \nif(f===\"from\"&&i[0]===\":\"){i.shift();if(Ze.test(i[0])){var c=tt(i)}else{var c=y(i,x);if(c===\"closest\"||c===\"find\"||c===\"next\"||c===\"previous\"){i.shift();var h=tt(i);if(h.length>0){c+=\" \"+h}}}u.from=c}else if(f===\"target\"&&i[0]===\":\"){i.shift();u.target=tt(i)}else \nif(f===\"throttle\"&&i[0]===\":\"){i.shift();u.throttle=d(y(i,x))}else if(f===\"queue\"&&i[0]===\":\"){i.shift();u.queue=y(i,x)}else if(f===\"root\"&&i[0]===\":\"){i.shift();u[f]=tt(i)}else \nif(f===\"threshold\"&&i[0]===\":\"){i.shift();u[f]=y(i,x)}else{fe(e,\"htmx:syntax:error\",{token:i.shift()})}}n.push(u)}}if(i.length===a){fe(e,\"htmx:syntax:error\",{token:i.shift()})}y(i,Je)}while(i[0]===\",\"&&i.shift());if(r){r[t]=n}return n}function it(e){var t=te(e,\"hx-trigger\");var r=[];if(t){var \nn=Q.config.triggerSpecsCache;r=n&&n[t]||nt(e,t,n)}if(r.length>0){return r}else if(h(e,\"form\")){return[{trigger:\"submit\"}]}else if(h(e,'input[type=\"button\"], input[type=\"submit\"]')){return[{trigger:\"click\"}]}else if(h(e,rt)){return[{trigger:\"change\"}]}else{return[{trigger:\"click\"}]}}function \nat(e){ae(e).cancelled=true}function ot(e,t,r){var n=ae(e);n.timeout=setTimeout(function(){if(se(e)&&n.cancelled!==true){if(!ct(r,e,Wt(\"hx:poll:trigger\",{triggerSpec:r,target:e}))){t(e)}ot(e,t,r)}},r.pollInterval)}function st(e){return location.hostname===e.hostname&&ee(e,\"href\")&&ee(e,\"href\").indexOf(\"#\")!==0}function \nlt(t,r,e){if(t.tagName===\"A\"&&st(t)&&(t.target===\"\"||t.target===\"_self\")||t.tagName===\"FORM\"){r.boosted=true;var n,i;if(t.tagName===\"A\"){n=\"get\";i=ee(t,\"href\")}else{var \na=ee(t,\"method\");n=a?a.toLowerCase():\"get\";if(n===\"get\"){}i=ee(t,\"action\")}e.forEach(function(e){ht(t,function(e,t){if(v(e,Q.config.disableSelector)){p(e);return}he(n,i,e,t)},r,e,true)})}}function ut(e,t){if(e.type===\"submit\"||e.type===\"click\"){if(t.tagName===\"FORM\"){return true}if(h(t,'input[type=\"submit\"], \nbutton')&&v(t,\"form\")!==null){return true}if(t.tagName===\"A\"&&t.href&&(t.getAttribute(\"href\")===\"#\"||t.getAttribute(\"href\").indexOf(\"#\")!==0)){return true}}return false}function ft(e,t){return ae(e).boosted&&e.tagName===\"A\"&&t.type===\"click\"&&(t.ctrlKey||t.metaKey)}function ct(e,t,r){var \nn=e.eventFilter;if(n){try{return n.call(t,r)!==true}catch(e){fe(re().body,\"htmx:eventFilter:error\",{error:e,source:n.source});return true}}return false}function ht(a,o,e,s,l){var u=ae(a);var t;if(s.from){t=Z(a,s.from)}else{t=[a]}if(s.changed){t.forEach(function(e){var t=ae(e);t.lastValue=e.value})}oe(t,function(n){var \ni=function(e){if(!se(a)){n.removeEventListener(s.trigger,i);return}if(ft(a,e)){return}if(l||ut(e,a)){e.preventDefault()}if(ct(s,a,e)){return}var \nt=ae(e);t.triggerSpec=s;if(t.handledFor==null){t.handledFor=[]}if(t.handledFor.indexOf(a)<0){t.handledFor.push(a);if(s.consume){e.stopPropagation()}if(s.target&&e.target){if(!h(e.target,s.target)){return}}if(s.once){if(u.triggeredOnce){return}else{u.triggeredOnce=true}}if(s.changed){var \nr=ae(n);if(r.lastValue===n.value){return}r.lastValue=n.value}if(u.delayed){clearTimeout(u.delayed)}if(u.throttle){return}if(s.throttle>0){if(!u.throttle){o(a,e);u.throttle=setTimeout(function(){u.throttle=null},s.throttle)}}else \nif(s.delay>0){u.delayed=setTimeout(function(){o(a,e)},s.delay)}else{ce(a,\"htmx:trigger\");o(a,e)}}};if(e.listenerInfos==null){e.listenerInfos=[]}e.listenerInfos.push({trigger:s.trigger,listener:i,on:n});n.addEventListener(s.trigger,i)})}var vt=false;var dt=null;function \ngt(){if(!dt){dt=function(){vt=true};window.addEventListener(\"scroll\",dt);setInterval(function(){if(vt){vt=false;oe(re().querySelectorAll(\"[hx-trigger='revealed'],[data-hx-trigger='revealed']\"),function(e){mt(e)})}},200)}}function mt(t){if(!o(t,\"data-hx-revealed\")&&X(t)){t.setAttribute(\"data-hx-revealed\",\"true\");var \ne=ae(t);if(e.initHash){ce(t,\"revealed\")}else{t.addEventListener(\"htmx:afterProcessNode\",function(e){ce(t,\"revealed\")},{once:true})}}}function pt(e,t,r){var n=D(r);for(var i=0;i<n.length;i++){var a=n[i].split(/:(.+)/);if(a[0]===\"connect\"){xt(e,a[1],0)}if(a[0]===\"send\"){bt(e)}}}function \nxt(s,r,n){if(!se(s)){return}if(r.indexOf(\"/\")==0){var e=location.hostname+(location.port?\":\"+location.port:\"\");if(location.protocol==\"https:\"){r=\"wss://\"+e+r}else if(location.protocol==\"http:\"){r=\"ws://\"+e+r}}var \nt=Q.createWebSocket(r);t.onerror=function(e){fe(s,\"htmx:wsError\",{error:e,socket:t});yt(s)};t.onclose=function(e){if([1006,1012,1013].indexOf(e.code)>=0){var t=wt(n);setTimeout(function(){xt(s,r,n+1)},t)}};t.onopen=function(e){n=0};ae(s).webSocket=t;t.addEventListener(\"message\",function(e){if(yt(s)){return}var \nt=e.data;R(s,function(e){t=e.transformResponse(t,null,s)});var r=T(s);var n=l(t);var i=M(n.children);for(var a=0;a<i.length;a++){var o=i[a];Ee(te(o,\"hx-swap-oob\")||\"true\",o,r)}nr(r.tasks)})}function yt(e){if(!se(e)){ae(e).webSocket.close();return true}}function bt(u){var f=c(u,function(e){return \nae(e).webSocket!=null});if(f){u.addEventListener(it(u)[0].trigger,function(e){var t=ae(f).webSocket;var r=xr(u,f);var n=dr(u,\"post\");var i=n.errors;var a=n.values;var o=Hr(u);var s=le(a,o);var \nl=yr(s,u);l[\"HEADERS\"]=r;if(i&&i.length>0){ce(u,\"htmx:validation:halted\",i);return}t.send(JSON.stringify(l));if(ut(e,u)){e.preventDefault()}})}else{fe(u,\"htmx:noWebSocketSourceError\")}}function wt(e){var t=Q.config.wsReconnectDelay;if(typeof t===\"function\"){return t(e)}if(t===\"full-jitter\"){var r=Math.min(e,6);var \nn=1e3*Math.pow(2,r);return n*Math.random()}b('htmx.config.wsReconnectDelay must either be a function or the string \"full-jitter\"')}function St(e,t,r){var n=D(r);for(var i=0;i<n.length;i++){var a=n[i].split(/:(.+)/);if(a[0]===\"connect\"){Et(e,a[1])}if(a[0]===\"swap\"){Ct(e,a[1])}}}function Et(t,e){var \nr=Q.createEventSource(e);r.onerror=function(e){fe(t,\"htmx:sseError\",{error:e,source:r});Tt(t)};ae(t).sseEventSource=r}function Ct(a,o){var s=c(a,Ot);if(s){var l=ae(s).sseEventSource;var u=function(e){if(Tt(s)){return}if(!se(a)){l.removeEventListener(o,u);return}var \nt=e.data;R(a,function(e){t=e.transformResponse(t,null,a)});var r=wr(a);var n=ye(a);var i=T(a);je(r.swapStyle,n,a,t,i);nr(i.tasks);ce(a,\"htmx:sseMessage\",e)};ae(a).sseListener=u;l.addEventListener(o,u)}else{fe(a,\"htmx:noSSESourceError\")}}function Rt(e,t,r){var n=c(e,Ot);if(n){var i=ae(n).sseEventSource;var \na=function(){if(!Tt(n)){if(se(e)){t(e)}else{i.removeEventListener(r,a)}}};ae(e).sseListener=a;i.addEventListener(r,a)}else{fe(e,\"htmx:noSSESourceError\")}}function Tt(e){if(!se(e)){ae(e).sseEventSource.close();return true}}function Ot(e){return ae(e).sseEventSource!=null}function qt(e,t,r,n){var \ni=function(){if(!r.loaded){r.loaded=true;t(e)}};if(n>0){setTimeout(i,n)}else{i()}}function Ht(t,i,e){var a=false;oe(w,function(r){if(o(t,\"hx-\"+r)){var n=te(t,\"hx-\"+r);a=true;i.path=n;i.verb=r;e.forEach(function(e){Lt(t,e,i,function(e,t){if(v(e,Q.config.disableSelector)){p(e);return}he(r,n,e,t)})})}});return a}function \nLt(n,e,t,r){if(e.sseEvent){Rt(n,r,e.sseEvent)}else if(e.trigger===\"revealed\"){gt();ht(n,r,t,e);mt(n)}else if(e.trigger===\"intersect\"){var i={};if(e.root){i.root=ue(n,e.root)}if(e.threshold){i.threshold=parseFloat(e.threshold)}var a=new IntersectionObserver(function(e){for(var t=0;t<e.length;t++){var \nr=e[t];if(r.isIntersecting){ce(n,\"intersect\");break}}},i);a.observe(n);ht(n,r,t,e)}else if(e.trigger===\"load\"){if(!ct(e,n,Wt(\"load\",{elt:n}))){qt(n,r,t,e.delay)}}else if(e.pollInterval>0){t.polling=true;ot(n,r,e)}else{ht(n,r,t,e)}}function \nAt(e){if(Q.config.allowScriptTags&&(e.type===\"text/javascript\"||e.type===\"module\"||e.type===\"\")){var t=re().createElement(\"script\");oe(e.attributes,function(e){t.setAttribute(e.name,e.value)});t.textContent=e.textContent;t.async=false;if(Q.config.inlineScriptNonce){t.nonce=Q.config.inlineScriptNonce}var \nr=e.parentElement;try{r.insertBefore(t,e)}catch(e){b(e)}finally{if(e.parentElement){e.parentElement.removeChild(e)}}}}function Nt(e){if(h(e,\"script\")){At(e)}oe(f(e,\"script\"),function(e){At(e)})}function It(e){var t=e.attributes;for(var r=0;r<t.length;r++){var \nn=t[r].name;if(s(n,\"hx-on:\")||s(n,\"data-hx-on:\")||s(n,\"hx-on-\")||s(n,\"data-hx-on-\")){return true}}return false}function kt(e){var t=null;var r=[];if(It(e)){r.push(e)}if(document.evaluate){var n=document.evaluate('.//*[@*[ starts-with(name(), \"hx-on:\") or starts-with(name(), \"data-hx-on:\") or'+' starts-with(name(), \n\"hx-on-\") or starts-with(name(), \"data-hx-on-\") ]]',e);while(t=n.iterateNext())r.push(t)}else{var i=e.getElementsByTagName(\"*\");for(var a=0;a<i.length;a++){if(It(i[a])){r.push(i[a])}}}return r}function Pt(e){if(e.querySelectorAll){var t=\", [hx-boost] a, [data-hx-boost] a, a[hx-boost], a[data-hx-boost]\";var \nr=e.querySelectorAll(i+t+\", form, [type='submit'], [hx-sse], [data-hx-sse], [hx-ws],\"+\" [data-hx-ws], [hx-ext], [data-hx-ext], [hx-trigger], [data-hx-trigger], [hx-on], [data-hx-on]\");return r}else{return[]}}function Mt(e){var t=v(e.target,\"button, input[type='submit']\");var \n\nr=Dt(e);if(r){r.lastButtonClicked=t}}function Xt(e){var t=Dt(e);if(t){t.lastButtonClicked=null}}function Dt(e){var t=v(e.target,\"button, input[type='submit']\");if(!t){return}var r=g(\"#\"+ee(t,\"form\"))||v(t,\"form\");if(!r){return}return ae(r)}function \nUt(e){e.addEventListener(\"click\",Mt);e.addEventListener(\"focusin\",Mt);e.addEventListener(\"focusout\",Xt)}function Bt(e){var t=Ye(e);var r=0;for(var n=0;n<t.length;n++){const i=t[n];if(i===\"{\"){r++}else if(i===\"}\"){r--}}return r}function Ft(t,e,r){var n=ae(t);if(!Array.isArray(n.onHandlers)){n.onHandlers=[]}var i;var \na=function(e){return Tr(t,function(){if(!i){i=new Function(\"event\",r)}i.call(t,e)})};t.addEventListener(e,a);n.onHandlers.push({event:e,listener:a})}function Vt(e){var t=te(e,\"hx-on\");if(t){var r={};var n=t.split(\"\\n\");var i=null;var a=0;while(n.length>0){var o=n.shift();var \ns=o.match(/^\\s*([a-zA-Z:\\-\\.]+:)(.*)/);if(a===0&&s){o.split(\":\");i=s[1].slice(0,-1);r[i]=s[2]}else{r[i]+=o}a+=Bt(o)}for(var l in r){Ft(e,l,r[l])}}}function jt(e){Ae(e);for(var t=0;t<e.attributes.length;t++){var r=e.attributes[t].name;var n=e.attributes[t].value;if(s(r,\"hx-on\")||s(r,\"data-hx-on\")){var \ni=r.indexOf(\"-on\")+3;var a=r.slice(i,i+1);if(a===\"-\"||a===\":\"){var o=r.slice(i+1);if(s(o,\":\")){o=\"htmx\"+o}else if(s(o,\"-\")){o=\"htmx:\"+o.slice(1)}else if(s(o,\"htmx-\")){o=\"htmx:\"+o.slice(5)}Ft(e,o,n)}}}}function _t(t){if(v(t,Q.config.disableSelector)){p(t);return}var \nr=ae(t);if(r.initHash!==Le(t)){Ne(t);r.initHash=Le(t);Vt(t);ce(t,\"htmx:beforeProcessNode\");if(t.value){r.lastValue=t.value}var e=it(t);var n=Ht(t,r,e);if(!n){if(ne(t,\"hx-boost\")===\"true\"){lt(t,r,e)}else \nif(o(t,\"hx-trigger\")){e.forEach(function(e){Lt(t,e,r,function(){})})}}if(t.tagName===\"FORM\"||ee(t,\"type\")===\"submit\"&&o(t,\"form\")){Ut(t)}var i=te(t,\"hx-sse\");if(i){St(t,r,i)}var a=te(t,\"hx-ws\");if(a){pt(t,r,a)}ce(t,\"htmx:afterProcessNode\")}}function \nzt(e){e=g(e);if(v(e,Q.config.disableSelector)){p(e);return}_t(e);oe(Pt(e),function(e){_t(e)});oe(kt(e),jt)}function $t(e){return e.replace(/([a-z0-9])([A-Z])/g,\"$1-$2\").toLowerCase()}function Wt(e,t){var r;if(window.CustomEvent&&typeof window.CustomEvent===\"function\"){r=new \nCustomEvent(e,{bubbles:true,cancelable:true,detail:t})}else{r=re().createEvent(\"CustomEvent\");r.initCustomEvent(e,true,true,t)}return r}function fe(e,t,r){ce(e,t,le({error:t},r))}function Gt(e){return e===\"htmx:afterProcessNode\"}function R(e,t){oe(Fr(e),function(e){try{t(e)}catch(e){b(e)}})}function \nb(e){if(console.error){console.error(e)}else if(console.log){console.log(\"ERROR: \",e)}}function ce(e,t,r){e=g(e);if(r==null){r={}}r[\"elt\"]=e;var n=Wt(t,r);if(Q.logger&&!Gt(t)){Q.logger(e,t,r)}if(r.error){b(r.error);ce(e,\"htmx:error\",{errorInfo:r})}var i=e.dispatchEvent(n);var a=$t(t);if(i&&a!==t){var \no=Wt(a,n.detail);i=i&&e.dispatchEvent(o)}R(e,function(e){i=i&&(e.onEvent(t,n)!==false&&!n.defaultPrevented)});return i}var Jt=location.pathname+location.search;function Zt(){var e=re().querySelector(\"[hx-history-elt],[data-hx-history-elt]\");return e||re().body}function \nKt(e,t,r,n){if(!U()){return}if(Q.config.historyCacheSize<=0){localStorage.removeItem(\"htmx-history-cache\");return}e=B(e);var i=E(localStorage.getItem(\"htmx-history-cache\"))||[];for(var a=0;a<i.length;a++){if(i[a].url===e){i.splice(a,1);break}}var \no={url:e,content:t,title:r,scroll:n};ce(re().body,\"htmx:historyItemCreated\",{item:o,cache:i});i.push(o);while(i.length>Q.config.historyCacheSize){i.shift()}while(i.length>0){try{localStorage.setItem(\"htmx-history-cache\",JSON.stringify(i));break}catch(e){fe(re().body,\"htmx:historyCacheError\",{cause:e,cache:i});i.shift(\n)}}}function Yt(e){if(!U()){return null}e=B(e);var t=E(localStorage.getItem(\"htmx-history-cache\"))||[];for(var r=0;r<t.length;r++){if(t[r].url===e){return t[r]}}return null}function Qt(e){var t=Q.config.requestClass;var r=e.cloneNode(true);oe(f(r,\".\"+t),function(e){n(e,t)});return r.innerHTML}function er(){var \ne=Zt();var t=Jt||location.pathname+location.search;var r;try{r=re().querySelector('[hx-history=\"false\" i],[data-hx-history=\"false\" \ni]')}catch(e){r=re().querySelector('[hx-history=\"false\"],[data-hx-history=\"false\"]')}if(!r){ce(re().body,\"htmx:beforeHistorySave\",{path:t,historyElt:e});Kt(t,Qt(e),re().title,window.scrollY)}if(Q.config.historyEnabled)history.replaceState({htmx:true},re().title,window.location.href)}function \ntr(e){if(Q.config.getCacheBusterParam){e=e.replace(/org\\.htmx\\.cache-buster=[^&]*&?/,\"\");if(G(e,\"&\")||G(e,\"?\")){e=e.slice(0,-1)}}if(Q.config.historyEnabled){history.pushState({htmx:true},\"\",e)}Jt=e}function rr(e){if(Q.config.historyEnabled)history.replaceState({htmx:true},\"\",e);Jt=e}function \nnr(e){oe(e,function(e){e.call()})}function ir(a){var e=new XMLHttpRequest;var \no={path:a,xhr:e};ce(re().body,\"htmx:historyCacheMiss\",o);e.open(\"GET\",a,true);e.setRequestHeader(\"HX-Request\",\"true\");e.setRequestHeader(\"HX-History-Restore-Request\",\"true\");e.setRequestHeader(\"HX-Current-URL\",re().location.href);e.onload=function(){if(this.status>=200&&this.status<400){ce(re().body,\"htmx:historyCache\nMissLoad\",o);var e=l(this.response);e=e.querySelector(\"[hx-history-elt],[data-hx-history-elt]\")||e;var t=Zt();var r=T(t);var n=Ve(this.response);if(n){var \ni=C(\"title\");if(i){i.innerHTML=n}else{window.document.title=n}}Ue(t,e,r);nr(r.tasks);Jt=a;ce(re().body,\"htmx:historyRestore\",{path:a,cacheMiss:true,serverResponse:this.response})}else{fe(re().body,\"htmx:historyCacheMissLoadError\",o)}};e.send()}function ar(e){er();e=e||location.pathname+location.search;var \nt=Yt(e);if(t){var r=l(t.content);var n=Zt();var i=T(n);Ue(n,r,i);nr(i.tasks);document.title=t.title;setTimeout(function(){window.scrollTo(0,t.scroll)},0);Jt=e;ce(re().body,\"htmx:historyRestore\",{path:e,item:t})}else{if(Q.config.refreshOnHistoryMiss){window.location.reload(true)}else{ir(e)}}}function or(e){var \nt=pe(e,\"hx-indicator\");if(t==null){t=[e]}oe(t,function(e){var t=ae(e);t.requestCount=(t.requestCount||0)+1;e.classList[\"add\"].call(e.classList,Q.config.requestClass)});return t}function sr(e){var t=pe(e,\"hx-disabled-elt\");if(t==null){t=[]}oe(t,function(e){var \nt=ae(e);t.requestCount=(t.requestCount||0)+1;e.setAttribute(\"disabled\",\"\")});return t}function lr(e,t){oe(e,function(e){var t=ae(e);t.requestCount=(t.requestCount||0)-1;if(t.requestCount===0){e.classList[\"remove\"].call(e.classList,Q.config.requestClass)}});oe(t,function(e){var \nt=ae(e);t.requestCount=(t.requestCount||0)-1;if(t.requestCount===0){e.removeAttribute(\"disabled\")}})}function ur(e,t){for(var r=0;r<e.length;r++){var n=e[r];if(n.isSameNode(t)){return true}}return false}function fr(e){if(e.name===\"\"||e.name==null||e.disabled||v(e,\"fieldset[disabled]\")){return \nfalse}if(e.type===\"button\"||e.type===\"submit\"||e.tagName===\"image\"||e.tagName===\"reset\"||e.tagName===\"file\"){return false}if(e.type===\"checkbox\"||e.type===\"radio\"){return e.checked}return true}function cr(e,t,r){if(e!=null&&t!=null){var n=r[e];if(n===undefined){r[e]=t}else \nif(Array.isArray(n)){if(Array.isArray(t)){r[e]=n.concat(t)}else{n.push(t)}}else{if(Array.isArray(t)){r[e]=[n].concat(t)}else{r[e]=[n,t]}}}}function hr(t,r,n,e,i){if(e==null||ur(t,e)){return}else{t.push(e)}if(fr(e)){var a=ee(e,\"name\");var \no=e.value;if(e.multiple&&e.tagName===\"SELECT\"){o=M(e.querySelectorAll(\"option:checked\")).map(function(e){return e.value})}if(e.files){o=M(e.files)}cr(a,o,r);if(i){vr(e,n)}}if(h(e,\"form\")){var s=e.elements;oe(s,function(e){hr(t,r,n,e,i)})}}function \nvr(e,t){if(e.willValidate){ce(e,\"htmx:validation:validate\");if(!e.checkValidity()){t.push({elt:e,message:e.validationMessage,validity:e.validity});ce(e,\"htmx:validation:failed\",{message:e.validationMessage,validity:e.validity})}}}function dr(e,t){var r=[];var n={};var i={};var a=[];var \no=ae(e);if(o.lastButtonClicked&&!se(o.lastButtonClicked)){o.lastButtonClicked=null}var \ns=h(e,\"form\")&&e.noValidate!==true||te(e,\"hx-validate\")===\"true\";if(o.lastButtonClicked){s=s&&o.lastButtonClicked.formNoValidate!==true}if(t!==\"get\"){hr(r,i,a,v(e,\"form\"),s)}hr(r,n,a,e,s);if(o.lastButtonClicked||e.tagName===\"BUTTON\"||e.tagName===\"INPUT\"&&ee(e,\"type\")===\"submit\"){var l=o.lastButtonClicked||e;var \nu=ee(l,\"name\");cr(u,l.value,i)}var f=pe(e,\"hx-include\");oe(f,function(e){hr(r,n,a,e,s);if(!h(e,\"form\")){oe(e.querySelectorAll(rt),function(e){hr(r,n,a,e,s)})}});n=le(n,i);return{errors:a,values:n}}function gr(e,t,r){if(e!==\"\"){e+=\"&\"}if(String(r)===\"[object Object]\"){r=JSON.stringify(r)}var \nn=encodeURIComponent(r);e+=encodeURIComponent(t)+\"=\"+n;return e}function mr(e){var t=\"\";for(var r in e){if(e.hasOwnProperty(r)){var n=e[r];if(Array.isArray(n)){oe(n,function(e){t=gr(t,r,e)})}else{t=gr(t,r,n)}}}return t}function pr(e){var t=new FormData;for(var r in e){if(e.hasOwnProperty(r)){var \nn=e[r];if(Array.isArray(n)){oe(n,function(e){t.append(r,e)})}else{t.append(r,n)}}}return t}function xr(e,t,r){var \nn={\"HX-Request\":\"true\",\"HX-Trigger\":ee(e,\"id\"),\"HX-Trigger-Name\":ee(e,\"name\"),\"HX-Target\":te(t,\"id\"),\"HX-Current-URL\":re().location.href};Rr(e,\"hx-headers\",false,n);if(r!==undefined){n[\"HX-Prompt\"]=r}if(ae(e).boosted){n[\"HX-Boosted\"]=\"true\"}return n}function yr(t,e){var \nr=ne(e,\"hx-params\");if(r){if(r===\"none\"){return{}}else if(r===\"*\"){return t}else if(r.indexOf(\"not \")===0){oe(r.substr(4).split(\",\"),function(e){e=e.trim();delete t[e]});return t}else{var n={};oe(r.split(\",\"),function(e){e=e.trim();n[e]=t[e]});return n}}else{return t}}function br(e){return \nee(e,\"href\")&&ee(e,\"href\").indexOf(\"#\")>=0}function wr(e,t){var r=t?t:ne(e,\"hx-swap\");var n={swapStyle:ae(e).boosted?\"innerHTML\":Q.config.defaultSwapStyle,swapDelay:Q.config.defaultSwapDelay,settleDelay:Q.config.defaultSettleDelay};if(Q.config.scrollIntoViewOnBoost&&ae(e).boosted&&!br(e)){n[\"show\"]=\"top\"}if(r){var \ni=D(r);if(i.length>0){for(var a=0;a<i.length;a++){var o=i[a];if(o.indexOf(\"swap:\")===0){n[\"swapDelay\"]=d(o.substr(5))}else if(o.indexOf(\"settle:\")===0){n[\"settleDelay\"]=d(o.substr(7))}else if(o.indexOf(\"transition:\")===0){n[\"transition\"]=o.substr(11)===\"true\"}else \nif(o.indexOf(\"ignoreTitle:\")===0){n[\"ignoreTitle\"]=o.substr(12)===\"true\"}else if(o.indexOf(\"scroll:\")===0){var s=o.substr(7);var l=s.split(\":\");var u=l.pop();var f=l.length>0?l.join(\":\"):null;n[\"scroll\"]=u;n[\"scrollTarget\"]=f}else if(o.indexOf(\"show:\")===0){var c=o.substr(5);var l=c.split(\":\");var h=l.pop();var \nf=l.length>0?l.join(\":\"):null;n[\"show\"]=h;n[\"showTarget\"]=f}else if(o.indexOf(\"focus-scroll:\")===0){var v=o.substr(\"focus-scroll:\".length);n[\"focusScroll\"]=v==\"true\"}else if(a==0){n[\"swapStyle\"]=o}else{b(\"Unknown modifier in hx-swap: \"+o)}}}}return n}function Sr(e){return \nne(e,\"hx-encoding\")===\"multipart/form-data\"||h(e,\"form\")&&ee(e,\"enctype\")===\"multipart/form-data\"}function Er(t,r,n){var i=null;R(r,function(e){if(i==null){i=e.encodeParameters(t,n,r)}});if(i!=null){return i}else{if(Sr(r)){return pr(n)}else{return mr(n)}}}function T(e){return{tasks:[],elts:[e]}}function Cr(e,t){var \nr=e[0];var n=e[e.length-1];if(t.scroll){var i=null;if(t.scrollTarget){i=ue(r,t.scrollTarget)}if(t.scroll===\"top\"&&(r||i)){i=i||r;i.scrollTop=0}if(t.scroll===\"bottom\"&&(n||i)){i=i||n;i.scrollTop=i.scrollHeight}}if(t.show){var i=null;if(t.showTarget){var \na=t.showTarget;if(t.showTarget===\"window\"){a=\"body\"}i=ue(r,a)}if(t.show===\"top\"&&(r||i)){i=i||r;i.scrollIntoView({block:\"start\",behavior:Q.config.scrollBehavior})}if(t.show===\"bottom\"&&(n||i)){i=i||n;i.scrollIntoView({block:\"end\",behavior:Q.config.scrollBehavior})}}}function \nRr(e,t,r,n){if(n==null){n={}}if(e==null){return n}var i=te(e,t);if(i){var a=i.trim();var o=r;if(a===\"unset\"){return null}if(a.indexOf(\"javascript:\")===0){a=a.substr(11);o=true}else if(a.indexOf(\"js:\")===0){a=a.substr(3);o=true}if(a.indexOf(\"{\")!==0){a=\"{\"+a+\"}\"}var s;if(o){s=Tr(e,function(){return Function(\"return \n(\"+a+\")\")()},{})}else{s=E(a)}for(var l in s){if(s.hasOwnProperty(l)){if(n[l]==null){n[l]=s[l]}}}}return Rr(u(e),t,r,n)}function Tr(e,t,r){if(Q.config.allowEval){return t()}else{fe(e,\"htmx:evalDisallowedError\");return r}}function Or(e,t){return Rr(e,\"hx-vars\",true,t)}function qr(e,t){return \nRr(e,\"hx-vals\",false,t)}function Hr(e){return le(Or(e),qr(e))}function Lr(t,r,n){if(n!==null){try{t.setRequestHeader(r,n)}catch(e){t.setRequestHeader(r,encodeURIComponent(n));t.setRequestHeader(r+\"-URI-AutoEncoded\",\"true\")}}}function Ar(t){if(t.responseURL&&typeof URL!==\"undefined\"){try{var e=new \nURL(t.responseURL);return e.pathname+e.search}catch(e){fe(re().body,\"htmx:badResponseUrl\",{url:t.responseURL})}}}function O(e,t){return t.test(e.getAllResponseHeaders())}function Nr(e,t,r){e=e.toLowerCase();if(r){if(r instanceof Element||I(r,\"String\")){return \nhe(e,t,null,null,{targetOverride:g(r),returnPromise:true})}else{return he(e,t,g(r.source),r.event,{handler:r.handler,headers:r.headers,values:r.values,targetOverride:g(r.target),swapOverride:r.swap,select:r.select,returnPromise:true})}}else{return he(e,t,null,null,{returnPromise:true})}}function Ir(e){var \nt=[];while(e){t.push(e);e=e.parentElement}return t}function kr(e,t,r){var n;var i;if(typeof URL===\"function\"){i=new URL(t,document.location.href);var a=document.location.origin;n=a===i.origin}else{i=t;n=s(t,document.location.origin)}if(Q.config.selfRequestsOnly){if(!n){return false}}return \nce(e,\"htmx:validateUrl\",le({url:i,sameHost:n},r))}function he(t,r,n,i,a,e){var o=null;var s=null;a=a!=null?a:{};if(a.returnPromise&&typeof Promise!==\"undefined\"){var l=new Promise(function(e,t){o=e;s=t})}if(n==null){n=re().body}var M=a.handler||Mr;var X=a.select||null;if(!se(n)){ie(o);return l}var \nu=a.targetOverride||ye(n);if(u==null||u==me){fe(n,\"htmx:targetError\",{target:te(n,\"hx-target\")});ie(s);return l}var f=ae(n);var c=f.lastButtonClicked;if(c){var h=ee(c,\"formaction\");if(h!=null){r=h}var v=ee(c,\"formmethod\");if(v!=null){if(v.toLowerCase()!==\"dialog\"){t=v}}}var d=ne(n,\"hx-confirm\");if(e===undefined){var \nD=function(e){return he(t,r,n,i,a,!!e)};var U={target:u,elt:n,path:r,verb:t,triggeringEvent:i,etc:a,issueRequest:D,question:d};if(ce(n,\"htmx:confirm\",U)===false){ie(o);return l}}var g=n;var m=ne(n,\"hx-sync\");var p=null;var x=false;if(m){var B=m.split(\":\");var \nF=B[0].trim();if(F===\"this\"){g=xe(n,\"hx-sync\")}else{g=ue(n,F)}m=(B[1]||\"drop\").trim();f=ae(g);if(m===\"drop\"&&f.xhr&&f.abortable!==true){ie(o);return l}else if(m===\"abort\"){if(f.xhr){ie(o);return l}else{x=true}}else if(m===\"replace\"){ce(g,\"htmx:abort\")}else if(m.indexOf(\"queue\")===0){var V=m.split(\" \n\");p=(V[1]||\"last\").trim()}}if(f.xhr){if(f.abortable){ce(g,\"htmx:abort\")}else{if(p==null){if(i){var \ny=ae(i);if(y&&y.triggerSpec&&y.triggerSpec.queue){p=y.triggerSpec.queue}}if(p==null){p=\"last\"}}if(f.queuedRequests==null){f.queuedRequests=[]}if(p===\"first\"&&f.queuedRequests.length===0){f.queuedRequests.push(function(){he(t,r,n,i,a)})}else if(p===\"all\"){f.queuedRequests.push(function(){he(t,r,n,i,a)})}else \nif(p===\"last\"){f.queuedRequests=[];f.queuedRequests.push(function(){he(t,r,n,i,a)})}ie(o);return l}}var b=new XMLHttpRequest;f.xhr=b;f.abortable=x;var w=function(){f.xhr=null;f.abortable=false;if(f.queuedRequests!=null&&f.queuedRequests.length>0){var e=f.queuedRequests.shift();e()}};var j=ne(n,\"hx-prompt\");if(j){var \nS=prompt(j);if(S===null||!ce(n,\"htmx:prompt\",{prompt:S,target:u})){ie(o);w();return l}}if(d&&!e){if(!confirm(d)){ie(o);w();return l}}var E=xr(n,u,S);if(t!==\"get\"&&!Sr(n)){E[\"Content-Type\"]=\"application/x-www-form-urlencoded\"}if(a.headers){E=le(E,a.headers)}var _=dr(n,t);var C=_.errors;var \nR=_.values;if(a.values){R=le(R,a.values)}var z=Hr(n);var $=le(R,z);var T=yr($,n);if(Q.config.getCacheBusterParam&&t===\"get\"){T[\"org.htmx.cache-buster\"]=ee(u,\"id\")||\"true\"}if(r==null||r===\"\"){r=re().location.href}var O=Rr(n,\"hx-request\");var W=ae(n).boosted;var q=Q.config.methodsThatUseUrlParams.indexOf(t)>=0;var \nH={boosted:W,useUrlParams:q,parameters:T,unfilteredParameters:$,headers:E,target:u,verb:t,errors:C,withCredentials:a.credentials||O.credentials||Q.config.withCredentials,timeout:a.timeout||O.timeout||Q.config.timeout,path:r,triggeringEvent:i};if(!ce(n,\"htmx:configRequest\",H)){ie(o);w();return \nl}r=H.path;t=H.verb;E=H.headers;T=H.parameters;C=H.errors;q=H.useUrlParams;if(C&&C.length>0){ce(n,\"htmx:validation:halted\",H);ie(o);w();return l}var G=r.split(\"#\");var J=G[0];var L=G[1];var A=r;if(q){A=J;var \nZ=Object.keys(T).length!==0;if(Z){if(A.indexOf(\"?\")<0){A+=\"?\"}else{A+=\"&\"}A+=mr(T);if(L){A+=\"#\"+L}}}if(!kr(n,A,H)){fe(n,\"htmx:invalidPath\",H);ie(s);return l}b.open(t.toUpperCase(),A,true);b.overrideMimeType(\"text/html\");b.withCredentials=H.withCredentials;b.timeout=H.timeout;if(O.noHeaders){}else{for(var N in \nE){if(E.hasOwnProperty(N)){var K=E[N];Lr(b,N,K)}}}var I={xhr:b,target:u,requestConfig:H,etc:a,boosted:W,select:X,pathInfo:{requestPath:r,finalRequestPath:A,anchor:L}};b.onload=function(){try{var e=Ir(n);I.pathInfo.responsePath=Ar(b);M(n,I);lr(k,P);ce(n,\"htmx:afterRequest\",I);ce(n,\"htmx:afterOnLoad\",I);if(!se(n)){var \nt=null;while(e.length>0&&t==null){var r=e.shift();if(se(r)){t=r}}if(t){ce(t,\"htmx:afterRequest\",I);ce(t,\"htmx:afterOnLoad\",I)}}ie(o);w()}catch(e){fe(n,\"htmx:onLoadError\",le({error:e},I));throw \ne}};b.onerror=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:sendError\",I);ie(s);w()};b.onabort=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:sendAbort\",I);ie(s);w()};b.ontimeout=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:timeout\",I);ie(s);w()};if(!ce(n,\"htmx:beforeRequest\",I)){ie\n(o);w();return l}var k=or(n);var P=sr(n);oe([\"loadstart\",\"loadend\",\"progress\",\"abort\"],function(t){oe([b,b.upload],function(e){e.addEventListener(t,function(e){ce(n,\"htmx:xhr:\"+t,{lengthComputable:e.lengthComputable,loaded:e.loaded,total:e.total})})})});ce(n,\"htmx:beforeSend\",I);var Y=q?null:Er(b,n,T);b.send(Y);return \nl}function Pr(e,t){var r=t.xhr;var n=null;var i=null;if(O(r,/HX-Push:/i)){n=r.getResponseHeader(\"HX-Push\");i=\"push\"}else if(O(r,/HX-Push-Url:/i)){n=r.getResponseHeader(\"HX-Push-Url\");i=\"push\"}else \nif(O(r,/HX-Replace-Url:/i)){n=r.getResponseHeader(\"HX-Replace-Url\");i=\"replace\"}if(n){if(n===\"false\"){return{}}else{return{type:i,path:n}}}var a=t.pathInfo.finalRequestPath;var o=t.pathInfo.responsePath;var s=ne(e,\"hx-push-url\");var l=ne(e,\"hx-replace-url\");var u=ae(e).boosted;var f=null;var \nc=null;if(s){f=\"push\";c=s}else if(l){f=\"replace\";c=l}else if(u){f=\"push\";c=o||a}if(c){if(c===\"false\"){return{}}if(c===\"true\"){c=o||a}if(t.pathInfo.anchor&&c.indexOf(\"#\")===-1){c=c+\"#\"+t.pathInfo.anchor}return{type:f,path:c}}else{return{}}}function Mr(l,u){var f=u.xhr;var c=u.target;var e=u.etc;var \nt=u.requestConfig;var h=u.select;if(!ce(l,\"htmx:beforeOnLoad\",u))return;if(O(f,/HX-Trigger:/i)){_e(f,\"HX-Trigger\",l)}if(O(f,/HX-Location:/i)){er();var r=f.getResponseHeader(\"HX-Location\");var v;if(r.indexOf(\"{\")===0){v=E(r);r=v[\"path\"];delete v[\"path\"]}Nr(\"GET\",r,v).then(function(){tr(r)});return}var \nn=O(f,/HX-Refresh:/i)&&\"true\"===f.getResponseHeader(\"HX-Refresh\");if(O(f,/HX-Redirect:/i)){location.href=f.getResponseHeader(\"HX-Redirect\");n&&location.reload();return}if(n){location.reload();return}if(O(f,/HX-Retarget:/i)){if(f.getResponseHeader(\"HX-Retarget\")===\"this\"){u.target=l}else{u.target=ue(l,f.getResponseHead\ner(\"HX-Retarget\"))}}var d=Pr(l,u);var i=f.status>=200&&f.status<400&&f.status!==204;var g=f.response;var a=f.status>=400;var m=Q.config.ignoreTitle;var \no=le({shouldSwap:i,serverResponse:g,isError:a,ignoreTitle:m},u);if(!ce(c,\"htmx:beforeSwap\",o))return;c=o.target;g=o.serverResponse;a=o.isError;m=o.ignoreTitle;u.target=c;u.failed=a;u.successful=!a;if(o.shouldSwap){if(f.status===286){at(l)}R(l,function(e){g=e.transformResponse(g,f,l)});if(d.type){er()}var \ns=e.swapOverride;if(O(f,/HX-Reswap:/i)){s=f.getResponseHeader(\"HX-Reswap\")}var v=wr(l,s);if(v.hasOwnProperty(\"ignoreTitle\")){m=v.ignoreTitle}c.classList.add(Q.config.swappingClass);var p=null;var x=null;var y=function(){try{var e=document.activeElement;var \nt={};try{t={elt:e,start:e?e.selectionStart:null,end:e?e.selectionEnd:null}}catch(e){}var \nr;if(h){r=h}if(O(f,/HX-Reselect:/i)){r=f.getResponseHeader(\"HX-Reselect\")}if(d.type){ce(re().body,\"htmx:beforeHistoryUpdate\",le({history:d},u));if(d.type===\"push\"){tr(d.path);ce(re().body,\"htmx:pushedIntoHistory\",{path:d.path})}else{rr(d.path);ce(re().body,\"htmx:replacedInHistory\",{path:d.path})}}var \nn=T(c);je(v.swapStyle,c,l,g,n,r);if(t.elt&&!se(t.elt)&&ee(t.elt,\"id\")){var i=document.getElementById(ee(t.elt,\"id\"));var \na={preventScroll:v.focusScroll!==undefined?!v.focusScroll:!Q.config.defaultFocusScroll};if(i){if(t.start&&i.setSelectionRange){try{i.setSelectionRange(t.start,t.end)}catch(e){}}i.focus(a)}}c.classList.remove(Q.config.swappingClass);oe(n.elts,function(e){if(e.classList){e.classList.add(Q.config.settlingClass)}ce(e,\"htm\nx:afterSwap\",u)});if(O(f,/HX-Trigger-After-Swap:/i)){var o=l;if(!se(l)){o=re().body}_e(f,\"HX-Trigger-After-Swap\",o)}var s=function(){oe(n.tasks,function(e){e.call()});oe(n.elts,function(e){if(e.classList){e.classList.remove(Q.config.settlingClass)}ce(e,\"htmx:afterSettle\",u)});if(u.pathInfo.anchor){var \ne=re().getElementById(u.pathInfo.anchor);if(e){e.scrollIntoView({block:\"start\",behavior:\"auto\"})}}if(n.title&&!m){var t=C(\"title\");if(t){t.innerHTML=n.title}else{window.document.title=n.title}}Cr(n.elts,v);if(O(f,/HX-Trigger-After-Settle:/i)){var \nr=l;if(!se(l)){r=re().body}_e(f,\"HX-Trigger-After-Settle\",r)}ie(p)};if(v.settleDelay>0){setTimeout(s,v.settleDelay)}else{s()}}catch(e){fe(l,\"htmx:swapError\",u);ie(x);throw e}};var b=Q.config.globalViewTransitions;if(v.hasOwnProperty(\"transition\")){b=v.transition}if(b&&ce(l,\"htmx:beforeTransition\",u)&&typeof \nPromise!==\"undefined\"&&document.startViewTransition){var w=new Promise(function(e,t){p=e;x=t});var S=y;y=function(){document.startViewTransition(function(){S();return w})}}if(v.swapDelay>0){setTimeout(y,v.swapDelay)}else{y()}}if(a){fe(l,\"htmx:responseError\",le({error:\"Response Status Error Code \"+f.status+\" from \n\"+u.pathInfo.requestPath},u))}}var Xr={};function Dr(){return{init:function(e){return null},onEvent:function(e,t){return true},transformResponse:function(e,t,r){return e},isInlineSwap:function(e){return false},handleSwap:function(e,t,r,n){return false},encodeParameters:function(e,t,r){return null}}}function \nUr(e,t){if(t.init){t.init(r)}Xr[e]=le(Dr(),t)}function Br(e){delete Xr[e]}function Fr(e,r,n){if(e==undefined){return r}if(r==undefined){r=[]}if(n==undefined){n=[]}var t=te(e,\"hx-ext\");if(t){oe(t.split(\",\"),function(e){e=e.replace(/ /g,\"\");if(e.slice(0,7)==\"ignore:\"){n.push(e.slice(7));return}if(n.indexOf(e)<0){var \nt=Xr[e];if(t&&r.indexOf(t)<0){r.push(t)}}})}return Fr(u(e),r,n)}var Vr=false;re().addEventListener(\"DOMContentLoaded\",function(){Vr=true});function jr(e){if(Vr||re().readyState===\"complete\"){e()}else{re().addEventListener(\"DOMContentLoaded\",e)}}function \n_r(){if(Q.config.includeIndicatorStyles!==false){re().head.insertAdjacentHTML(\"beforeend\",\"<style>                      .\"+Q.config.indicatorClass+\"{opacity:0}                      .\"+Q.config.requestClass+\" .\"+Q.config.indicatorClass+\"{opacity:1; transition: opacity 200ms ease-in;}                      \n.\"+Q.config.requestClass+\".\"+Q.config.indicatorClass+\"{opacity:1; transition: opacity 200ms ease-in;}                    </style>\")}}function zr(){var e=re().querySelector('meta[name=\"htmx-config\"]');if(e){return E(e.content)}else{return null}}function $r(){var \ne=zr();if(e){Q.config=le(Q.config,e)}}jr(function(){$r();_r();var e=re().body;zt(e);var t=re().querySelectorAll(\"[hx-trigger='restored'],[data-hx-trigger='restored']\");e.addEventListener(\"htmx:abort\",function(e){var t=e.target;var r=ae(t);if(r&&r.xhr){r.xhr.abort()}});const \nr=window.onpopstate?window.onpopstate.bind(window):null;window.onpopstate=function(e){if(e.state&&e.state.htmx){ar();oe(t,function(e){ce(e,\"htmx:restored\",{document:re(),triggerEvent:ce})})}else{if(r){r(e)}}};setTimeout(function(){ce(e,\"htmx:load\",{});e=null},0)});return Q}()}); \n \nFix:  \n1.\u200b The following buttons do not work on panel 2. Generate Study Material: \na.\u200b Summary \nb.\u200b Key Terms \nc.\u200b Practice Questions \nd.\u200b External Resources \n \n",
        "added_at": "2025-11-30T01:34:48.762532"
      },
      {
        "name": "326 Proj.pdf",
        "type": "document",
        "text": "Python\ngit add .  \ngit commit -m \u201c   \u201c \ngit push -u origin main  \n \n \nmain.py:  \n \nimport os \nimport json \nimport uuid \nimport subprocess \nfrom datetime import datetime \nfrom typing import Dict, Any \n \nimport fitz  # PyMuPDF \nfrom pptx import Presentation \nfrom fastapi import FastAPI, Request, UploadFile, File, Form \nfrom fastapi.responses import HTMLResponse, PlainTextResponse \nfrom fastapi.staticfiles import StaticFiles \nfrom fastapi.templating import Jinja2Templates \nimport google.generativeai as genai \n \n# Ai model  \ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\")) \nGEM_MODEL = genai.GenerativeModel(\"gemini-1.5-flash\") \n \n# ----------------- CONFIG ----------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \nUPLOAD_DIR = os.path.join(BASE_DIR, \"upload\") \nAUDIO_DIR = os.path.join(UPLOAD_DIR, \"audio\") \nDOC_DIR = os.path.join(UPLOAD_DIR, \"docs\") \nSESSIONS_PATH = os.path.join(BASE_DIR, \"sessions.json\") \n \nos.makedirs(UPLOAD_DIR, exist_ok=True) \nos.makedirs(AUDIO_DIR, exist_ok=True) \nos.makedirs(DOC_DIR, exist_ok=True) \n \n#client = (api_key=os.getenv(\"GEMINI_API_KEY\")) \n \napp = FastAPI(title=\"Helektron Study Assistant\") \napp.mount(\"/static\", StaticFiles(directory=os.path.join(BASE_DIR, \"static\")), \nname=\"static\") \ntemplates = Jinja2Templates(directory=os.path.join(BASE_DIR, \"templates\")) \n \n\n# ----------------- SESSION STORAGE ----------------- \ndef load_sessions() -> Dict[str, Any]: \n    if not os.path.exists(SESSIONS_PATH): \n        return {} \n    with open(SESSIONS_PATH, \"r\") as f: \n        try: \n            return json.load(f) \n        except json.JSONDecodeError: \n            return {} \n \ndef save_sessions(sessions: Dict[str, Any]) -> None: \n    with open(SESSIONS_PATH, \"w\") as f: \n        json.dump(sessions, f, indent=2) \n \nsessions: Dict[str, Any] = load_sessions() \n \ndef get_or_create_session(session_id: str | None) -> str: \n    global sessions \n    if session_id and session_id in sessions: \n        return session_id \n    new_id = str(uuid.uuid4()) \n    sessions[new_id] = { \n        \"created_at\": datetime.utcnow().isoformat(), \n        \"files\": [],   # list of {\"name\",\"type\",\"text\"} \n    } \n    save_sessions(sessions) \n    return new_id \n \ndef add_text_to_session(session_id: str, filename: str, file_type: str, text: \nstr) -> None: \n    global sessions \n    if session_id not in sessions: \n        session_id = get_or_create_session(None) \n    sessions[session_id][\"files\"].append({ \n        \"name\": filename, \n        \"type\": file_type, \n        \"text\": text, \n        \"added_at\": datetime.utcnow().isoformat() \n    }) \n    save_sessions(sessions) \n \ndef get_session_text(session_id: str) -> str: \n    session = sessions.get(session_id) \n    if not session: \n\n        return \"\" \n    blocks = [f\"--- {f['name']} ({f['type']}) ---\\n{f['text']}\" for f in \nsession[\"files\"] if f.get(\"text\")] \n    return \"\\n\\n\".join(blocks) \n \n# ----------------- HELPERS: FILE TEXT EXTRACTION ----------------- \ndef extract_txt(path: str) -> str: \n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        return f.read() \n \ndef extract_pdf(path: str) -> str: \n    doc = fitz.open(path) \n    texts = [] \n    for page in doc: \n        texts.append(page.get_text()) \n    doc.close() \n    return \"\\n\".join(texts) \n \ndef extract_pptx(path: str) -> str: \n    prs = Presentation(path) \n    texts = [] \n    for slide in prs.slides: \n        for shape in slide.shapes: \n            if hasattr(shape, \"text\"): \n                texts.append(shape.text) \n    return \"\\n\".join(texts) \n \ndef transcribe_audio(path: str) -> str: \n    # Find whisper binary \n    candidates = [ \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"build\", \"bin\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper-main\", \"build\", \"bin\", \"whisper-cli\"), \n    ] \n    whisper_bin = next((c for c in candidates if os.path.exists(c)), None) \n    if not whisper_bin: \n        raise RuntimeError(\"Whisper binary not found. Build whisper.cpp \nfirst.\") \n \n    wav_path = path + \".wav\" \n    subprocess.run( \n        [\"ffmpeg\", \"-y\", \"-i\", path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \n\"-ac\", \"1\", wav_path], \n        check=True \n\n    ) \n \n    out_prefix = path + \"_out\" \n    subprocess.run( \n        [whisper_bin, \"-m\", os.path.join(BASE_DIR, \"whisper.cpp\", \"models\", \n\"ggml-base.en.bin\"), \n         \"-f\", wav_path, \"--output-txt\", \"--output-file\", out_prefix], \n        check=True \n    ) \n \n    txt_path = out_prefix + \".txt\" \n    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        transcript = f.read() \n \n    # cleanup \n    for p in [wav_path, txt_path]: \n        if os.path.exists(p): \n            os.remove(p) \n \n    return transcript \n \n# ----------------- GPT PROMPTS ----------------- \ndef get_summary_prompt(transcript: str) -> str: \n    return f\"\"\" \nBased on the following combined study materials (lectures, slides, PDFs, notes, \ntranscripts), \ncreate a **structured, detailed, and academically accurate study summary**. \n \nOrganize the output into the following clearly labeled sections: \n \n- **Overview**: Briefly describe the overall topic and goals. \n- **Key Concepts Introduced**: List and explain major ideas, theories, \nformulas, or processes. \n- **Detailed Topic Breakdown**: Group related ideas and summarize explanations, \nreasoning steps, and relationships. \n- **Important Definitions**: Provide concise definitions for technical terms or \ndomain-specific vocabulary. \n- **Examples or Applications**: Summarize any examples, demonstrations, or \nreal-world applications. \n- **Main Takeaways**: What a student should remember after studying this \nmaterial. \n \nExpectations: \n- Use clear, concise bullet points. \n\n- Prioritize conceptual correctness. \n- Use simple academic language suitable for undergraduate study. \n- If information is unclear or missing, label it as `TBD`. \n \n---BEGIN MATERIAL--- \n{transcript} \n---END MATERIAL--- \n\"\"\" \n \ndef get_keyterms_prompt(text: str) -> str: \n    return f\"\"\" \nFrom the combined study materials below, extract **10\u201320 key terms** with short \ndefinitions. \n \nFor each term, include: \n- The term \n- A 1\u20132 sentence definition \n- (Optional) A quick note on why it matters in this context. \n \nFormat as a bulleted list. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_questions_prompt(text: str) -> str: \n    return f\"\"\" \nCreate **8\u201312 practice questions** based on the combined study materials below. \n \nInclude a mix of: \n- Conceptual understanding questions \n- Short-answer questions \n- Application/problem-style questions (where possible) \n \nDo NOT provide answers, only questions. \nGroup them into sections if appropriate (e.g., 'Conceptual', 'Application'). \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_resources_prompt(text: str) -> str: \n\n    return f\"\"\" \nBased on the topics and concepts in the combined study materials below, \nrecommend **3\u20137 high-quality external resources** for further study. \n \nEach resource should include: \n- Title \n- Type (e.g., textbook chapter, .edu article, video lecture) \n- Source (e.g., university, well-known platform) \n- 1\u20132 sentences on why it is relevant. \n \nPrefer: \n- .edu domains \n- Reputable textbooks \n- Well-known educational channels \n \nYou do NOT need to provide URLs, just clear, identifiable references. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef call_gemini(prompt: str) -> str: \n    try: \n        response = GEM_MODEL.generate_content(prompt) \n        return response.text.strip() \n    except Exception as e: \n        return f\"[Gemini API Error: {e}]\" \n \n# ----------------- ROUTES ----------------- \n@app.get(\"/\", response_class=HTMLResponse) \ndef index(request: Request): \n    return templates.TemplateResponse(\"index.html\", {\"request\": request}) \n \n# Upload any material (txt, pdf, pptx, mp4, m4a) \n@app.post(\"/upload\", response_class=HTMLResponse) \nasync def upload_material( \n    request: Request, \n    file: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    global sessions \n \n    session_id = get_or_create_session(session_id) \n\n    filename = file.filename \n    ext = filename.split(\".\")[-1].lower() \n \n    # Save file \n    if ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n        save_dir = AUDIO_DIR \n        kind = \"audio\" \n    else: \n        save_dir = DOC_DIR \n        kind = \"document\" \n \n    os.makedirs(save_dir, exist_ok=True) \n    saved_path = os.path.join(save_dir, f\"{uuid.uuid4().hex}_{filename}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await file.read()) \n \n    # Extract text \n    try: \n        if ext == \"txt\": \n            text = extract_txt(saved_path) \n        elif ext == \"pdf\": \n            text = extract_pdf(saved_path) \n        elif ext in [\"pptx\"]: \n            text = extract_pptx(saved_path) \n        elif ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n            text = transcribe_audio(saved_path) \n        else: \n            text = f\"[Unsupported file type: {ext}]\" \n    except Exception as e: \n        text = f\"[Error processing file {filename}: {e}]\" \n \n    add_text_to_session(session_id, filename, kind, text) \n \n    # Render updated materials list (fragment) \n    session = sessions[session_id] \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n\n \n# Live transcription: audio blob from the browser \n@app.post(\"/upload_live_audio\", response_class=HTMLResponse) \nasync def upload_live_audio( \n    request: Request, \n    audio: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    session_id = get_or_create_session(session_id) \n    filename = audio.filename or \"live_recording.webm\" \n    ext = filename.split(\".\")[-1].lower() \n \n    os.makedirs(AUDIO_DIR, exist_ok=True) \n    saved_path = os.path.join(AUDIO_DIR, f\"live_{uuid.uuid4().hex}.{ext}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await audio.read()) \n \n    try: \n        text = transcribe_audio(saved_path) \n    except Exception as e: \n        text = f\"[Error transcribing live audio: {e}]\" \n \n    add_text_to_session(session_id, filename, \"audio-live\", text) \n    session = sessions[session_id] \n \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# ---- Study tools (summary, key terms, questions, resources) ---- \n \n@app.get(\"/summary/{session_id}\", response_class=PlainTextResponse) \ndef generate_summary(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_summary_prompt(text) \n\nPython\n        content = call_gemini(prompt) \n    # Return plain text directly to HTMX target \n    return content \n \n \n@app.get(\"/keyterms/{session_id}\", response_class=PlainTextResponse) \ndef generate_keyterms(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_keyterms_prompt(text) \n        content = call_gemini(prompt) \n    return content \n \n \n@app.get(\"/questions/{session_id}\", response_class=PlainTextResponse) \ndef generate_questions_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_questions_prompt(text) \n        content = call_gemini(prompt) \n    return content \n \n \n@app.get(\"/resources/{session_id}\", response_class=PlainTextResponse) \ndef generate_resources_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_resources_prompt(text) \n        content = call_gemini(prompt) \n    return content \nRag Folder:  \n1.\u200b build_vector_store.py  \n \nimport os \nimport json \n\nimport faiss \nfrom langchain_community.vectorstores import FAISS \nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain_core.documents import Document \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter \n \n# -------------------------------------------- \n# Correct project paths \n# -------------------------------------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \n \n# Uploaded document text is stored here by your FastAPI app \nDOC_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"docs\") \n \n# Vector store output directory \nVS_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"vs\") \n \nos.makedirs(VS_DIR, exist_ok=True) \n \n \n# -------------------------------------------- \n# Load documents from upload/docs/ \n# -------------------------------------------- \ndef load_documents(): \n    docs = [] \n    if not os.path.exists(DOC_DIR): \n        print(f\"Missing directory {DOC_DIR}. No documents found.\") \n        return docs \n \n    for filename in os.listdir(DOC_DIR): \n        path = os.path.join(DOC_DIR, filename) \n        if not os.path.isfile(path): \n            continue \n \n        try: \n            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n                text = f.read().strip() \n                if text: \n                    docs.append(Document(page_content=text, metadata={\"source\": \nfilename})) \n        except Exception as e: \n            print(f\"Error loading {filename}: {e}\") \n \n    return docs \n\n \n \n# -------------------------------------------- \n# Build vector store \n# -------------------------------------------- \ndef build_vector_store(): \n    print(f\"Building vector store from {DOC_DIR}...\") \n \n    docs = load_documents() \n \n    if not docs: \n        print(\"No documents found in upload/docs/. Upload files in the web app \nfirst.\") \n        return \n \n    print(f\"Loaded {len(docs)} raw documents.\") \n \n    # ------------------------------- \n    # Chunk documents \n    # ------------------------------- \n    splitter = RecursiveCharacterTextSplitter( \n        chunk_size=1000, # ~750 tokens \n        chunk_overlap=200, # contextual overlap \n        length_function=len \n    ) \n \n    split_docs = [] \n    for doc in docs: \n        chunks = splitter.split_text(doc.page_content) \n        for chunk in chunks: \n            split_docs.append( \n                Document( \n                    page_content=chunk, \n                    metadata=doc.metadata  # keep source info \n                ) \n            ) \n \n    print(f\"Created {len(split_docs)} total chunks.\") \n \n    # ------------------------------- \n    # Generate embeddings \n    # ------------------------------- \n    embeddings = HuggingFaceEmbeddings() \n \n\nPython\n    print(\"Generating embeddings...\") \n    vectorstore = FAISS.from_documents(split_docs, embeddings) \n \n    # ------------------------------- \n    # Save FAISS index + metadata \n    # ------------------------------- \n    print(\"Saving FAISS index and metadata...\") \n \n    faiss.write_index(vectorstore.index, os.path.join(VS_DIR, \"vs.faiss\")) \n \n    with open(os.path.join(VS_DIR, \"docs.json\"), \"w\") as f: \n        json.dump([d.dict() for d in split_docs], f, indent=2) \n \n    print(\"\\n\ud83c\udf89 Vector store built successfully!\") \n    print(f\"Saved to: {VS_DIR}\") \n \n \nif __name__ == \"__main__\": \n    build_vector_store() \n \n2.\u200b rag_utils.py \n \nimport os \nimport json \nimport numpy as np \nfrom google import genai \n \n# Load Gemini client \nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\")) \n \n \n# ----------------------------- \n# Helper: Load vector store \n# ----------------------------- \ndef load_store(path): \n    if not os.path.exists(path): \n        return [] \n    with open(path, \"r\") as f: \n        return json.load(f) \n \n \n\n# ----------------------------- \n# Helper: Save vector store \n# ----------------------------- \ndef save_store(path, data): \n    with open(path, \"w\") as f: \n        json.dump(data, f, indent=4) \n \n \n# ----------------------------- \n# Embed text using Gemini \n# ----------------------------- \ndef embed_text(text: str): \n    \"\"\" \n    Uses Gemini embeddings. \n    Returns list[float]. \n    \"\"\" \n \n    result = client.models.embed_content( \n        model=\"text-embedding-004\", \n        contents=text, \n    ) \n \n    # Gemini wraps embeddings inside .output_embedding \n    return result.output_embedding \n \n \n# ----------------------------- \n# Add text to class vector store \n# ----------------------------- \ndef add_to_vector_store(class_id: str, text: str): \n    \"\"\" \n    Creates or appends to the vector store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    store = load_store(store_path) \n \n    embedding = embed_text(text) \n \n    store.append({ \n        \"text\": text, \n        \"embedding\": embedding, \n\n    }) \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Build store from scratch (optional) \n# ----------------------------- \ndef build_vector_store(class_id: str, text: str): \n    \"\"\" \n    Clears and rebuilds the entire store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    embedding = embed_text(text) \n \n    store = [ \n        { \n            \"text\": text, \n            \"embedding\": embedding, \n        } \n    ] \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Search RAG store using cosine similarity \n# ----------------------------- \ndef search_vector_store(class_id: str, query: str): \n    \"\"\" \n    Returns the most relevant text chunk for the given query. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    store = load_store(store_path) \n \n    if not store: \n        return None \n \n    query_embedding = embed_text(query) \n    query_vec = np.array(query_embedding) \n\nHTML\n \n    best_score = -999 \n    best_text = None \n \n    for item in store: \n        vec = np.array(item[\"embedding\"]) \n \n        # cosine similarity \n        score = np.dot(query_vec, vec) / ( \n            np.linalg.norm(query_vec) * np.linalg.norm(vec) \n        ) \n \n        if score > best_score: \n            best_score = score \n            best_text = item[\"text\"] \n \n    return best_text \n \n \nTemplates \n1.\u200b templates/index.html  \n \n{% extends \"base.html\" %} \n{% block content %} \n \n<h1>Helektron Study Assistant</h1> \n<p class=\"subtitle\"> \n    Upload lecture materials (PDF, slides, audio, notes), then generate \nsummaries, key terms, questions, and more. \n</p> \n \n<div class=\"layout\"> \n \n    <!-- LEFT PANEL --> \n    <div class=\"card\"> \n        <h2>1. Upload Materials</h2> \n \n        <div class=\"upload-box\"> \n            <p><strong>Upload files:</strong> .txt, .pdf, .pptx, .mp4, .m4a</p> \n \n            <form \n\n                hx-post=\"/upload\" \n                hx-target=\"#materials-panel\" \n                hx-swap=\"innerHTML\" \n                hx-indicator=\"#upload-loading\" \n                enctype=\"multipart/form-data\"> \n \n                <input type=\"file\" name=\"file\" required> \n                <input type=\"hidden\" name=\"session_id\" id=\"session_id_input\"> \n                <br><br> \n                <button class=\"primary\" type=\"submit\">Upload</button> \n            </form> \n        </div> \n \n        <div class=\"upload-box\"> \n            <p><strong>Live Transcription:</strong> record audio and transcribe \nwith Whisper.</p> \n            <button class=\"secondary\" id=\"record-btn\">\ud83c\udf99 Start \nRecording</button> \n            <p id=\"record-status\" style=\"font-size: 12px; color:#555; \nmargin-top:6px;\"></p> \n        </div> \n \n        <div id=\"upload-loading\" class=\"htmx-indicator\"> \n            Uploading/processing material... \n        </div> \n \n        <!-- \ud83d\udd25 THIS GETS UPDATED BY upload_status.html --> \n        <div id=\"materials-panel\"> \n            <em>No materials uploaded yet.</em> \n        </div> \n \n    </div> \n \n    <!-- RIGHT PANEL --> \n    <div class=\"card\"> \n        <h2>2. Generate Study Material</h2> \n \n        <p style=\"font-size:13px; color:#555;\">Choose what you want the AI to \ngenerate.</p> \n \n        <div style=\"display:flex; gap:8px; flex-wrap:wrap; \nmargin-bottom:10px;\"> \n            <button class=\"primary\" \n                    id=\"btn-summary\" \n\n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Summary \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-keyterms\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Key Terms \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-questions\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Practice Questions \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-resources\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                External Resources \n            </button> \n        </div> \n \n        <div id=\"generate-loading\" class=\"htmx-indicator\"> \n            Generating material... \n        </div> \n \n        <div id=\"results-panel\"> \n            <em>Your generated study content will appear here.</em> \n        </div> \n \n    </div> \n\nHTML\nHTML\n</div> \n \n<script src=\"/static/app.js\"></script> \n \n{% endblock %} \n \n2.\u200b templates/base.html  \n \n<!DOCTYPE html> \n<html lang=\"en\"> \n<head> \n    <meta charset=\"UTF-8\"> \n    <title>Helektron Study Assistant</title> \n    <link rel=\"stylesheet\" href=\"/static/styles.css\"> \n</head> \n \n<body> \n<div class=\"app-shell\"> \n    {% block content %}{% endblock %} \n</div> \n \n<script src=\"https://unpkg.com/htmx.org@1.9.10\"></script> \n</body> \n</html> \n \n3.\u200b Fragments folder \na.\u200b templates/fragments/class_list.html  \n \n<ul> \n    {% for cls in classes %} \n    <li> \n        <button  \n            hx-post=\"/set_active_class\"  \n            hx-vals='{\"class_id\": \"{{ cls.id }}\"}' \n            hx-target=\"#upload-list\" \n            hx-swap=\"innerHTML\" \n            class=\"{% if cls.id == active_class_id %}active{% endif %}\" \n\nHTML\nHTML\nHTML\n        > \n            {{ cls.name }} \n        </button> \n    </li> \n    {% endfor %} \n</ul> \n \nb.\u200b templates/fragments/upload_status.html  \n \n{% if session_id %} \n<div data-session-id=\"{{ session_id }}\"></div> \n{% endif %} \n \n{% if files %} \n<ul> \n    {% for f in files %} \n    <li><strong>{{ f.name }}</strong> ({{ f.type }})</li> \n    {% endfor %} \n</ul> \n{% else %} \n<em>No materials uploaded yet.</em> \n{% endif %} \n \nc.\u200b templates/fragments/summary.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n \nd.\u200b templates/fragments/resources.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n\nHTML\nHTML\nCSS\ne.\u200b templates/fragments/questions.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \nf.\u200b\ntemplates/fragments/keyterms.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n \n \nStatic:  \n1.\u200b static/styles.css \n \nbody { \n    margin: 0; \n    font-family: system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", \nsans-serif; \n    background: #f4f4fb; \n} \n \n.app-shell { \n    max-width: 1100px; \n    margin: 0 auto; \n    padding: 24px; \n} \n \nh1 { \n    font-size: 28px; \n    margin-bottom: 8px; \n} \n \n.subtitle { \n    color: #555; \n    margin-bottom: 24px; \n} \n\n \n.layout { \n    display: grid; \n    grid-template-columns: 1.2fr 1.8fr; \n    gap: 20px; \n} \n \n.card { \n    background: white; \n    border-radius: 12px; \n    padding: 16px 20px; \n    box-shadow: 0 1px 3px rgba(15,23,42,0.08); \n    border: 1px solid #e2e8f0; \n} \n \n.card h2 { \n    font-size: 18px; \n    margin-top: 0; \n    margin-bottom: 12px; \n} \n \n.upload-box { \n    border: 2px dashed #cbd5f5; \n    border-radius: 10px; \n    padding: 16px; \n    text-align: center; \n    background: #f9fafb; \n    margin-bottom: 12px; \n} \n \n.upload-box input[type=\"file\"] { \n    margin-top: 8px; \n} \n \nbutton.primary { \n    background: #4f46e5; \n    color: white; \n    border: none; \n    border-radius: 8px; \n    padding: 10px 16px; \n    font-weight: 600; \n    cursor: pointer; \n    font-size: 14px; \n} \n\n \nbutton.primary:disabled { \n    opacity: 0.6; \n    cursor: not-allowed; \n} \n \nbutton.primary:hover:not(:disabled) { \n    background: #4338ca; \n} \n \nbutton.secondary { \n    background: white; \n    color: #4f46e5; \n    border: 1px solid #cbd5f5; \n    border-radius: 8px; \n    padding: 8px 14px; \n    font-weight: 500; \n    cursor: pointer; \n    font-size: 13px; \n} \n \nbutton.secondary:hover { \n    background: #eef2ff; \n} \n \n#materials-list { \n    font-size: 14px; \n    max-height: 200px; \n    overflow-y: auto; \n} \n \n#materials-list ul { \n    padding-left: 18px; \n} \n \n#results-panel { \n    min-height: 250px; \n    font-size: 14px; \n    white-space: pre-wrap; \n} \n \n/* htmx loading indicator */ \n.htmx-indicator { \n    display: none; \n\nJavaScript\n    margin-bottom: 8px; \n    font-size: 13px; \n    color: #6b21a8; \n} \n \n.htmx-request .htmx-indicator { \n    display: block; \n} \n2.\u200b static/app.js \n \nlet currentSessionId = null; \n \nfunction setSessionId(id) { \n    currentSessionId = id; \n    const hidden = document.getElementById(\"session_id_input\"); \n    if (hidden) hidden.value = id; \n    enableStudyButtons(); \n} \n \nfunction enableStudyButtons() { \n    if (!currentSessionId) return; \n \n    const summaryBtn = document.getElementById(\"btn-summary\"); \n    const keyBtn     = document.getElementById(\"btn-keyterms\"); \n    const qBtn       = document.getElementById(\"btn-questions\"); \n    const rBtn       = document.getElementById(\"btn-resources\"); \n \n    [summaryBtn, keyBtn, qBtn, rBtn].forEach(btn => btn.disabled = false); \n \n    summaryBtn.setAttribute(\"hx-get\", `/summary/${currentSessionId}`); \n    keyBtn.setAttribute(\"hx-get\", `/keyterms/${currentSessionId}`); \n    qBtn.setAttribute(\"hx-get\", `/questions/${currentSessionId}`); \n    rBtn.setAttribute(\"hx-get\", `/resources/${currentSessionId}`); \n} \n \ndocument.body.addEventListener(\"htmx:afterSettle\", function () { \n    // Look for the element *inside* #materials-panel that has data-session-id \n    const sidElem = document.querySelector(\"#materials-panel \n[data-session-id]\"); \n    const sid = sidElem ? sidElem.getAttribute(\"data-session-id\") : null; \n \n\n    if (sid && sid !== currentSessionId) { \n        setSessionId(sid); \n    } \n}); \n \n// recording JS: \nlet mediaRecorder = null; \nlet recordedChunks = []; \n \nconst recordBtn = document.getElementById(\"record-btn\"); \nconst recordStatus = document.getElementById(\"record-status\"); \n \nrecordBtn.addEventListener(\"click\", async () => { \n    if (!mediaRecorder || mediaRecorder.state === \"inactive\") { \n        try { \n            const stream = await navigator.mediaDevices.getUserMedia({ audio: \ntrue }); \n            mediaRecorder = new MediaRecorder(stream); \n            recordedChunks = []; \n \n            mediaRecorder.ondataavailable = (e) => { \n                if (e.data.size > 0) recordedChunks.push(e.data); \n            }; \n \n            mediaRecorder.onstop = async () => { \n                const blob = new Blob(recordedChunks, { type: \"audio/webm\" }); \n                await uploadLiveAudio(blob); \n                stream.getTracks().forEach(t => t.stop()); \n                recordStatus.textContent = \"Recording finished. \nTranscribing...\"; \n            }; \n \n            mediaRecorder.start(); \n            recordBtn.textContent = \"\u23f9 Stop Recording\"; \n            recordStatus.textContent = \"Recording...\"; \n        } catch (err) { \n            console.error(err); \n            alert(\"Could not access microphone.\"); \n        } \n    } else if (mediaRecorder.state === \"recording\") { \n        mediaRecorder.stop(); \n        recordBtn.textContent = \"\ud83c\udf99 Start Recording\"; \n    } \n}); \n\nJavaScript\n \nasync function uploadLiveAudio(blob) { \n    const formData = new FormData(); \n    formData.append(\"audio\", blob, \"live_recording.webm\"); \n    if (currentSessionId) { \n        formData.append(\"session_id\", currentSessionId); \n    } \n \n    const resp = await fetch(\"/upload_live_audio\", { \n        method: \"POST\", \n        body: formData \n    }); \n \n    const html = await resp.text(); \n    document.getElementById(\"materials-panel\").innerHTML = html; \n    recordStatus.textContent = \"Live audio processed.\"; \n} \n3.\u200b static/htmx.min.js \n \n(function(e,t){if(typeof define===\"function\"&&define.amd){define([],t)}else if(typeof module===\"object\"&&module.exports){module.exports=t()}else{e.htmx=e.htmx||t()}})(typeof self!==\"undefined\"?self:this,function(){return function(){\"use strict\";var \nQ={onLoad:F,process:zt,on:de,off:ge,trigger:ce,ajax:Nr,find:C,findAll:f,closest:v,values:function(e,t){var r=dr(e,t||\"post\");return \nr.values},remove:_,addClass:z,removeClass:n,toggleClass:$,takeClass:W,defineExtension:Ur,removeExtension:Br,logAll:V,logNone:j,logger:null,config:{historyEnabled:true,historyCacheSize:10,refreshOnHistoryMiss:false,defaultSwapStyle:\"innerHTML\",defaultSwapDelay:0,defaultSettleDelay:20,includeIndicatorStyles:true,indicat\norClass:\"htmx-indicator\",requestClass:\"htmx-request\",addedClass:\"htmx-added\",settlingClass:\"htmx-settling\",swappingClass:\"htmx-swapping\",allowEval:true,allowScriptTags:true,inlineScriptNonce:\"\",attributesToSettle:[\"class\",\"style\",\"width\",\"height\"],withCredentials:false,timeout:0,wsReconnectDelay:\"full-jitter\",wsBinary\nType:\"blob\",disableSelector:\"[hx-disable], \n[data-hx-disable]\",useTemplateFragments:false,scrollBehavior:\"smooth\",defaultFocusScroll:false,getCacheBusterParam:false,globalViewTransitions:false,methodsThatUseUrlParams:[\"get\"],selfRequestsOnly:false,ignoreTitle:false,scrollIntoViewOnBoost:true,triggerSpecsCache:null},parseInterval:d,_:t,createEventSource:function\n(e){return new EventSource(e,{withCredentials:true})},createWebSocket:function(e){var t=new WebSocket(e,[]);t.binaryType=Q.config.wsBinaryType;return t},version:\"1.9.10\"};var \nr={addTriggerHandler:Lt,bodyContains:se,canAccessLocalStorage:U,findThisElement:xe,filterValues:yr,hasAttribute:o,getAttributeValue:te,getClosestAttributeValue:ne,getClosestMatch:c,getExpressionVars:Hr,getHeaders:xr,getInputValues:dr,getInternalData:ae,getSwapSpecification:wr,getTriggerSpecs:it,getTarget:ye,makeFragme\nnt:l,mergeObjects:le,makeSettleInfo:T,oobSwap:Ee,querySelectorExt:ue,selectAndSwap:je,settleImmediately:nr,shouldCancel:ut,triggerEvent:ce,triggerErrorEvent:fe,withExtensions:R};var w=[\"get\",\"post\",\"put\",\"delete\",\"patch\"];var i=w.map(function(e){return\"[hx-\"+e+\"], [data-hx-\"+e+\"]\"}).join(\", \");var \nS=e(\"head\"),q=e(\"title\"),H=e(\"svg\",true);function e(e,t=false){return new RegExp(`<${e}(\\\\s[^>]*>|>)([\\\\s\\\\S]*?)<\\\\/${e}>`,t?\"gim\":\"im\")}function d(e){if(e==undefined){return undefined}let t=NaN;if(e.slice(-2)==\"ms\"){t=parseFloat(e.slice(0,-2))}else if(e.slice(-1)==\"s\"){t=parseFloat(e.slice(0,-1))*1e3}else \nif(e.slice(-1)==\"m\"){t=parseFloat(e.slice(0,-1))*1e3*60}else{t=parseFloat(e)}return isNaN(t)?undefined:t}function ee(e,t){return e.getAttribute&&e.getAttribute(t)}function o(e,t){return e.hasAttribute&&(e.hasAttribute(t)||e.hasAttribute(\"data-\"+t))}function te(e,t){return ee(e,t)||ee(e,\"data-\"+t)}function u(e){return \ne.parentElement}function re(){return document}function c(e,t){while(e&&!t(e)){e=u(e)}return e?e:null}function L(e,t,r){var n=te(t,r);var i=te(t,\"hx-disinherit\");if(e!==t&&i&&(i===\"*\"||i.split(\" \").indexOf(r)>=0)){return\"unset\"}else{return n}}function ne(t,r){var n=null;c(t,function(e){return \nn=L(t,e,r)});if(n!==\"unset\"){return n}}function h(e,t){var r=e.matches||e.matchesSelector||e.msMatchesSelector||e.mozMatchesSelector||e.webkitMatchesSelector||e.oMatchesSelector;return r&&r.call(e,t)}function A(e){var t=/<([a-z][^\\/\\0>\\x20\\t\\r\\n\\f]*)/i;var r=t.exec(e);if(r){return \nr[1].toLowerCase()}else{return\"\"}}function a(e,t){var r=new DOMParser;var n=r.parseFromString(e,\"text/html\");var i=n.body;while(t>0){t--;i=i.firstChild}if(i==null){i=re().createDocumentFragment()}return i}function N(e){return/<body/.test(e)}function l(e){var t=!N(e);var r=A(e);var \nn=e;if(r===\"head\"){n=n.replace(S,\"\")}if(Q.config.useTemplateFragments&&t){var i=a(\"<body><template>\"+n+\"</template></body>\",0);return i.querySelector(\"template\").content}switch(r){case\"thead\":case\"tbody\":case\"tfoot\":case\"colgroup\":case\"caption\":return a(\"<table>\"+n+\"</table>\",1);case\"col\":return \na(\"<table><colgroup>\"+n+\"</colgroup></table>\",2);case\"tr\":return a(\"<table><tbody>\"+n+\"</tbody></table>\",2);case\"td\":case\"th\":return a(\"<table><tbody><tr>\"+n+\"</tr></tbody></table>\",3);case\"script\":case\"style\":return a(\"<div>\"+n+\"</div>\",1);default:return a(n,0)}}function ie(e){if(e){e()}}function I(e,t){return \nObject.prototype.toString.call(e)===\"[object \"+t+\"]\"}function k(e){return I(e,\"Function\")}function P(e){return I(e,\"Object\")}function ae(e){var t=\"htmx-internal-data\";var r=e[t];if(!r){r=e[t]={}}return r}function M(e){var t=[];if(e){for(var r=0;r<e.length;r++){t.push(e[r])}}return t}function oe(e,t){if(e){for(var \nr=0;r<e.length;r++){t(e[r])}}}function X(e){var t=e.getBoundingClientRect();var r=t.top;var n=t.bottom;return r<window.innerHeight&&n>=0}function se(e){if(e.getRootNode&&e.getRootNode()instanceof window.ShadowRoot){return re().body.contains(e.getRootNode().host)}else{return re().body.contains(e)}}function D(e){return \ne.trim().split(/\\s+/)}function le(e,t){for(var r in t){if(t.hasOwnProperty(r)){e[r]=t[r]}}return e}function E(e){try{return JSON.parse(e)}catch(e){b(e);return null}}function U(){var e=\"htmx:localStorageTest\";try{localStorage.setItem(e,e);localStorage.removeItem(e);return true}catch(e){return false}}function \nB(t){try{var e=new URL(t);if(e){t=e.pathname+e.search}if(!/^\\/$/.test(t)){t=t.replace(/\\/+$/,\"\")}return t}catch(e){return t}}function t(e){return Tr(re().body,function(){return eval(e)})}function F(t){var e=Q.on(\"htmx:load\",function(e){t(e.detail.elt)});return e}function \nV(){Q.logger=function(e,t,r){if(console){console.log(t,e,r)}}}function j(){Q.logger=null}function C(e,t){if(t){return e.querySelector(t)}else{return C(re(),e)}}function f(e,t){if(t){return e.querySelectorAll(t)}else{return f(re(),e)}}function \n_(e,t){e=g(e);if(t){setTimeout(function(){_(e);e=null},t)}else{e.parentElement.removeChild(e)}}function z(e,t,r){e=g(e);if(r){setTimeout(function(){z(e,t);e=null},r)}else{e.classList&&e.classList.add(t)}}function \nn(e,t,r){e=g(e);if(r){setTimeout(function(){n(e,t);e=null},r)}else{if(e.classList){e.classList.remove(t);if(e.classList.length===0){e.removeAttribute(\"class\")}}}}function $(e,t){e=g(e);e.classList.toggle(t)}function W(e,t){e=g(e);oe(e.parentElement.children,function(e){n(e,t)});z(e,t)}function \nv(e,t){e=g(e);if(e.closest){return e.closest(t)}else{do{if(e==null||h(e,t)){return e}}while(e=e&&u(e));return null}}function s(e,t){return e.substring(0,t.length)===t}function G(e,t){return e.substring(e.length-t.length)===t}function J(e){var t=e.trim();if(s(t,\"<\")&&G(t,\"/>\")){return \nt.substring(1,t.length-2)}else{return t}}function Z(e,t){if(t.indexOf(\"closest \")===0){return[v(e,J(t.substr(8)))]}else if(t.indexOf(\"find \")===0){return[C(e,J(t.substr(5)))]}else if(t===\"next\"){return[e.nextElementSibling]}else if(t.indexOf(\"next \")===0){return[K(e,J(t.substr(5)))]}else \nif(t===\"previous\"){return[e.previousElementSibling]}else if(t.indexOf(\"previous \")===0){return[Y(e,J(t.substr(9)))]}else if(t===\"document\"){return[document]}else if(t===\"window\"){return[window]}else if(t===\"body\"){return[document.body]}else{return re().querySelectorAll(J(t))}}var K=function(e,t){var \nr=re().querySelectorAll(t);for(var n=0;n<r.length;n++){var i=r[n];if(i.compareDocumentPosition(e)===Node.DOCUMENT_POSITION_PRECEDING){return i}}};var Y=function(e,t){var r=re().querySelectorAll(t);for(var n=r.length-1;n>=0;n--){var i=r[n];if(i.compareDocumentPosition(e)===Node.DOCUMENT_POSITION_FOLLOWING){return \ni}}};function ue(e,t){if(t){return Z(e,t)[0]}else{return Z(re().body,e)[0]}}function g(e){if(I(e,\"String\")){return C(e)}else{return e}}function ve(e,t,r){if(k(t)){return{target:re().body,event:e,listener:t}}else{return{target:g(e),event:t,listener:r}}}function de(t,r,n){jr(function(){var \ne=ve(t,r,n);e.target.addEventListener(e.event,e.listener)});var e=k(r);return e?r:n}function ge(t,r,n){jr(function(){var e=ve(t,r,n);e.target.removeEventListener(e.event,e.listener)});return k(r)?r:n}var me=re().createElement(\"output\");function pe(e,t){var r=ne(e,t);if(r){if(r===\"this\"){return[xe(e,t)]}else{var \nn=Z(e,r);if(n.length===0){b('The selector \"'+r+'\" on '+t+\" returned no matches!\");return[me]}else{return n}}}}function xe(e,t){return c(e,function(e){return te(e,t)!=null})}function ye(e){var t=ne(e,\"hx-target\");if(t){if(t===\"this\"){return xe(e,\"hx-target\")}else{return ue(e,t)}}else{var r=ae(e);if(r.boosted){return \nre().body}else{return e}}}function be(e){var t=Q.config.attributesToSettle;for(var r=0;r<t.length;r++){if(e===t[r]){return true}}return false}function \nwe(t,r){oe(t.attributes,function(e){if(!r.hasAttribute(e.name)&&be(e.name)){t.removeAttribute(e.name)}});oe(r.attributes,function(e){if(be(e.name)){t.setAttribute(e.name,e.value)}})}function Se(e,t){var r=Fr(t);for(var n=0;n<r.length;n++){var i=r[n];try{if(i.isInlineSwap(e)){return true}}catch(e){b(e)}}return \ne===\"outerHTML\"}function Ee(e,i,a){var t=\"#\"+ee(i,\"id\");var o=\"outerHTML\";if(e===\"true\"){}else if(e.indexOf(\":\")>0){o=e.substr(0,e.indexOf(\":\"));t=e.substr(e.indexOf(\":\")+1,e.length)}else{o=e}var r=re().querySelectorAll(t);if(r){oe(r,function(e){var t;var \nr=i.cloneNode(true);t=re().createDocumentFragment();t.appendChild(r);if(!Se(o,e)){t=r}var \nn={shouldSwap:true,target:e,fragment:t};if(!ce(e,\"htmx:oobBeforeSwap\",n))return;e=n.target;if(n[\"shouldSwap\"]){Fe(o,e,e,t,a)}oe(a.elts,function(e){ce(e,\"htmx:oobAfterSwap\",n)})});i.parentNode.removeChild(i)}else{i.parentNode.removeChild(i);fe(re().body,\"htmx:oobErrorNoTarget\",{content:i})}return e}function \nCe(e,t,r){var n=ne(e,\"hx-select-oob\");if(n){var i=n.split(\",\");for(var a=0;a<i.length;a++){var o=i[a].split(\":\",2);var s=o[0].trim();if(s.indexOf(\"#\")===0){s=s.substring(1)}var l=o[1]||\"true\";var u=t.querySelector(\"#\"+s);if(u){Ee(l,u,r)}}}oe(f(t,\"[hx-swap-oob], [data-hx-swap-oob]\"),function(e){var \nt=te(e,\"hx-swap-oob\");if(t!=null){Ee(t,e,r)}})}function Re(e){oe(f(e,\"[hx-preserve], [data-hx-preserve]\"),function(e){var t=te(e,\"id\");var r=re().getElementById(t);if(r!=null){e.parentNode.replaceChild(r,e)}})}function Te(o,e,s){oe(e.querySelectorAll(\"[id]\"),function(e){var t=ee(e,\"id\");if(t&&t.length>0){var \nr=t.replace(\"'\",\"\\\\'\");var n=e.tagName.replace(\":\",\"\\\\:\");var i=o.querySelector(n+\"[id='\"+r+\"']\");if(i&&i!==o){var a=e.cloneNode();we(e,i);s.tasks.push(function(){we(e,a)})}}})}function Oe(e){return function(){n(e,Q.config.addedClass);zt(e);Nt(e);qe(e);ce(e,\"htmx:load\")}}function qe(e){var t=\"[autofocus]\";var \nr=h(e,t)?e:e.querySelector(t);if(r!=null){r.focus()}}function m(e,t,r,n){Te(e,r,n);while(r.childNodes.length>0){var i=r.firstChild;z(i,Q.config.addedClass);e.insertBefore(i,t);if(i.nodeType!==Node.TEXT_NODE&&i.nodeType!==Node.COMMENT_NODE){n.tasks.push(Oe(i))}}}function He(e,t){var \nr=0;while(r<e.length){t=(t<<5)-t+e.charCodeAt(r++)|0}return t}function Le(e){var t=0;if(e.attributes){for(var r=0;r<e.attributes.length;r++){var n=e.attributes[r];if(n.value){t=He(n.name,t);t=He(n.value,t)}}}return t}function Ae(e){var t=ae(e);if(t.onHandlers){for(var r=0;r<t.onHandlers.length;r++){const \nn=t.onHandlers[r];e.removeEventListener(n.event,n.listener)}delete t.onHandlers}}function Ne(e){var \nt=ae(e);if(t.timeout){clearTimeout(t.timeout)}if(t.webSocket){t.webSocket.close()}if(t.sseEventSource){t.sseEventSource.close()}if(t.listenerInfos){oe(t.listenerInfos,function(e){if(e.on){e.on.removeEventListener(e.trigger,e.listener)}})}Ae(e);oe(Object.keys(t),function(e){delete t[e]})}function \np(e){ce(e,\"htmx:beforeCleanupElement\");Ne(e);if(e.children){oe(e.children,function(e){p(e)})}}function Ie(t,e,r){if(t.tagName===\"BODY\"){return Ue(t,e,r)}else{var n;var i=t.previousSibling;m(u(t),t,e,r);if(i==null){n=u(t).firstChild}else{n=i.nextSibling}r.elts=r.elts.filter(function(e){return \ne!=t});while(n&&n!==t){if(n.nodeType===Node.ELEMENT_NODE){r.elts.push(n)}n=n.nextElementSibling}p(t);u(t).removeChild(t)}}function ke(e,t,r){return m(e,e.firstChild,t,r)}function Pe(e,t,r){return m(u(e),e,t,r)}function Me(e,t,r){return m(e,null,t,r)}function Xe(e,t,r){return m(u(e),e.nextSibling,t,r)}function \nDe(e,t,r){p(e);return u(e).removeChild(e)}function Ue(e,t,r){var n=e.firstChild;m(e,n,t,r);if(n){while(n.nextSibling){p(n.nextSibling);e.removeChild(n.nextSibling)}p(n);e.removeChild(n)}}function Be(e,t,r){var n=r||ne(e,\"hx-select\");if(n){var \ni=re().createDocumentFragment();oe(t.querySelectorAll(n),function(e){i.appendChild(e)});t=i}return t}function \nFe(e,t,r,n,i){switch(e){case\"none\":return;case\"outerHTML\":Ie(r,n,i);return;case\"afterbegin\":ke(r,n,i);return;case\"beforebegin\":Pe(r,n,i);return;case\"beforeend\":Me(r,n,i);return;case\"afterend\":Xe(r,n,i);return;case\"delete\":De(r,n,i);return;default:var a=Fr(t);for(var o=0;o<a.length;o++){var s=a[o];try{var \nl=s.handleSwap(e,r,n,i);if(l){if(typeof l.length!==\"undefined\"){for(var u=0;u<l.length;u++){var f=l[u];if(f.nodeType!==Node.TEXT_NODE&&f.nodeType!==Node.COMMENT_NODE){i.tasks.push(Oe(f))}}}return}}catch(e){b(e)}}if(e===\"innerHTML\"){Ue(r,n,i)}else{Fe(Q.config.defaultSwapStyle,t,r,n,i)}}}function \nVe(e){if(e.indexOf(\"<title\")>-1){var t=e.replace(H,\"\");var r=t.match(q);if(r){return r[2]}}}function je(e,t,r,n,i,a){i.title=Ve(n);var o=l(n);if(o){Ce(r,o,i);o=Be(r,o,a);Re(o);return Fe(e,r,t,o,i)}}function _e(e,t,r){var n=e.getResponseHeader(t);if(n.indexOf(\"{\")===0){var i=E(n);for(var a in \ni){if(i.hasOwnProperty(a)){var o=i[a];if(!P(o)){o={value:o}}ce(r,a,o)}}}else{var s=n.split(\",\");for(var l=0;l<s.length;l++){ce(r,s[l].trim(),[])}}}var ze=/\\s/;var x=/[\\s,]/;var $e=/[_$a-zA-Z]/;var We=/[_$a-zA-Z0-9]/;var Ge=['\"',\"'\",\"/\"];var Je=/[^\\s]/;var Ze=/[{(]/;var Ke=/[})]/;function Ye(e){var t=[];var \nr=0;while(r<e.length){if($e.exec(e.charAt(r))){var n=r;while(We.exec(e.charAt(r+1))){r++}t.push(e.substr(n,r-n+1))}else if(Ge.indexOf(e.charAt(r))!==-1){var i=e.charAt(r);var n=r;r++;while(r<e.length&&e.charAt(r)!==i){if(e.charAt(r)===\"\\\\\"){r++}r++}t.push(e.substr(n,r-n+1))}else{var a=e.charAt(r);t.push(a)}r++}return \nt}function Qe(e,t,r){return $e.exec(e.charAt(0))&&e!==\"true\"&&e!==\"false\"&&e!==\"this\"&&e!==r&&t!==\".\"}function et(e,t,r){if(t[0]===\"[\"){t.shift();var n=1;var i=\" return (function(\"+r+\"){ return (\";var a=null;while(t.length>0){var o=t[0];if(o===\"]\"){n--;if(n===0){if(a===null){i=i+\"true\"}t.shift();i+=\")})\";try{var \ns=Tr(e,function(){return Function(i)()},function(){return true});s.source=i;return s}catch(e){fe(re().body,\"htmx:syntax:error\",{error:e,source:i});return null}}}else if(o===\"[\"){n++}if(Qe(o,a,r)){i+=\"((\"+r+\".\"+o+\") ? (\"+r+\".\"+o+\") : (window.\"+o+\"))\"}else{i=i+o}a=t.shift()}}}function y(e,t){var \nr=\"\";while(e.length>0&&!t.test(e[0])){r+=e.shift()}return r}function tt(e){var t;if(e.length>0&&Ze.test(e[0])){e.shift();t=y(e,Ke).trim();e.shift()}else{t=y(e,x)}return t}var rt=\"input, textarea, select\";function nt(e,t,r){var n=[];var i=Ye(t);do{y(i,Je);var a=i.length;var \no=y(i,/[,\\[\\s]/);if(o!==\"\"){if(o===\"every\"){var s={trigger:\"every\"};y(i,Je);s.pollInterval=d(y(i,/[,\\[\\s]/));y(i,Je);var l=et(e,i,\"event\");if(l){s.eventFilter=l}n.push(s)}else if(o.indexOf(\"sse:\")===0){n.push({trigger:\"sse\",sseEvent:o.substr(4)})}else{var u={trigger:o};var \nl=et(e,i,\"event\");if(l){u.eventFilter=l}while(i.length>0&&i[0]!==\",\"){y(i,Je);var f=i.shift();if(f===\"changed\"){u.changed=true}else if(f===\"once\"){u.once=true}else if(f===\"consume\"){u.consume=true}else if(f===\"delay\"&&i[0]===\":\"){i.shift();u.delay=d(y(i,x))}else \nif(f===\"from\"&&i[0]===\":\"){i.shift();if(Ze.test(i[0])){var c=tt(i)}else{var c=y(i,x);if(c===\"closest\"||c===\"find\"||c===\"next\"||c===\"previous\"){i.shift();var h=tt(i);if(h.length>0){c+=\" \"+h}}}u.from=c}else if(f===\"target\"&&i[0]===\":\"){i.shift();u.target=tt(i)}else \nif(f===\"throttle\"&&i[0]===\":\"){i.shift();u.throttle=d(y(i,x))}else if(f===\"queue\"&&i[0]===\":\"){i.shift();u.queue=y(i,x)}else if(f===\"root\"&&i[0]===\":\"){i.shift();u[f]=tt(i)}else \nif(f===\"threshold\"&&i[0]===\":\"){i.shift();u[f]=y(i,x)}else{fe(e,\"htmx:syntax:error\",{token:i.shift()})}}n.push(u)}}if(i.length===a){fe(e,\"htmx:syntax:error\",{token:i.shift()})}y(i,Je)}while(i[0]===\",\"&&i.shift());if(r){r[t]=n}return n}function it(e){var t=te(e,\"hx-trigger\");var r=[];if(t){var \nn=Q.config.triggerSpecsCache;r=n&&n[t]||nt(e,t,n)}if(r.length>0){return r}else if(h(e,\"form\")){return[{trigger:\"submit\"}]}else if(h(e,'input[type=\"button\"], input[type=\"submit\"]')){return[{trigger:\"click\"}]}else if(h(e,rt)){return[{trigger:\"change\"}]}else{return[{trigger:\"click\"}]}}function \nat(e){ae(e).cancelled=true}function ot(e,t,r){var n=ae(e);n.timeout=setTimeout(function(){if(se(e)&&n.cancelled!==true){if(!ct(r,e,Wt(\"hx:poll:trigger\",{triggerSpec:r,target:e}))){t(e)}ot(e,t,r)}},r.pollInterval)}function st(e){return location.hostname===e.hostname&&ee(e,\"href\")&&ee(e,\"href\").indexOf(\"#\")!==0}function \nlt(t,r,e){if(t.tagName===\"A\"&&st(t)&&(t.target===\"\"||t.target===\"_self\")||t.tagName===\"FORM\"){r.boosted=true;var n,i;if(t.tagName===\"A\"){n=\"get\";i=ee(t,\"href\")}else{var \na=ee(t,\"method\");n=a?a.toLowerCase():\"get\";if(n===\"get\"){}i=ee(t,\"action\")}e.forEach(function(e){ht(t,function(e,t){if(v(e,Q.config.disableSelector)){p(e);return}he(n,i,e,t)},r,e,true)})}}function ut(e,t){if(e.type===\"submit\"||e.type===\"click\"){if(t.tagName===\"FORM\"){return true}if(h(t,'input[type=\"submit\"], \nbutton')&&v(t,\"form\")!==null){return true}if(t.tagName===\"A\"&&t.href&&(t.getAttribute(\"href\")===\"#\"||t.getAttribute(\"href\").indexOf(\"#\")!==0)){return true}}return false}function ft(e,t){return ae(e).boosted&&e.tagName===\"A\"&&t.type===\"click\"&&(t.ctrlKey||t.metaKey)}function ct(e,t,r){var \nn=e.eventFilter;if(n){try{return n.call(t,r)!==true}catch(e){fe(re().body,\"htmx:eventFilter:error\",{error:e,source:n.source});return true}}return false}function ht(a,o,e,s,l){var u=ae(a);var t;if(s.from){t=Z(a,s.from)}else{t=[a]}if(s.changed){t.forEach(function(e){var t=ae(e);t.lastValue=e.value})}oe(t,function(n){var \ni=function(e){if(!se(a)){n.removeEventListener(s.trigger,i);return}if(ft(a,e)){return}if(l||ut(e,a)){e.preventDefault()}if(ct(s,a,e)){return}var \nt=ae(e);t.triggerSpec=s;if(t.handledFor==null){t.handledFor=[]}if(t.handledFor.indexOf(a)<0){t.handledFor.push(a);if(s.consume){e.stopPropagation()}if(s.target&&e.target){if(!h(e.target,s.target)){return}}if(s.once){if(u.triggeredOnce){return}else{u.triggeredOnce=true}}if(s.changed){var \nr=ae(n);if(r.lastValue===n.value){return}r.lastValue=n.value}if(u.delayed){clearTimeout(u.delayed)}if(u.throttle){return}if(s.throttle>0){if(!u.throttle){o(a,e);u.throttle=setTimeout(function(){u.throttle=null},s.throttle)}}else \nif(s.delay>0){u.delayed=setTimeout(function(){o(a,e)},s.delay)}else{ce(a,\"htmx:trigger\");o(a,e)}}};if(e.listenerInfos==null){e.listenerInfos=[]}e.listenerInfos.push({trigger:s.trigger,listener:i,on:n});n.addEventListener(s.trigger,i)})}var vt=false;var dt=null;function \ngt(){if(!dt){dt=function(){vt=true};window.addEventListener(\"scroll\",dt);setInterval(function(){if(vt){vt=false;oe(re().querySelectorAll(\"[hx-trigger='revealed'],[data-hx-trigger='revealed']\"),function(e){mt(e)})}},200)}}function mt(t){if(!o(t,\"data-hx-revealed\")&&X(t)){t.setAttribute(\"data-hx-revealed\",\"true\");var \ne=ae(t);if(e.initHash){ce(t,\"revealed\")}else{t.addEventListener(\"htmx:afterProcessNode\",function(e){ce(t,\"revealed\")},{once:true})}}}function pt(e,t,r){var n=D(r);for(var i=0;i<n.length;i++){var a=n[i].split(/:(.+)/);if(a[0]===\"connect\"){xt(e,a[1],0)}if(a[0]===\"send\"){bt(e)}}}function \nxt(s,r,n){if(!se(s)){return}if(r.indexOf(\"/\")==0){var e=location.hostname+(location.port?\":\"+location.port:\"\");if(location.protocol==\"https:\"){r=\"wss://\"+e+r}else if(location.protocol==\"http:\"){r=\"ws://\"+e+r}}var \nt=Q.createWebSocket(r);t.onerror=function(e){fe(s,\"htmx:wsError\",{error:e,socket:t});yt(s)};t.onclose=function(e){if([1006,1012,1013].indexOf(e.code)>=0){var t=wt(n);setTimeout(function(){xt(s,r,n+1)},t)}};t.onopen=function(e){n=0};ae(s).webSocket=t;t.addEventListener(\"message\",function(e){if(yt(s)){return}var \nt=e.data;R(s,function(e){t=e.transformResponse(t,null,s)});var r=T(s);var n=l(t);var i=M(n.children);for(var a=0;a<i.length;a++){var o=i[a];Ee(te(o,\"hx-swap-oob\")||\"true\",o,r)}nr(r.tasks)})}function yt(e){if(!se(e)){ae(e).webSocket.close();return true}}function bt(u){var f=c(u,function(e){return \nae(e).webSocket!=null});if(f){u.addEventListener(it(u)[0].trigger,function(e){var t=ae(f).webSocket;var r=xr(u,f);var n=dr(u,\"post\");var i=n.errors;var a=n.values;var o=Hr(u);var s=le(a,o);var \nl=yr(s,u);l[\"HEADERS\"]=r;if(i&&i.length>0){ce(u,\"htmx:validation:halted\",i);return}t.send(JSON.stringify(l));if(ut(e,u)){e.preventDefault()}})}else{fe(u,\"htmx:noWebSocketSourceError\")}}function wt(e){var t=Q.config.wsReconnectDelay;if(typeof t===\"function\"){return t(e)}if(t===\"full-jitter\"){var r=Math.min(e,6);var \nn=1e3*Math.pow(2,r);return n*Math.random()}b('htmx.config.wsReconnectDelay must either be a function or the string \"full-jitter\"')}function St(e,t,r){var n=D(r);for(var i=0;i<n.length;i++){var a=n[i].split(/:(.+)/);if(a[0]===\"connect\"){Et(e,a[1])}if(a[0]===\"swap\"){Ct(e,a[1])}}}function Et(t,e){var \nr=Q.createEventSource(e);r.onerror=function(e){fe(t,\"htmx:sseError\",{error:e,source:r});Tt(t)};ae(t).sseEventSource=r}function Ct(a,o){var s=c(a,Ot);if(s){var l=ae(s).sseEventSource;var u=function(e){if(Tt(s)){return}if(!se(a)){l.removeEventListener(o,u);return}var \nt=e.data;R(a,function(e){t=e.transformResponse(t,null,a)});var r=wr(a);var n=ye(a);var i=T(a);je(r.swapStyle,n,a,t,i);nr(i.tasks);ce(a,\"htmx:sseMessage\",e)};ae(a).sseListener=u;l.addEventListener(o,u)}else{fe(a,\"htmx:noSSESourceError\")}}function Rt(e,t,r){var n=c(e,Ot);if(n){var i=ae(n).sseEventSource;var \na=function(){if(!Tt(n)){if(se(e)){t(e)}else{i.removeEventListener(r,a)}}};ae(e).sseListener=a;i.addEventListener(r,a)}else{fe(e,\"htmx:noSSESourceError\")}}function Tt(e){if(!se(e)){ae(e).sseEventSource.close();return true}}function Ot(e){return ae(e).sseEventSource!=null}function qt(e,t,r,n){var \ni=function(){if(!r.loaded){r.loaded=true;t(e)}};if(n>0){setTimeout(i,n)}else{i()}}function Ht(t,i,e){var a=false;oe(w,function(r){if(o(t,\"hx-\"+r)){var n=te(t,\"hx-\"+r);a=true;i.path=n;i.verb=r;e.forEach(function(e){Lt(t,e,i,function(e,t){if(v(e,Q.config.disableSelector)){p(e);return}he(r,n,e,t)})})}});return a}function \nLt(n,e,t,r){if(e.sseEvent){Rt(n,r,e.sseEvent)}else if(e.trigger===\"revealed\"){gt();ht(n,r,t,e);mt(n)}else if(e.trigger===\"intersect\"){var i={};if(e.root){i.root=ue(n,e.root)}if(e.threshold){i.threshold=parseFloat(e.threshold)}var a=new IntersectionObserver(function(e){for(var t=0;t<e.length;t++){var \nr=e[t];if(r.isIntersecting){ce(n,\"intersect\");break}}},i);a.observe(n);ht(n,r,t,e)}else if(e.trigger===\"load\"){if(!ct(e,n,Wt(\"load\",{elt:n}))){qt(n,r,t,e.delay)}}else if(e.pollInterval>0){t.polling=true;ot(n,r,e)}else{ht(n,r,t,e)}}function \nAt(e){if(Q.config.allowScriptTags&&(e.type===\"text/javascript\"||e.type===\"module\"||e.type===\"\")){var t=re().createElement(\"script\");oe(e.attributes,function(e){t.setAttribute(e.name,e.value)});t.textContent=e.textContent;t.async=false;if(Q.config.inlineScriptNonce){t.nonce=Q.config.inlineScriptNonce}var \nr=e.parentElement;try{r.insertBefore(t,e)}catch(e){b(e)}finally{if(e.parentElement){e.parentElement.removeChild(e)}}}}function Nt(e){if(h(e,\"script\")){At(e)}oe(f(e,\"script\"),function(e){At(e)})}function It(e){var t=e.attributes;for(var r=0;r<t.length;r++){var \nn=t[r].name;if(s(n,\"hx-on:\")||s(n,\"data-hx-on:\")||s(n,\"hx-on-\")||s(n,\"data-hx-on-\")){return true}}return false}function kt(e){var t=null;var r=[];if(It(e)){r.push(e)}if(document.evaluate){var n=document.evaluate('.//*[@*[ starts-with(name(), \"hx-on:\") or starts-with(name(), \"data-hx-on:\") or'+' starts-with(name(), \n\"hx-on-\") or starts-with(name(), \"data-hx-on-\") ]]',e);while(t=n.iterateNext())r.push(t)}else{var i=e.getElementsByTagName(\"*\");for(var a=0;a<i.length;a++){if(It(i[a])){r.push(i[a])}}}return r}function Pt(e){if(e.querySelectorAll){var t=\", [hx-boost] a, [data-hx-boost] a, a[hx-boost], a[data-hx-boost]\";var \nr=e.querySelectorAll(i+t+\", form, [type='submit'], [hx-sse], [data-hx-sse], [hx-ws],\"+\" [data-hx-ws], [hx-ext], [data-hx-ext], [hx-trigger], [data-hx-trigger], [hx-on], [data-hx-on]\");return r}else{return[]}}function Mt(e){var t=v(e.target,\"button, input[type='submit']\");var \n\nr=Dt(e);if(r){r.lastButtonClicked=t}}function Xt(e){var t=Dt(e);if(t){t.lastButtonClicked=null}}function Dt(e){var t=v(e.target,\"button, input[type='submit']\");if(!t){return}var r=g(\"#\"+ee(t,\"form\"))||v(t,\"form\");if(!r){return}return ae(r)}function \nUt(e){e.addEventListener(\"click\",Mt);e.addEventListener(\"focusin\",Mt);e.addEventListener(\"focusout\",Xt)}function Bt(e){var t=Ye(e);var r=0;for(var n=0;n<t.length;n++){const i=t[n];if(i===\"{\"){r++}else if(i===\"}\"){r--}}return r}function Ft(t,e,r){var n=ae(t);if(!Array.isArray(n.onHandlers)){n.onHandlers=[]}var i;var \na=function(e){return Tr(t,function(){if(!i){i=new Function(\"event\",r)}i.call(t,e)})};t.addEventListener(e,a);n.onHandlers.push({event:e,listener:a})}function Vt(e){var t=te(e,\"hx-on\");if(t){var r={};var n=t.split(\"\\n\");var i=null;var a=0;while(n.length>0){var o=n.shift();var \ns=o.match(/^\\s*([a-zA-Z:\\-\\.]+:)(.*)/);if(a===0&&s){o.split(\":\");i=s[1].slice(0,-1);r[i]=s[2]}else{r[i]+=o}a+=Bt(o)}for(var l in r){Ft(e,l,r[l])}}}function jt(e){Ae(e);for(var t=0;t<e.attributes.length;t++){var r=e.attributes[t].name;var n=e.attributes[t].value;if(s(r,\"hx-on\")||s(r,\"data-hx-on\")){var \ni=r.indexOf(\"-on\")+3;var a=r.slice(i,i+1);if(a===\"-\"||a===\":\"){var o=r.slice(i+1);if(s(o,\":\")){o=\"htmx\"+o}else if(s(o,\"-\")){o=\"htmx:\"+o.slice(1)}else if(s(o,\"htmx-\")){o=\"htmx:\"+o.slice(5)}Ft(e,o,n)}}}}function _t(t){if(v(t,Q.config.disableSelector)){p(t);return}var \nr=ae(t);if(r.initHash!==Le(t)){Ne(t);r.initHash=Le(t);Vt(t);ce(t,\"htmx:beforeProcessNode\");if(t.value){r.lastValue=t.value}var e=it(t);var n=Ht(t,r,e);if(!n){if(ne(t,\"hx-boost\")===\"true\"){lt(t,r,e)}else \nif(o(t,\"hx-trigger\")){e.forEach(function(e){Lt(t,e,r,function(){})})}}if(t.tagName===\"FORM\"||ee(t,\"type\")===\"submit\"&&o(t,\"form\")){Ut(t)}var i=te(t,\"hx-sse\");if(i){St(t,r,i)}var a=te(t,\"hx-ws\");if(a){pt(t,r,a)}ce(t,\"htmx:afterProcessNode\")}}function \nzt(e){e=g(e);if(v(e,Q.config.disableSelector)){p(e);return}_t(e);oe(Pt(e),function(e){_t(e)});oe(kt(e),jt)}function $t(e){return e.replace(/([a-z0-9])([A-Z])/g,\"$1-$2\").toLowerCase()}function Wt(e,t){var r;if(window.CustomEvent&&typeof window.CustomEvent===\"function\"){r=new \nCustomEvent(e,{bubbles:true,cancelable:true,detail:t})}else{r=re().createEvent(\"CustomEvent\");r.initCustomEvent(e,true,true,t)}return r}function fe(e,t,r){ce(e,t,le({error:t},r))}function Gt(e){return e===\"htmx:afterProcessNode\"}function R(e,t){oe(Fr(e),function(e){try{t(e)}catch(e){b(e)}})}function \nb(e){if(console.error){console.error(e)}else if(console.log){console.log(\"ERROR: \",e)}}function ce(e,t,r){e=g(e);if(r==null){r={}}r[\"elt\"]=e;var n=Wt(t,r);if(Q.logger&&!Gt(t)){Q.logger(e,t,r)}if(r.error){b(r.error);ce(e,\"htmx:error\",{errorInfo:r})}var i=e.dispatchEvent(n);var a=$t(t);if(i&&a!==t){var \no=Wt(a,n.detail);i=i&&e.dispatchEvent(o)}R(e,function(e){i=i&&(e.onEvent(t,n)!==false&&!n.defaultPrevented)});return i}var Jt=location.pathname+location.search;function Zt(){var e=re().querySelector(\"[hx-history-elt],[data-hx-history-elt]\");return e||re().body}function \nKt(e,t,r,n){if(!U()){return}if(Q.config.historyCacheSize<=0){localStorage.removeItem(\"htmx-history-cache\");return}e=B(e);var i=E(localStorage.getItem(\"htmx-history-cache\"))||[];for(var a=0;a<i.length;a++){if(i[a].url===e){i.splice(a,1);break}}var \no={url:e,content:t,title:r,scroll:n};ce(re().body,\"htmx:historyItemCreated\",{item:o,cache:i});i.push(o);while(i.length>Q.config.historyCacheSize){i.shift()}while(i.length>0){try{localStorage.setItem(\"htmx-history-cache\",JSON.stringify(i));break}catch(e){fe(re().body,\"htmx:historyCacheError\",{cause:e,cache:i});i.shift(\n)}}}function Yt(e){if(!U()){return null}e=B(e);var t=E(localStorage.getItem(\"htmx-history-cache\"))||[];for(var r=0;r<t.length;r++){if(t[r].url===e){return t[r]}}return null}function Qt(e){var t=Q.config.requestClass;var r=e.cloneNode(true);oe(f(r,\".\"+t),function(e){n(e,t)});return r.innerHTML}function er(){var \ne=Zt();var t=Jt||location.pathname+location.search;var r;try{r=re().querySelector('[hx-history=\"false\" i],[data-hx-history=\"false\" \ni]')}catch(e){r=re().querySelector('[hx-history=\"false\"],[data-hx-history=\"false\"]')}if(!r){ce(re().body,\"htmx:beforeHistorySave\",{path:t,historyElt:e});Kt(t,Qt(e),re().title,window.scrollY)}if(Q.config.historyEnabled)history.replaceState({htmx:true},re().title,window.location.href)}function \ntr(e){if(Q.config.getCacheBusterParam){e=e.replace(/org\\.htmx\\.cache-buster=[^&]*&?/,\"\");if(G(e,\"&\")||G(e,\"?\")){e=e.slice(0,-1)}}if(Q.config.historyEnabled){history.pushState({htmx:true},\"\",e)}Jt=e}function rr(e){if(Q.config.historyEnabled)history.replaceState({htmx:true},\"\",e);Jt=e}function \nnr(e){oe(e,function(e){e.call()})}function ir(a){var e=new XMLHttpRequest;var \no={path:a,xhr:e};ce(re().body,\"htmx:historyCacheMiss\",o);e.open(\"GET\",a,true);e.setRequestHeader(\"HX-Request\",\"true\");e.setRequestHeader(\"HX-History-Restore-Request\",\"true\");e.setRequestHeader(\"HX-Current-URL\",re().location.href);e.onload=function(){if(this.status>=200&&this.status<400){ce(re().body,\"htmx:historyCache\nMissLoad\",o);var e=l(this.response);e=e.querySelector(\"[hx-history-elt],[data-hx-history-elt]\")||e;var t=Zt();var r=T(t);var n=Ve(this.response);if(n){var \ni=C(\"title\");if(i){i.innerHTML=n}else{window.document.title=n}}Ue(t,e,r);nr(r.tasks);Jt=a;ce(re().body,\"htmx:historyRestore\",{path:a,cacheMiss:true,serverResponse:this.response})}else{fe(re().body,\"htmx:historyCacheMissLoadError\",o)}};e.send()}function ar(e){er();e=e||location.pathname+location.search;var \nt=Yt(e);if(t){var r=l(t.content);var n=Zt();var i=T(n);Ue(n,r,i);nr(i.tasks);document.title=t.title;setTimeout(function(){window.scrollTo(0,t.scroll)},0);Jt=e;ce(re().body,\"htmx:historyRestore\",{path:e,item:t})}else{if(Q.config.refreshOnHistoryMiss){window.location.reload(true)}else{ir(e)}}}function or(e){var \nt=pe(e,\"hx-indicator\");if(t==null){t=[e]}oe(t,function(e){var t=ae(e);t.requestCount=(t.requestCount||0)+1;e.classList[\"add\"].call(e.classList,Q.config.requestClass)});return t}function sr(e){var t=pe(e,\"hx-disabled-elt\");if(t==null){t=[]}oe(t,function(e){var \nt=ae(e);t.requestCount=(t.requestCount||0)+1;e.setAttribute(\"disabled\",\"\")});return t}function lr(e,t){oe(e,function(e){var t=ae(e);t.requestCount=(t.requestCount||0)-1;if(t.requestCount===0){e.classList[\"remove\"].call(e.classList,Q.config.requestClass)}});oe(t,function(e){var \nt=ae(e);t.requestCount=(t.requestCount||0)-1;if(t.requestCount===0){e.removeAttribute(\"disabled\")}})}function ur(e,t){for(var r=0;r<e.length;r++){var n=e[r];if(n.isSameNode(t)){return true}}return false}function fr(e){if(e.name===\"\"||e.name==null||e.disabled||v(e,\"fieldset[disabled]\")){return \nfalse}if(e.type===\"button\"||e.type===\"submit\"||e.tagName===\"image\"||e.tagName===\"reset\"||e.tagName===\"file\"){return false}if(e.type===\"checkbox\"||e.type===\"radio\"){return e.checked}return true}function cr(e,t,r){if(e!=null&&t!=null){var n=r[e];if(n===undefined){r[e]=t}else \nif(Array.isArray(n)){if(Array.isArray(t)){r[e]=n.concat(t)}else{n.push(t)}}else{if(Array.isArray(t)){r[e]=[n].concat(t)}else{r[e]=[n,t]}}}}function hr(t,r,n,e,i){if(e==null||ur(t,e)){return}else{t.push(e)}if(fr(e)){var a=ee(e,\"name\");var \no=e.value;if(e.multiple&&e.tagName===\"SELECT\"){o=M(e.querySelectorAll(\"option:checked\")).map(function(e){return e.value})}if(e.files){o=M(e.files)}cr(a,o,r);if(i){vr(e,n)}}if(h(e,\"form\")){var s=e.elements;oe(s,function(e){hr(t,r,n,e,i)})}}function \nvr(e,t){if(e.willValidate){ce(e,\"htmx:validation:validate\");if(!e.checkValidity()){t.push({elt:e,message:e.validationMessage,validity:e.validity});ce(e,\"htmx:validation:failed\",{message:e.validationMessage,validity:e.validity})}}}function dr(e,t){var r=[];var n={};var i={};var a=[];var \no=ae(e);if(o.lastButtonClicked&&!se(o.lastButtonClicked)){o.lastButtonClicked=null}var \ns=h(e,\"form\")&&e.noValidate!==true||te(e,\"hx-validate\")===\"true\";if(o.lastButtonClicked){s=s&&o.lastButtonClicked.formNoValidate!==true}if(t!==\"get\"){hr(r,i,a,v(e,\"form\"),s)}hr(r,n,a,e,s);if(o.lastButtonClicked||e.tagName===\"BUTTON\"||e.tagName===\"INPUT\"&&ee(e,\"type\")===\"submit\"){var l=o.lastButtonClicked||e;var \nu=ee(l,\"name\");cr(u,l.value,i)}var f=pe(e,\"hx-include\");oe(f,function(e){hr(r,n,a,e,s);if(!h(e,\"form\")){oe(e.querySelectorAll(rt),function(e){hr(r,n,a,e,s)})}});n=le(n,i);return{errors:a,values:n}}function gr(e,t,r){if(e!==\"\"){e+=\"&\"}if(String(r)===\"[object Object]\"){r=JSON.stringify(r)}var \nn=encodeURIComponent(r);e+=encodeURIComponent(t)+\"=\"+n;return e}function mr(e){var t=\"\";for(var r in e){if(e.hasOwnProperty(r)){var n=e[r];if(Array.isArray(n)){oe(n,function(e){t=gr(t,r,e)})}else{t=gr(t,r,n)}}}return t}function pr(e){var t=new FormData;for(var r in e){if(e.hasOwnProperty(r)){var \nn=e[r];if(Array.isArray(n)){oe(n,function(e){t.append(r,e)})}else{t.append(r,n)}}}return t}function xr(e,t,r){var \nn={\"HX-Request\":\"true\",\"HX-Trigger\":ee(e,\"id\"),\"HX-Trigger-Name\":ee(e,\"name\"),\"HX-Target\":te(t,\"id\"),\"HX-Current-URL\":re().location.href};Rr(e,\"hx-headers\",false,n);if(r!==undefined){n[\"HX-Prompt\"]=r}if(ae(e).boosted){n[\"HX-Boosted\"]=\"true\"}return n}function yr(t,e){var \nr=ne(e,\"hx-params\");if(r){if(r===\"none\"){return{}}else if(r===\"*\"){return t}else if(r.indexOf(\"not \")===0){oe(r.substr(4).split(\",\"),function(e){e=e.trim();delete t[e]});return t}else{var n={};oe(r.split(\",\"),function(e){e=e.trim();n[e]=t[e]});return n}}else{return t}}function br(e){return \nee(e,\"href\")&&ee(e,\"href\").indexOf(\"#\")>=0}function wr(e,t){var r=t?t:ne(e,\"hx-swap\");var n={swapStyle:ae(e).boosted?\"innerHTML\":Q.config.defaultSwapStyle,swapDelay:Q.config.defaultSwapDelay,settleDelay:Q.config.defaultSettleDelay};if(Q.config.scrollIntoViewOnBoost&&ae(e).boosted&&!br(e)){n[\"show\"]=\"top\"}if(r){var \ni=D(r);if(i.length>0){for(var a=0;a<i.length;a++){var o=i[a];if(o.indexOf(\"swap:\")===0){n[\"swapDelay\"]=d(o.substr(5))}else if(o.indexOf(\"settle:\")===0){n[\"settleDelay\"]=d(o.substr(7))}else if(o.indexOf(\"transition:\")===0){n[\"transition\"]=o.substr(11)===\"true\"}else \nif(o.indexOf(\"ignoreTitle:\")===0){n[\"ignoreTitle\"]=o.substr(12)===\"true\"}else if(o.indexOf(\"scroll:\")===0){var s=o.substr(7);var l=s.split(\":\");var u=l.pop();var f=l.length>0?l.join(\":\"):null;n[\"scroll\"]=u;n[\"scrollTarget\"]=f}else if(o.indexOf(\"show:\")===0){var c=o.substr(5);var l=c.split(\":\");var h=l.pop();var \nf=l.length>0?l.join(\":\"):null;n[\"show\"]=h;n[\"showTarget\"]=f}else if(o.indexOf(\"focus-scroll:\")===0){var v=o.substr(\"focus-scroll:\".length);n[\"focusScroll\"]=v==\"true\"}else if(a==0){n[\"swapStyle\"]=o}else{b(\"Unknown modifier in hx-swap: \"+o)}}}}return n}function Sr(e){return \nne(e,\"hx-encoding\")===\"multipart/form-data\"||h(e,\"form\")&&ee(e,\"enctype\")===\"multipart/form-data\"}function Er(t,r,n){var i=null;R(r,function(e){if(i==null){i=e.encodeParameters(t,n,r)}});if(i!=null){return i}else{if(Sr(r)){return pr(n)}else{return mr(n)}}}function T(e){return{tasks:[],elts:[e]}}function Cr(e,t){var \nr=e[0];var n=e[e.length-1];if(t.scroll){var i=null;if(t.scrollTarget){i=ue(r,t.scrollTarget)}if(t.scroll===\"top\"&&(r||i)){i=i||r;i.scrollTop=0}if(t.scroll===\"bottom\"&&(n||i)){i=i||n;i.scrollTop=i.scrollHeight}}if(t.show){var i=null;if(t.showTarget){var \na=t.showTarget;if(t.showTarget===\"window\"){a=\"body\"}i=ue(r,a)}if(t.show===\"top\"&&(r||i)){i=i||r;i.scrollIntoView({block:\"start\",behavior:Q.config.scrollBehavior})}if(t.show===\"bottom\"&&(n||i)){i=i||n;i.scrollIntoView({block:\"end\",behavior:Q.config.scrollBehavior})}}}function \nRr(e,t,r,n){if(n==null){n={}}if(e==null){return n}var i=te(e,t);if(i){var a=i.trim();var o=r;if(a===\"unset\"){return null}if(a.indexOf(\"javascript:\")===0){a=a.substr(11);o=true}else if(a.indexOf(\"js:\")===0){a=a.substr(3);o=true}if(a.indexOf(\"{\")!==0){a=\"{\"+a+\"}\"}var s;if(o){s=Tr(e,function(){return Function(\"return \n(\"+a+\")\")()},{})}else{s=E(a)}for(var l in s){if(s.hasOwnProperty(l)){if(n[l]==null){n[l]=s[l]}}}}return Rr(u(e),t,r,n)}function Tr(e,t,r){if(Q.config.allowEval){return t()}else{fe(e,\"htmx:evalDisallowedError\");return r}}function Or(e,t){return Rr(e,\"hx-vars\",true,t)}function qr(e,t){return \nRr(e,\"hx-vals\",false,t)}function Hr(e){return le(Or(e),qr(e))}function Lr(t,r,n){if(n!==null){try{t.setRequestHeader(r,n)}catch(e){t.setRequestHeader(r,encodeURIComponent(n));t.setRequestHeader(r+\"-URI-AutoEncoded\",\"true\")}}}function Ar(t){if(t.responseURL&&typeof URL!==\"undefined\"){try{var e=new \nURL(t.responseURL);return e.pathname+e.search}catch(e){fe(re().body,\"htmx:badResponseUrl\",{url:t.responseURL})}}}function O(e,t){return t.test(e.getAllResponseHeaders())}function Nr(e,t,r){e=e.toLowerCase();if(r){if(r instanceof Element||I(r,\"String\")){return \nhe(e,t,null,null,{targetOverride:g(r),returnPromise:true})}else{return he(e,t,g(r.source),r.event,{handler:r.handler,headers:r.headers,values:r.values,targetOverride:g(r.target),swapOverride:r.swap,select:r.select,returnPromise:true})}}else{return he(e,t,null,null,{returnPromise:true})}}function Ir(e){var \nt=[];while(e){t.push(e);e=e.parentElement}return t}function kr(e,t,r){var n;var i;if(typeof URL===\"function\"){i=new URL(t,document.location.href);var a=document.location.origin;n=a===i.origin}else{i=t;n=s(t,document.location.origin)}if(Q.config.selfRequestsOnly){if(!n){return false}}return \nce(e,\"htmx:validateUrl\",le({url:i,sameHost:n},r))}function he(t,r,n,i,a,e){var o=null;var s=null;a=a!=null?a:{};if(a.returnPromise&&typeof Promise!==\"undefined\"){var l=new Promise(function(e,t){o=e;s=t})}if(n==null){n=re().body}var M=a.handler||Mr;var X=a.select||null;if(!se(n)){ie(o);return l}var \nu=a.targetOverride||ye(n);if(u==null||u==me){fe(n,\"htmx:targetError\",{target:te(n,\"hx-target\")});ie(s);return l}var f=ae(n);var c=f.lastButtonClicked;if(c){var h=ee(c,\"formaction\");if(h!=null){r=h}var v=ee(c,\"formmethod\");if(v!=null){if(v.toLowerCase()!==\"dialog\"){t=v}}}var d=ne(n,\"hx-confirm\");if(e===undefined){var \nD=function(e){return he(t,r,n,i,a,!!e)};var U={target:u,elt:n,path:r,verb:t,triggeringEvent:i,etc:a,issueRequest:D,question:d};if(ce(n,\"htmx:confirm\",U)===false){ie(o);return l}}var g=n;var m=ne(n,\"hx-sync\");var p=null;var x=false;if(m){var B=m.split(\":\");var \nF=B[0].trim();if(F===\"this\"){g=xe(n,\"hx-sync\")}else{g=ue(n,F)}m=(B[1]||\"drop\").trim();f=ae(g);if(m===\"drop\"&&f.xhr&&f.abortable!==true){ie(o);return l}else if(m===\"abort\"){if(f.xhr){ie(o);return l}else{x=true}}else if(m===\"replace\"){ce(g,\"htmx:abort\")}else if(m.indexOf(\"queue\")===0){var V=m.split(\" \n\");p=(V[1]||\"last\").trim()}}if(f.xhr){if(f.abortable){ce(g,\"htmx:abort\")}else{if(p==null){if(i){var \ny=ae(i);if(y&&y.triggerSpec&&y.triggerSpec.queue){p=y.triggerSpec.queue}}if(p==null){p=\"last\"}}if(f.queuedRequests==null){f.queuedRequests=[]}if(p===\"first\"&&f.queuedRequests.length===0){f.queuedRequests.push(function(){he(t,r,n,i,a)})}else if(p===\"all\"){f.queuedRequests.push(function(){he(t,r,n,i,a)})}else \nif(p===\"last\"){f.queuedRequests=[];f.queuedRequests.push(function(){he(t,r,n,i,a)})}ie(o);return l}}var b=new XMLHttpRequest;f.xhr=b;f.abortable=x;var w=function(){f.xhr=null;f.abortable=false;if(f.queuedRequests!=null&&f.queuedRequests.length>0){var e=f.queuedRequests.shift();e()}};var j=ne(n,\"hx-prompt\");if(j){var \nS=prompt(j);if(S===null||!ce(n,\"htmx:prompt\",{prompt:S,target:u})){ie(o);w();return l}}if(d&&!e){if(!confirm(d)){ie(o);w();return l}}var E=xr(n,u,S);if(t!==\"get\"&&!Sr(n)){E[\"Content-Type\"]=\"application/x-www-form-urlencoded\"}if(a.headers){E=le(E,a.headers)}var _=dr(n,t);var C=_.errors;var \nR=_.values;if(a.values){R=le(R,a.values)}var z=Hr(n);var $=le(R,z);var T=yr($,n);if(Q.config.getCacheBusterParam&&t===\"get\"){T[\"org.htmx.cache-buster\"]=ee(u,\"id\")||\"true\"}if(r==null||r===\"\"){r=re().location.href}var O=Rr(n,\"hx-request\");var W=ae(n).boosted;var q=Q.config.methodsThatUseUrlParams.indexOf(t)>=0;var \nH={boosted:W,useUrlParams:q,parameters:T,unfilteredParameters:$,headers:E,target:u,verb:t,errors:C,withCredentials:a.credentials||O.credentials||Q.config.withCredentials,timeout:a.timeout||O.timeout||Q.config.timeout,path:r,triggeringEvent:i};if(!ce(n,\"htmx:configRequest\",H)){ie(o);w();return \nl}r=H.path;t=H.verb;E=H.headers;T=H.parameters;C=H.errors;q=H.useUrlParams;if(C&&C.length>0){ce(n,\"htmx:validation:halted\",H);ie(o);w();return l}var G=r.split(\"#\");var J=G[0];var L=G[1];var A=r;if(q){A=J;var \nZ=Object.keys(T).length!==0;if(Z){if(A.indexOf(\"?\")<0){A+=\"?\"}else{A+=\"&\"}A+=mr(T);if(L){A+=\"#\"+L}}}if(!kr(n,A,H)){fe(n,\"htmx:invalidPath\",H);ie(s);return l}b.open(t.toUpperCase(),A,true);b.overrideMimeType(\"text/html\");b.withCredentials=H.withCredentials;b.timeout=H.timeout;if(O.noHeaders){}else{for(var N in \nE){if(E.hasOwnProperty(N)){var K=E[N];Lr(b,N,K)}}}var I={xhr:b,target:u,requestConfig:H,etc:a,boosted:W,select:X,pathInfo:{requestPath:r,finalRequestPath:A,anchor:L}};b.onload=function(){try{var e=Ir(n);I.pathInfo.responsePath=Ar(b);M(n,I);lr(k,P);ce(n,\"htmx:afterRequest\",I);ce(n,\"htmx:afterOnLoad\",I);if(!se(n)){var \nt=null;while(e.length>0&&t==null){var r=e.shift();if(se(r)){t=r}}if(t){ce(t,\"htmx:afterRequest\",I);ce(t,\"htmx:afterOnLoad\",I)}}ie(o);w()}catch(e){fe(n,\"htmx:onLoadError\",le({error:e},I));throw \ne}};b.onerror=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:sendError\",I);ie(s);w()};b.onabort=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:sendAbort\",I);ie(s);w()};b.ontimeout=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:timeout\",I);ie(s);w()};if(!ce(n,\"htmx:beforeRequest\",I)){ie\n(o);w();return l}var k=or(n);var P=sr(n);oe([\"loadstart\",\"loadend\",\"progress\",\"abort\"],function(t){oe([b,b.upload],function(e){e.addEventListener(t,function(e){ce(n,\"htmx:xhr:\"+t,{lengthComputable:e.lengthComputable,loaded:e.loaded,total:e.total})})})});ce(n,\"htmx:beforeSend\",I);var Y=q?null:Er(b,n,T);b.send(Y);return \nl}function Pr(e,t){var r=t.xhr;var n=null;var i=null;if(O(r,/HX-Push:/i)){n=r.getResponseHeader(\"HX-Push\");i=\"push\"}else if(O(r,/HX-Push-Url:/i)){n=r.getResponseHeader(\"HX-Push-Url\");i=\"push\"}else \nif(O(r,/HX-Replace-Url:/i)){n=r.getResponseHeader(\"HX-Replace-Url\");i=\"replace\"}if(n){if(n===\"false\"){return{}}else{return{type:i,path:n}}}var a=t.pathInfo.finalRequestPath;var o=t.pathInfo.responsePath;var s=ne(e,\"hx-push-url\");var l=ne(e,\"hx-replace-url\");var u=ae(e).boosted;var f=null;var \nc=null;if(s){f=\"push\";c=s}else if(l){f=\"replace\";c=l}else if(u){f=\"push\";c=o||a}if(c){if(c===\"false\"){return{}}if(c===\"true\"){c=o||a}if(t.pathInfo.anchor&&c.indexOf(\"#\")===-1){c=c+\"#\"+t.pathInfo.anchor}return{type:f,path:c}}else{return{}}}function Mr(l,u){var f=u.xhr;var c=u.target;var e=u.etc;var \nt=u.requestConfig;var h=u.select;if(!ce(l,\"htmx:beforeOnLoad\",u))return;if(O(f,/HX-Trigger:/i)){_e(f,\"HX-Trigger\",l)}if(O(f,/HX-Location:/i)){er();var r=f.getResponseHeader(\"HX-Location\");var v;if(r.indexOf(\"{\")===0){v=E(r);r=v[\"path\"];delete v[\"path\"]}Nr(\"GET\",r,v).then(function(){tr(r)});return}var \nn=O(f,/HX-Refresh:/i)&&\"true\"===f.getResponseHeader(\"HX-Refresh\");if(O(f,/HX-Redirect:/i)){location.href=f.getResponseHeader(\"HX-Redirect\");n&&location.reload();return}if(n){location.reload();return}if(O(f,/HX-Retarget:/i)){if(f.getResponseHeader(\"HX-Retarget\")===\"this\"){u.target=l}else{u.target=ue(l,f.getResponseHead\ner(\"HX-Retarget\"))}}var d=Pr(l,u);var i=f.status>=200&&f.status<400&&f.status!==204;var g=f.response;var a=f.status>=400;var m=Q.config.ignoreTitle;var \no=le({shouldSwap:i,serverResponse:g,isError:a,ignoreTitle:m},u);if(!ce(c,\"htmx:beforeSwap\",o))return;c=o.target;g=o.serverResponse;a=o.isError;m=o.ignoreTitle;u.target=c;u.failed=a;u.successful=!a;if(o.shouldSwap){if(f.status===286){at(l)}R(l,function(e){g=e.transformResponse(g,f,l)});if(d.type){er()}var \ns=e.swapOverride;if(O(f,/HX-Reswap:/i)){s=f.getResponseHeader(\"HX-Reswap\")}var v=wr(l,s);if(v.hasOwnProperty(\"ignoreTitle\")){m=v.ignoreTitle}c.classList.add(Q.config.swappingClass);var p=null;var x=null;var y=function(){try{var e=document.activeElement;var \nt={};try{t={elt:e,start:e?e.selectionStart:null,end:e?e.selectionEnd:null}}catch(e){}var \nr;if(h){r=h}if(O(f,/HX-Reselect:/i)){r=f.getResponseHeader(\"HX-Reselect\")}if(d.type){ce(re().body,\"htmx:beforeHistoryUpdate\",le({history:d},u));if(d.type===\"push\"){tr(d.path);ce(re().body,\"htmx:pushedIntoHistory\",{path:d.path})}else{rr(d.path);ce(re().body,\"htmx:replacedInHistory\",{path:d.path})}}var \nn=T(c);je(v.swapStyle,c,l,g,n,r);if(t.elt&&!se(t.elt)&&ee(t.elt,\"id\")){var i=document.getElementById(ee(t.elt,\"id\"));var \na={preventScroll:v.focusScroll!==undefined?!v.focusScroll:!Q.config.defaultFocusScroll};if(i){if(t.start&&i.setSelectionRange){try{i.setSelectionRange(t.start,t.end)}catch(e){}}i.focus(a)}}c.classList.remove(Q.config.swappingClass);oe(n.elts,function(e){if(e.classList){e.classList.add(Q.config.settlingClass)}ce(e,\"htm\nx:afterSwap\",u)});if(O(f,/HX-Trigger-After-Swap:/i)){var o=l;if(!se(l)){o=re().body}_e(f,\"HX-Trigger-After-Swap\",o)}var s=function(){oe(n.tasks,function(e){e.call()});oe(n.elts,function(e){if(e.classList){e.classList.remove(Q.config.settlingClass)}ce(e,\"htmx:afterSettle\",u)});if(u.pathInfo.anchor){var \ne=re().getElementById(u.pathInfo.anchor);if(e){e.scrollIntoView({block:\"start\",behavior:\"auto\"})}}if(n.title&&!m){var t=C(\"title\");if(t){t.innerHTML=n.title}else{window.document.title=n.title}}Cr(n.elts,v);if(O(f,/HX-Trigger-After-Settle:/i)){var \nr=l;if(!se(l)){r=re().body}_e(f,\"HX-Trigger-After-Settle\",r)}ie(p)};if(v.settleDelay>0){setTimeout(s,v.settleDelay)}else{s()}}catch(e){fe(l,\"htmx:swapError\",u);ie(x);throw e}};var b=Q.config.globalViewTransitions;if(v.hasOwnProperty(\"transition\")){b=v.transition}if(b&&ce(l,\"htmx:beforeTransition\",u)&&typeof \nPromise!==\"undefined\"&&document.startViewTransition){var w=new Promise(function(e,t){p=e;x=t});var S=y;y=function(){document.startViewTransition(function(){S();return w})}}if(v.swapDelay>0){setTimeout(y,v.swapDelay)}else{y()}}if(a){fe(l,\"htmx:responseError\",le({error:\"Response Status Error Code \"+f.status+\" from \n\"+u.pathInfo.requestPath},u))}}var Xr={};function Dr(){return{init:function(e){return null},onEvent:function(e,t){return true},transformResponse:function(e,t,r){return e},isInlineSwap:function(e){return false},handleSwap:function(e,t,r,n){return false},encodeParameters:function(e,t,r){return null}}}function \nUr(e,t){if(t.init){t.init(r)}Xr[e]=le(Dr(),t)}function Br(e){delete Xr[e]}function Fr(e,r,n){if(e==undefined){return r}if(r==undefined){r=[]}if(n==undefined){n=[]}var t=te(e,\"hx-ext\");if(t){oe(t.split(\",\"),function(e){e=e.replace(/ /g,\"\");if(e.slice(0,7)==\"ignore:\"){n.push(e.slice(7));return}if(n.indexOf(e)<0){var \nt=Xr[e];if(t&&r.indexOf(t)<0){r.push(t)}}})}return Fr(u(e),r,n)}var Vr=false;re().addEventListener(\"DOMContentLoaded\",function(){Vr=true});function jr(e){if(Vr||re().readyState===\"complete\"){e()}else{re().addEventListener(\"DOMContentLoaded\",e)}}function \n_r(){if(Q.config.includeIndicatorStyles!==false){re().head.insertAdjacentHTML(\"beforeend\",\"<style>                      .\"+Q.config.indicatorClass+\"{opacity:0}                      .\"+Q.config.requestClass+\" .\"+Q.config.indicatorClass+\"{opacity:1; transition: opacity 200ms ease-in;}                      \n.\"+Q.config.requestClass+\".\"+Q.config.indicatorClass+\"{opacity:1; transition: opacity 200ms ease-in;}                    </style>\")}}function zr(){var e=re().querySelector('meta[name=\"htmx-config\"]');if(e){return E(e.content)}else{return null}}function $r(){var \ne=zr();if(e){Q.config=le(Q.config,e)}}jr(function(){$r();_r();var e=re().body;zt(e);var t=re().querySelectorAll(\"[hx-trigger='restored'],[data-hx-trigger='restored']\");e.addEventListener(\"htmx:abort\",function(e){var t=e.target;var r=ae(t);if(r&&r.xhr){r.xhr.abort()}});const \nr=window.onpopstate?window.onpopstate.bind(window):null;window.onpopstate=function(e){if(e.state&&e.state.htmx){ar();oe(t,function(e){ce(e,\"htmx:restored\",{document:re(),triggerEvent:ce})})}else{if(r){r(e)}}};setTimeout(function(){ce(e,\"htmx:load\",{});e=null},0)});return Q}()}); \n \nFix:  \n1.\u200b The following buttons do not work on panel 2. Generate Study Material: \na.\u200b Summary \nb.\u200b Key Terms \nc.\u200b Practice Questions \nd.\u200b External Resources \n \n",
        "added_at": "2025-11-30T01:35:16.621954"
      }
    ]
  },
  "04479226-bc36-49d1-afa2-8f4cbbee43fb": {
    "created_at": "2025-11-30T01:35:29.987158",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-30T01:35:29.990155"
      },
      {
        "name": "514 Lectures  3.pdf",
        "type": "document",
        "text": "ALGORITHMS\nFOR\nDATA\nSCIENCE\nMotivations\n-\n1) People are intested in analyzing + learning from massive dataset\n2) Data Science is highly interdisciplinary\nTextbooks\n-\n1) foundations of Data Science\nof Massive Datasets\n2) Mining\n\nLecture A1 :\nmunun\n! PROBABILITY\nREVIEW !\nconditional prob :\nfor events A and B,\nPr(AIB)=\nA\nindependence :\nA and B\nare\nindependent if\nPr(AIB) = Pr(A)\nor equivalently\nPr(ANB) = Pr(A)\n. Pr(B)\nindependent random variables :\nX & Y\nare\nindependent\nif the events\nEX = s3\nand EY = + 3\nare independent for all values St.\nex . Consider two\nindependent coin flips :\nEHH , HT, TH, TT3\n- let A be the event the first flip is heads -\n> EHH, HTS\n-\n- let is\nbe the\nevent that there\nis\n1 heads\nin\ntotal -\n>\nHT , TH3\n- let\nC be the\nevent that there\nare\n2 heads in total > \u00b7 H H3\n-> What is Pr(CIA) ?\nPr(A)\n= E\n,\nC1A = SHH3\n-\n>\nPr((1A)=\nPr(2(a)\n= P\n-\n> are A&B\nindependent ?\nPr(A)=, Pr(B) = E\n-\n>\nEHTS\nPr(AIB) = OA =\nsince\nPrCAIB)=\n= Pr(A)\nEn\nPrCANB) =\n= pas . PB)\n, the\nevents are independent.\nconsider a\nrandom X variable taking values\nin\nsome\ndiscrete set\nSCIR.\n- Expectation :\nthe weighted\naverage of the possible\nvalues\nE(x] = [ Pr(X = s)\n. S\nSES\nand for any function f : IR-IR ,\nE(f(x))=\nPr(x= s)\u00b7\n\n-variance:\nA measure of\nnow\nconcentrated the\nrandom variable is\nvar [x] = [E[(X-1E[X])3]\nX, X2X3\nP,\nP2\nPs\ne .g. if\nX takes the\nvalues\n1 , 2, 4 with probabilities\n13 , 12\n, 16 then\nIE[X] = (5 1) + (t2) + (t\n\u00b7 4) = 2\nvar [x] = (5 (1 - 2))\n+ (2(2 -2)] + (t (4-2)2] =\n/\nP,\nX\nP2X2\nPs\nY\n- X, , . ... Yn\nare\nK-wise independent\nit for\nany A c[n]\nwith 1AIK\nPr(1EXi\n= Sib)= Pr(i = Si)\nUSESA, .... SES\nitA\nI\nfull independence\nI\nK-wise\nex. Toss two independent coins.\n- let\nX\n,\n= 1 if the first\ncoin is heads and\nO otherwise.\n- let X2 = 1\nif the second\ncoin\nis heads and\nO otherwise\n- let Xz= (X, ++2) % 2\n. Show these\nrandom variables\nare\n2-wise independent but not\n3-wise,\noutcome\nY3\n\u00b7 X, &X2\nare obviously independent\nHH\nO\n\u00b7X& X3\n:\nPr(X, = 1)=\nPr(Xz = 1) = E\nI T\nI\njoint -\n> Pr(x = 1 , Xz = 1)=\nEHT3\nTi\nproduct\n-\n> I: -equal V pairwise\nTT\nI \"\n\u00b7 XzXz\n: (same reasoning as above\n-\n\n\u00b7\nX.,XzX3\n- for full independence , we'd need :\nPr(X,\n= 1\n, xz\n= 1 , xg\n= 1) = Pr(X,\n= 1) Pr(Xz\n= 1) Pr(Xz = 1)\nno three\ncolumns\n= .\n. t\nhave\nIs\n0\n= 1s\n-\n>\n2-wise\nbut not\n3-wise\nimm\nproperties\n- for scalar < . E(c\n. X] = a . #[x]\nand var[a\n. x] = var(X]\n- linearity of Expectation\n:\nGiven\nany set\nof\nrandom variables\nX,\nX2,\n.\n. ., Yn :\n#([Xi]\n= [E[Xi]\n;\n-Given any set of random variables\nX, Xz, ... Yn tudt\nare pairwise\nindependent\n:\nvar [i]\n= &var (xi]\ni\n- A random\nvariable\nX that only takes the\nvalues\nO and 1\nis called an\nIndicator\nRandom Variable\n-the expectation\nis #[x]\n= Pr(X = 1)\n= P\n-\n> E[X:] = P\n- the variance is var(ix] = #[x]\n- #(x] = Pr(X\n= 1)\n- Pr(X=1) =\np - p2\n- Suppose random variable\nX\ncan\nbe written as\nX= A + Az +... + An\nwhere\neach\nA :\nare\nindependent indicator variables v/ Pr(Ai) = p.\n- then\nX\nis\na Binomial\nRandom Variable\nwith\nparameters\nn\nand\np.\nPr[X = i) = (i)\n= pi(z\n- p)n\n- i\nncome fromsummingonaa trin)\n- Recall ECAi)= p and var[Ai] = p- p2\n\u2191\nW\n\u21b3by linearity of expectation\nand variance , E(X) = up\n& var[X]\n= np(1-p)\nGeometric\nRandom Variable\n- repeatedly toss a coin where the probability of\nheads is p\n. Let X\nbethe #\nof\ncoin tosses\nobserved up to including the\nfirst heads\n-then\nX\nis the\ngeometric\nrandom var\nwith parameter p : Pr(X+i)= (l-p): p .\n&\n#2x]=\n\nmmmme\nA2 :\nMarkov & Chebyshev\n- we often need to consider quantifies of the form\nPr(X]t) = [Pr[X= i)\nit\ne.g. the probability we get 360\nheads when we toss\n100 fair coins\n- if we know Pr[X=i) for each i, we\ncan work this out exactly\n&\n- the simplest concentration\nbound is\nMarkov's inequality.\n-\n> for any non-negative\nrandom variable\nX\nand any\nt30 :\nPr[XIt)\nA refer to slides for proofA\n(the larger the deviation t\n, the smaller the probability)\n- Chebyshev's\nInequality\n:\nfor any\nrandom variable X\nand any 730 :\nPr[/X-E(X)1]t) [Va(x)\nAlooke slides for proof\n- Law of Large\nNumbers : With enough samples n, the\nsample\naverage\nwill\nalways\nconcentrate to the\nmean\nummmmmm\nA3 : Application Ominmarkov\n&\nChebyshev\n\u2466\n\u2461\n\n-\n> #ii\n: Gintestis a\nhe a se\n/ -\nindicati\nrandom\n#[Dij) = 1\n\u00b7 Pr (Di ,j = 1) + 0\n. Pr (Dij =0)\nvariable\n-\nfor\ni\n. r . U\n. the\n= Pr (Di ,j =1)\nexpectation is\nthat the prob\n. is\n= 1\n- Since each CAPTCHA\nis chosen uniformly at\n+\n----- -- ;\nrandom from I , Pr(Dist\n= En\n\u00b7 Now , lets define the totalof\npairs that\nare\nduplicates\n-\nD = &\nDis\n-o this\ncounts the total # of\npairs that\nare duplicate\ni,je[m] ,i<j\n\u00b7\nex . if\nCAPTCHA \"abC\"\nshows\nupa times, that\ncreates (2)\n= 6 pairs\n\u00b7 Expected Value :\n- E(D)\n= [ELDi,i)\n\u246a\ni\n,je[m], i <]\nSub\n#[Di ,j]=\nY\n\u2193\n= (~).\n= mm-\n\u00b7S\nO you take\nm = 1000 samples.\nIf the database\nhas\nn =\n1,000, 000\nentries,\nthen the expected\nnumber of\nduplicates\nis :\nE(X) =ma\n=\n0 . 49\n\u2461You see\n10 pairwise duplicates\nand suspect something is\nup. But\nnow confident\ncan you be in your\ntests ?\n\u21b3concentration Inequalities/Tail Bounds :\nBounds\non the probability\nthat a random variable deviates\na\ncertain distance from the\nmean\n-\n> used in understanding how statistical tests perform,\nthe behavior ofrandomized\nalgorithms , etc.\n\u2193cont\n\n\u2462By Markov's inequality\n, if the\ndatabase\nis\n1 = 1,000, 000\nand\nyou\nobserve D= 10 duplicates, then the probability\nof this happening is :\nPr[D110) = ESD-00995\n\u2193\nThis is very small and you feel pretty sure that\nthe # of unique CAPTCHAS\nis much less than\n1,000,000.\nHow\nto calculate\nk in\nChebysher\nIneg.\nPr(1H-M/ = ko)1 z\n\u2460Express\nin\nterms of the mean\nH -M = a - M\n\u2461k=\nM\n\u2193\nchebyshev Inequality\n:\nPr(1H-M11K0) - k\n\u2193\n-> # of standard deviations away from the mean\nM + the expected value\no- variance\nunmunimme\nAt : Chernoff & Bernstein Bounds\n\nflipping coins\nWe \ufb02ip n = 100 independent coins, each are heads with probability 1/2\nand tails with probability 1/2. Let H be the number of heads.\nE[H] = n\n2 = 50 and Var[H] = n\n4 = 25\nMarkov:\nPr(H \u226560) \uf8ff.833\nPr(H \u226570) \uf8ff.714\nPr(H \u226580) \uf8ff.625\nChebyshev:\nPr(H \u226560) \uf8ff.25\nPr(H \u226570) \uf8ff.0625\nPr(H \u226580) \uf8ff.0278\nIn Reality:\nPr(H \u226560) = 0.0284\nPr(H \u226570) = .000039\nPr(H \u226580) < 10\u22129\n1\n-Markov chains are\nloose bounds since it only uses the mean\n-chebysher uses variance and\nis therefore tighter from\nMarkov , but still\nlove.\n= 0 2 0 = 25 = 5\nECH]\nz\na\nA\nM\nA\nu\nL\nSTOZE0\nI\nexact\nY\nBinomial\nk = AM-5\nTail\n\ntighter concentration bounds\nTo be fair... Markov and Chebyshev\u2019s inequalities apply much more generally\nthan to Binomial random variables like coin \ufb02ips.\nCan we obtain tighter bounds that still apply to many distributions?\n\u2022 Markov: Pr(X \u2265t) \uf8ffE[X]\nt . Sometimes called a First Moment approach.\n\u2022 Chebyshev:\nPr(|X \u2212E[X]| \u2265t) = Pr(|X \u2212E[X]|2 \u2265t2) \uf8ffVar[X]\nt2\nProved via analyzing the Second Moment.\n\u2022 What if we just apply Markov\u2019s inequality to even higher moments?\n2\n\na fourth moment bound\nConsider any random variable X:\nPr(|X \u2212E[X]| \u2265t) = Pr\n\u21e3\n(X \u2212E[X])4 \u2265t4\u2318\n\uf8ff\nE\nh\n(X \u2212E[X])4i\nt4\n.\nApplication to Coin Flips: Recall we have n = 100 independent fair coins and\nH is the number of heads.\n\u2022 Bound the fourth moment:\nE\nh\n(H \u2212E[H])4i\n= E\n2\n4\n 100\nX\ni=1\nHi \u221250\n!43\n5 =\nX\ni,j,k,`\ncijk`E[HiHjHkH`] = 1862.5\nwhere Hi = 1 if ith \ufb02ip is heads and 0 otherwise. Last calculations are messy!\n\u2022 Apply Fourth Moment Bound: Pr (|H \u2212E[H]| \u2265t) \uf8ff1862.5\nt4\n3\nH = %H ;\nwith\nH,30, 13 ,\nPrCH= 1) =2\nElY;] = 0\ni\n- E[Y]\n=t\nYi = Hi-t\n. the\nH-ACH) = [ , Vi\nECY?]=o\nS\nf\nxplanation\non\nnext\npage\n\nmarkov inequality\nPr(1X-E()((t)\n= pr ((X- E(X])\"It - EXE(X)9]\napplied to nonnegative\nE random variable\n(X-ECX3)!\n- for t = 0\n, the function >UP\nis increasing\non UIO\n- (X-E(X]) can\nnever be negative (to ath pur)\n- Markov\nsays ,\nfor\nanynonneg\nrandom\nvar\nY\nand\nany\naso\n, Pr(4(a) Es\n& Take\nY = (x-E(X)\"\nand\na = +0\n\u00b7 them\n:\nPr((X-E(x))\"It))--E)\nECH-E(H])\n= E((Hi-501: [rieE(HiHjHiHe)\n= 1862\n. 5\ni,j,k,l\n-This\nis the #((X-E(x))] calculation\nseer\nin the moment Markov bound\n1)Setup\n: -Hiis 1\nis flip i\nis heads,\nO Otherwise -\n>\nHiw Bernoulli (1/2)\n- totalof heade is\nH =&Hi ;\nMean : #(H) : [ECH)\n= 100\n\u00b7 E\n=50 .\ni = 1\n\ntighter bounds\nChebyshev\u2019s:\nPr(H \u226560) \uf8ff.25\nPr(H \u226570) \uf8ff.0625\nPr(H \u226580) \uf8ff.04\n4th Moment:\nPr(H \u226560) \uf8ff.186\nPr(H \u226570) \uf8ff.0116\nPr(H \u226580) \uf8ff.0023\nIn Reality:\nPr(H \u226560) = 0.0284\nPr(H \u226570) = .000039\nPr(H \u226580) < 10\u22129\n\u2022 We aren\u2019t restricted to applying Markov\u2019s to |X \u2212E[X]|k for some k.\n\u2022 Can use any \u201cmonotonic\u201d function f satisfying b > a i\u21b5f (b) > f (a):\nPr(|X \u2212E[X]| > t) = Pr(f (|X \u2212E[X]|) > f (t))\n4\n100 coin flips\n50/50 chance\n\u2193\nThree waysto upper bound events for He Bin (100, 0 . 5)\nPr(IH-50127)\nPr(H-501t)50\nt4\nt = 10\nt = 20\nt =30\nf(u) = u2-\n> Chebyshev\nflu)= % - 9th bound Markov\nflu) = e* -\n> Chernoff/Hoeffding/Bernstein\n\nexponential concentration bounds\n\u2022 Moment Generating Function: Consider for any r > 0:\nMr(X) = er\u00b7(X\u2212E[X]) =\n1\nX\nk=0\nr k(X \u2212E[X])k\nk!\nand note Mr(X) is monotonic for any r > 0 and so\nPr[X \u2212E[X] \u2265\u03bb] = Pr[Mr(X) \u2265er\u03bb] \uf8ffE[Mr(X)]\ner\u03bb\n\u2022 Weighted sum of all moments (r controls the weights) and choosing r\nappropriately gives various powerful exponential concentration bounds\nsuch as Cherno\u21b5, Bernstein, Hoe\u21b5ding, Azuma, Berry-Esseen, etc.\n5\n\nbernstein inequality\nBernstein\nInequality:\nConsider\nindependent\nrandom\nvariables\nX1, . . . , Xn 2 [-1,1]. Let \u00b5 = E[Pn\ni=1 Xi] and \u03c32 = Var[Pn\ni=1 Xi]. For\nany s \u22650:\nPr\n #####\nn\nX\ni=1\nXi \u2212\u00b5\n##### \u2265s\u03c3\n!\n\uf8ff2 exp\n\u2713\n\u2212s2\n4\n\u25c6\n.\nAssume that M = 1 and plug in t = s \u00b7 \u03c3 for s \uf8ff\u03c3.\nCompare to Chebyshev: Pr\n(##Pn\ni=1 Xi \u2212\u00b5\n## \u2265s\u03c3\n)\n\uf8ff\n1\ns2 .\n\u2022 An exponentially stronger dependence on s!\n6\n\ncomparision to chebyshev\nFor the number of heads, H, amongst n = 100 independent coin \ufb02ips:\nChebyshev:\nPr(H \u226560) \uf8ff.25\nPr(H \u226570) \uf8ff.0625\nPr(H \u226580) \uf8ff.04\nBernstein:\nPr(H \u226560) \uf8ff.0.412\nPr(H \u226570) \uf8ff.0108\nPr(H \u226580) \uf8ff0.0000907\nIn Reality:\nPr(H \u226560) = 0.0284\nPr(H \u226570) = .000039\nPr(H \u226580) < 10\u22129\nGetting much closer to the true probability.\n7\n\nexponential tail bounds\nBernstein Inequality:\nConsider independent random variables\nX1, . . . , Xn all falling in [\u2212M, M].\nLet \u00b5 = E[Pn\ni=1 Xi] and\n\u03c32 = Var[Pn\ni=1 Xi] = Pn\ni=1 Var[Xi]. For any t \u22650:\nPr\n $$$$$\nn\nX\ni=1\nXi \u2212\u00b5\n$$$$$ \u2265t\n!\n\uf8ff2 exp\n \n\u2212\nt2\n2\u03c32 + 4\n3Mt\n!\n.\nA useful variation for binary (indicator) random variables is:\nCherno\u21b5Bound (simpli\ufb01ed version): Consider independent random\nvariables X1, . . . , Xn taking values in {0, 1}. Let \u00b5 = E[Pn\ni=1 Xi]. For\nany \u03b4 \u22650\nPr\n #####\nn\nX\ni=1\nXi \u2212\u00b5\n##### \u2265\u03b4\u00b5\n!\n\uf8ff2 exp\n\u2713\n\u2212\u03b42\u00b5\n2 + \u03b4\n\u25c6\n.\n8\n\napplication: maximum load\nAndrew McGregor\n0\nA5\nV\n\nrandomized load balancing\n\u2022 n requests or tasks are randomly assigned to k servers.\n\u2022 Let Ri be the number requests assigned to the ith server.\n\u2022 Ri is a binomial random variable and hence has expectation:\nE[Ri] =\nn\nX\nj=1\nE[Irequest j assigned to i] =\nn\nX\nj=1\nPr [j assigned to i] = n\nk .\n\u2022 Variance:\nVar[Ri] = Var[\nn\nX\nj=1\nIrequest j assigned to i] =\nn\nX\nj=1\nVar[Ij assigned to i] = n\n\u27131\nk \u22121\nk2\n\u25c6\n< n/k\n1\n\nmaximum server load\nSuppose a server crashes if it\u2019s load exceeds twice the expected load.\nWhat\u2019s the probability some server has load greater than 2 \u00b7 E[Ri] = 2n\nk ?\n\u2022 By Markov\u2019s inequality, Pr[Ri \u22652E[Ri]] \uf8ff1/2.\n\u2022 By Chebyshev\u2019s inequality, Pr[Ri \u22652E[Ri]] \uf8ffVar[Ri]\nE[Ri]2 < k\nn.\nWe want to upper bound:\nPr\n\u2713\nmax\ni\n(Ri) \u22652n\nk\n\u25c6\n= Pr\n\u2713\uf8ff\nR1 \u22652n\nk\n%\nor . . . or\n\uf8ff\nRk \u22652n\nk\n%\u25c6\n= Pr\n k[\ni=1\n\uf8ff\nRi \u22652n\nk\n%!\nHow do we do this since R1, . . . , Rk are not independent?\n2\n\nthe union bound\nUnion Bound: For any random events A1, A2, ..., Ak,\nPr (A1 [ A2 [ . . . [ Ak) \uf8ffPr(A1) + Pr(A2) + . . . + Pr(Ak).\nProof:\nLet X = P\ni Xi where Xi = 1 if Ai happens and 0 otherwise.\nPr (A1 [ A2 [ . . . [ Ak) = Pr(X1 + . . . + Xk \u22651) \uf8ffE[X] (by Markov)\nand E[X] = P\ni E[Xi] = P\ni Pr(Xi = 1) = Pr(A1) + . . . + Pr(Ak).\n3\n\napplying the union bound\nWhat\u2019s the probability some server has load greater than 2 \u00b7 E[Ri] = 2n\nk ?\nPr\n\u2713\nmax\ni\n(Ri) \u22652n\nk\n\u25c6\n= Pr\n k[\ni=1\n\uf8ff\nRi \u22652n\nk\n%!\n\uf8ff\nk\nX\ni=1\nPr\n\u2713\uf8ff\nRi \u22652n\nk\n%\u25c6\n(Union Bound)\n\uf8ff\nk\nX\ni=1\nk\nn = k2\nn\n(Bound from Chebyshev)\nIf k \u2327pn, max load will be small compared to the expected load with\ngood probability. When k is smaller, each server gets more requests and\nlaw of large numbers implies they gets closer to the expected number.\n4\n\nalternative analysis via chernoff bound\nCherno\u21b5Bound: Since Ri is binomial distributed:\nPr(Ri \u22652E[Ri]) \uf8ffPr (|Ri \u2212E[Ri]| \u2265E[Ri]) \uf8ff2 exp\n\u2713\n\u221212 \u00b7 E[Ri]\n2 + 1\n\u25c6\n= 2 exp\n\u21e3\n\u2212n\n3k\n\u2318\nand so by the union bound\nPr(Ri \u22652E[Ri] for some i 2 [k]) \uf8ff2k exp\n\u21e3\n\u2212n\n3k\n\u2318\nBecomes very small (as n tends to in\ufb01nity) if k \u2327n/(log n) whereas for\nthe Chebyshev analysis we needed k \u2327pn.\nExercise: If k = n, use Cherno\u21b5to show that for su\ufb03ciently large n,\nPr[max Ri \uf8ff3 log n] > 1 \u22121/n .\n5\n\nhash functions\nAndrew McGregor\n0\nA6\nV\n\nhash functions\n\u2022 In maximum load example, we made a separate choice about which\nserver to send each request. What if we want identical requests sent to\nsame server?\n\u2022 Classic Solution: Hash Functions\n\u2022 Suppose U is set of possible requests and we have n servers\n\u2022 Before the requests start arriving, randomly pick a function\nh : U ! [n]\n\u2022 Send request x 2 U to server h(x).\n\u2022 Simplest construction is picking a random value in [n] for each x 2 U\nand de\ufb01ne h(x) to be this value. Is fully random but very ine\ufb03cient!\n\u2022 Next we will see more e\ufb03cient (but less random) construction. (Also,\nin practice, often su\ufb03ces to use hash functions like MD5, SHA-2, etc.)\n1\n\nefficiently computable hash function: example\n\u2022 For simplicity, assume U = {0, 1, . . . , |U| \u22121}\n\u2022 Let p \u2265|U| be a prime number.\n\u2022 Pick a 2 {1, . . . , p \u22121} and b 2 {0, 1, . . . , p \u22121} uniformly at random\n\u2022 For each x 2 U de\ufb01ne h(x) = ((ax + b) mod p) mod n\nThis function is \u201c2-Universal\u201d and it\u2019s often the only property you need!\n2-Universal Hash Function (low collision probability). A random\nhash function from h : U ! [n] is two universal if for all x 6= y 2 U:\nPr[h(x) = h(y)] \uf8ff1\nn.\n2\n\npairwise independence\nAnother common requirement for a hash function:\nPairwise Independent Hash Function. A random hash function from\nh : U ! [n] is pairwise independent if for all i, j 2 [n] and for all\nx 6= y 2 U:\nPr[h(x) = i \\ h(y) = j] = 1\nn2 .\nWhich is a more stringent requirement? 2-universal or pairwise independent?\nPr[h(x) = h(y)] =\nn\nX\ni=1\nPr[h(x) = i \\ h(y) = i] = n \u00b7 1\nn2 = 1\nn .\nA closely related (ax + b) mod p construction gives pairwise independence\non top of 2-universality.\nNote: A fully random hash function is both 2-universal and pairwise\nindependent. But it is not e\ufb03ciently implementable.\n3\n\none-level hash tables\nAndrew McGregor\n0\nA7\nV\n\nhash tables\nWant to store a set of items from some \ufb01nite but massive universe of\nitems, e.g., images, text documents, 128-bit IP addresses.\nGoal: support query(x) to check if x is in the set in O(1) time.\nClassic Solution: Hash Tables\n\u2022 Static hashing since we won\u2019t worry about insertion and deletion today.\n1\n\nhash tables\n\u2022 Use 2-wise independent hash function h : U ! [n] maps elements in\nuniverse U to indices of an array. Store x 2 U in location h(x)\n\u2022 Collisions: when we insert m items into the hash table we may have\nto store multiple items in the same location (typically as a linked list).\n\u2022 If there are no collisions then the query time is O(1). More generally it\nis O(L) where L is the length of the longest linked list.\n2\n\nexpected number of collisions\n\u2022 The number of pairwise collisions is\nC =\nX\ni,j2[m],i<j\nCi,j\nwhere Ci,j = 1 if items i and j collide, and 0 otherwise.\n\u2022 Since the hash function was 2-wise independent,\nE[Ci,j] = Pr[Ci,j = 1] = Pr[h(xi) = h(xj)] = 1\nn .\n\u2022 By linearity of expectation,\nE[C] =\nX\ni,j2[m],i<j\n1\nn =\n\"m\n2\n#\nn\n= m(m \u22121)\n2n\n.\n3\n\ncollision free hashing\n\u2022 For n = 4m2 we have\nE[C] = m(m \u22121)\n2n\n= m(m \u22121)\n8m2\n< 1\n8 .\n\u2022 Apply Markov\u2019s Inequality:\nPr[C \u22651] \uf8ffE[C]\n1\n< 1\n8 .\n\u2022 Therefore,\nPr[C = 0] = 1 \u2212Pr[C \u22651] > 1 \u22121\n8 = 7\n8 .\n\u2022 Hence we can ensure O(1) query time but we are using O(m2) space\nto store m items.\n4\n\ntwo-level hash tables\nAndrew McGregor\n0\nAf\nV\n\ntwo level hashing\n\u2022 One-Level Hashing enabled O(1) query time but required O(m2) space.\n\u2022 Can we achieve O(1) query time with only O(m) space?\n\u2022 Two-Level Hashing:\n\u2022 For the ith bucket, pick a collision free hash function mapping [si] ! [4s2\ni ]\nwhere si values in the ith bucket.\n\u2022 Previously showed a random function is collision free with probability \u22657\n8\nso just generate a random hash function and check it is collision free.\n1\n\nspace usage\nQuery time for two level hashing is O(1): requires evaluating two hash\nfunctions. What is the expected space usage?\nUp to constants, space used is: E[S] = n + 4 Pn\ni=1 E[s2\ni ]\nE[s2\ni ] = E\n\" m\nX\nj=1\nIh(xj )=i\n!2#\n= E\n2\n4 X\nj,k2[m]\nIh(xj )=i \u00b7 Ih(xk )=i\n3\n5 =\nX\nj,k2[m]\nE\n\u21e5\nIh(xj )=i \u00b7 Ih(xk )=i\n\u21e4\n.\n\u2022 For j = k, E\n\u21e5\nIh(xj )=i \u00b7 Ih(xk )=i\n\u21e4\n= E\nh.\nIh(xj )=i\n/2i\n= Pr[h(xj) = i] = 1\nn.\n\u2022 For j 6= k, E\n\u21e5\nIh(xj )=i \u00b7 Ih(xk )=i\n\u21e4\n= Pr[h(xj) = i \\ h(xk) = i] =\n1\nn2 .\nxj, xk: stored items, n: hash table size, h: random hash function, S: space usage of two\nlevel hashing, si: # items stored in hash table at position i.\n2\n\nspace usage\nE[s2\ni ] =\nX\nj,k2[m]\nE\n\u21e5\nIh(xj )=i \u00b7 Ih(xk )=i\n\u21e4\n= m \u00b7 1\nn + 2 \u00b7\n \nm\n2\n!\n\u00b7 1\nn2\n= m\nn + m(m \u22121)\nn2\n\uf8ff2 (If we set n = m.)\n\u2022 For j = k, E\n\u21e5\nIh(xj )=i \u00b7 Ih(xk )=i\n\u21e4\n= 1\nn.\n\u2022 For j 6= k, E\n\u21e5\nIh(xj )=i \u00b7 Ih(xk )=i\n\u21e4\n=\n1\nn2 .\nTotal Expected Space Usage: (if we set n = m)\nE[S] = n + 4\nn\nX\ni=1\nE[s2\ni ] \uf8ffn + 4n \u00b7 2 = 9n = 9m.\nNear optimal space with O(1) query time!\nxj, xk: stored items, m: # stored items, n: hash table size, h: random hash function, S:\nspace usage of two level hashing, si: # items stored at pos i.\n3\n\nbloom filters\nAndrew McGregor\n0\nan\n\napproximately maintaining a set\nWant to store a set S of items from a massive universe of possible items\n(e.g., images, text documents, IP addresses).\nGoal: Support insert(x) to add x to the set and query(x) to check if x is\nin the set. Both in constant time. Allow small probability \u03b4 > 0 of false\npositives: for any x /2 S,\nPr(query(x) = 1) \uf8ff\u03b4.\nSolution: Bloom \ufb01lters.\nSpace advantage over two-level hash tables: While O(m) slots were\nsu\ufb03cient to store m objects, if each object is b bits this takes O(mb)\nbits. Bloom \ufb01lters will only use O(m) bits assuming \u03b4 is constant.\n1\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\nNo false negatives. False positives more likely with more insertions.\n2\n\napplications: determining when to cache\nAkamai (Boston-based company serving 15 \u221230% of all web tra\ufb03c) applies\nBloom \ufb01lters to prevent caching of \u2018one-hit-wonders\u2019 \u2013 pages only visited once\n\ufb01ll over 75% of cache.\n\u2022 Ideally, you\u2019d only cache a page if it\u2019s the second time it\u2019s been requested.\n\u2022 A Bloom Filter can be used to approximately track the url\u2019s you\u2019ve seen\nbefore without have to store them all! When url x comes in, if query(x) = 1,\ncache the page if it isn\u2019t already cached. If not, run insert(x) so that if it\ncomes in again, it will be cached.\n\u2022 False positive: If the Bloom \ufb01lter has a false positive rate of \u03b4 = .05, the\nnumber of cached one-hit-wonders will be reduced by at least 95%.\n3\n\nbloom filter analysis\nAndrew McGregor\n0\nAl\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nof elements U ! [m].\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n1\n\nanalysis\nFor a bloom \ufb01lter with m bits and k hash functions, the insertion and query\ntime is O(k). How does the false positive rate \u03b4 depend on m, k, and the\nnumber of items inserted?\nStep 1: What is the probability that after inserting n elements, the ith bit of\nthe array A is still 0? n \u21e5k total hashes must not hit bit i.\nPr(A[i] = 0) = Pr\n!\nh1(x1) 6= i \\ . . . \\ hk(x1) 6= i\n\\ h1(x2) 6= i . . . \\ hk(x2) 6= i \\ . . .\n\"\n= Pr\n!\nh1(x1) 6= i) \u21e5. . . \u21e5Pr\n!\nhk(x1) 6= i) \u21e5Pr\n!\nh1(x2) 6= i) . . .\n|\n{z\n}\nk\u00b7n events each occuring with probability 1\u22121/m\n=\n\u2713\n1 \u22121\nm\n\u25c6kn\n2\n\nanalysis\nHow does the false positive rate \u03b4 depend on m, k, and the number of items\ninserted?\nWhat is the probability that after inserting n elements, the ith bit of the array\nA is still 0?\nPr(A[i] = 0) =\n\u2713\n1 \u22121\nm\n\u25c6kn\n\u21e1e\u2212kn\nm\nLet T be the number of zeros in the array after n inserts. Then,\nE[T] = m\n\u2713\n1 \u22121\nm\n\u25c6kn\n\u21e1me\u2212kn\nm\nn: total number items in \ufb01lter, m: number of bits in \ufb01lter, k: number of random hash\nfunctions, h1, . . . hk: hash functions, A: bit array, \u03b4: false positive rate.\n3\n\ncorrect analysis sketch\nIf T is the number of 0 entries, for a non-inserted element w:\nPr(A[h1(w)] = . . . = A[hk(w)] = 1)\n= Pr(A[h1(w)] = 1) \u21e5. . . \u21e5Pr(A[hk(w)] = 1)\n= (1 \u2212T/m) \u21e5. . . \u21e5(1 \u2212T/m)\n= (1 \u2212T/m)k\n\u2022 How small is T/m? Note that T\nm \u2265m\u2212nk\nm\n\u21e1e\u2212kn\nm when kn \u2327m. More\ngenerally, it can be shown that T/m = \u2326\n\u21e3\ne\u2212kn\nm\n\u2318\nvia Theorem 2 of:\ncglab.ca/~morin/publications/ds/bloom-submitted.pdf\n4\n\nfalse positive rate\nFalse Positive Rate: with m bits of storage, k hash functions, and n items\ninserted \u03b4 \u21e1\n\u21e3\n1 \u2212e\n\u2212kn\nm\n\u2318k\n.\n0\n5\n10\n15\n20\n25\n30\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n\u2022 Can di\u21b5erentiate to show optimal number of hashes is k = ln 2 \u00b7 m\nn (rounded\nto the nearest integer). This gives \u03b4 \u21e11/2(m/n) ln 2.\n\u2022 Balances between \ufb01lling up the array with too many hashes and having\nenough hashes so that even when the array is pretty full, a new item is\nunlikely to have all its bits set (yield a false positive)\n5\n\ncombining estimates and the median trick\nAndrew McGregor\n0\nAl\n\ncombining noisy estimates\n\u2022 Suppose we have a randomized algorithm for estimating a quantity \u2713\nwhere the output of the algorithm is noisy.\n\u2022 Run the algorithm r times to get independent estimates X1, . . . , Xr\n\u2022 We can combine estimates to get a better estimate for \u2713.\n\u2022 \u201cCombine\u201d and \u201cBetter\u201d mean di\u21b5erent things in di\u21b5erent contexts. . .\n1\n\nexample 1\n\u2022 If estimates are never underestimates:\n\u2022 Return X = min(X1, . . . , Xr)\n\u2022 Suppose Pr[Xi > (1 + \u270f)\u2713] \uf8ff\u03b4 for each i.\n\u2022 Then\nPr[\u2713\uf8ffX \uf8ff(1 + \u270f)\u2713] = 1 \u2212Pr[Xi > (1 + \u270f)\u2713]r \u22651 \u2212\u03b4r\n2\n\nexample 2\n\u2022 If there are only two possible outputs and only one is correct:\n\u2022 Return X = majority(X1, . . . , Xr) where we assume r is odd to avoid ties.\n\u2022 Let Z be the number of answers that are incorrect.\nPr[X is incorrect] = Pr[Z > r/2]\n\u2022 If each Xi is incorrect with probability \u03b4 < 1/2 then Z \u21e0Bin(r, \u03b4) and we\ncan apply the Cherno\u21b5Bound:\nPr[Z > r/2] = Pr[Z > E[Z](1 + \u03b3)] \uf8ff2 exp(\u2212\u03b32E[Z]/(2 + \u03b3))\nwhere \u03b3 = 1/(2\u03b4) \u22121.\n\u2022 For example, if \u03b4 = 1/4 then \u03b3 = 1 and\nPr[X is incorrect ] \uf8ff2 exp(\u2212r/12) .\n3\n\nexample 3: averaging to reduce variance\n\u2022 Assuming estimates have correct expectation and small variance:\n\u2022 Return X = P\ni Xi/r, i.e., the mean of the estimates.\n\u2022 If E[Xi] = \u2713and Var[Xi] = \u03c32 for each i then\nE[X] = \u2713\nand\nVar[X] = \u03c32/r .\n\u2022 By the Chebyshev bound,\nPr[(1 \u2212\u270f)\u2713\uf8ffX \uf8ff(1 + \u270f)\u2713] = 1 \u2212Pr[|X \u2212\u2713| > \u270f\u2713] \u22651 \u2212\n\u03c32\nr\u270f2\u27132\n\u2022 E.g., setting r = 4\u03c32\u2713\u22122\u270f\u22122 ensures the probability is at least 3/4.\n\u2022 Could make 3/4 closer to 1 by increasing t but there\u2019s a better way. . .\n4\n\nexample 4: median trick\n\u2022 If estimates have the correct expectation and small variance:\n\u2022 Split r estimates into r1 = 12 log(2/\u03b4) groups of size r2 = 4\u03c32\u2713\u22122\u270f\u22122:\nX1,1\nX1,2\n. . .\nX1,r2\nX2,1\nX1,2\n. . .\nX2,r2\n...\n...\n...\n...\nXr1,1\nXr1,2\n. . .\nXr1,r2\n\u2022 Return Y = median(Y1, . . . , Yr1) where Yi is mean of ith row.\n\u2022 Example 3 analysis implies:\nPr[(1 \u2212\u270f)\u2713\uf8ffYi \uf8ff(1 + \u270f)\u2713] \u22651 \u2212\n\u03c32\nr2\u270f2\u27132 = 3/4\n\u2022 Example 2 analysis implies:\nPr[(1\u2212\u270f)\u2713\uf8ffmedian(Y1, . . . , Yr1) \uf8ff(1+\u270f)\u2713] \u22651\u22122 exp(\u2212r1/12) = 1\u2212\u03b4\n5\n\nstreams and frequent items\nAndrew McGregor\n0\n\u2191\n\nstreaming algorithms\nStream Processing: Have a massive dataset x1, x2, . . . , xn that arrive in\na continuous stream. Not nearly enough space to store all the items (in a\nsingle location).\n\u2022 Still want to analyze and learn from this data.\n\u2022 Typically must compress the data on the \ufb02y, storing a data structure\nfrom which you can still learn useful information.\n\u2022 Often the compression is randomized. E.g., Bloom \ufb01lters.\n\u2022 Compared to traditional algorithm design, which focuses on runtime,\nthe big question here is how much space is needed to answer a problem.\n1\n\nsome examples\n\u2022 Sensor data: images from telescopes (15 terabytes per night from the\nLarge Synoptic Survey Telescope), readings from seismometer arrays\nmonitoring and predicting earthquake activity, tra\ufb03c cameras and\ntravel time sensors (Smart Cities), electrical grid monitoring.\n\u2022 Internet Tra\ufb03c: 500 million Tweets per day, 5.6 billion Google\nsearches, billions of ad-clicks and other logs from instrumented\nwebpages, IPs routed by network switches, ...\n\u2022 Datasets in Machine Learning: When training e.g., a neural network\non a large dataset (ImageNet with 14 million images), the data is\ntypically processed in a stream due to storage limitations.\n2\n\nthe frequent items data stream problem\nk-Frequent Items (Heavy-Hitters) Problem: Consider a stream of n\nitems x1, . . . , xn (with possible duplicates). Return any item at appears at\nleast n/k times.\n\u2022 Trivial with O(n) space: Store the count for each item and return the\none that appears \u2265n/k times.\n\u2022 Can be at most k frequent items. Can we \ufb01nd them in o(n) space?\n3\n\nthe frequent items problem\nApplications of Frequent Items:\n\u2022 Finding top/viral items, e.g., products on Amazon, videos watched on\nYoutube, Google searches.\n\u2022 Finding very frequent IP addresses sending requests to detect DoS\nattacks/network anomalies.\n\u2022 \u2018Iceberg queries\u2019 for all items in a database with frequency above a\nthreshold.\nGenerally want very fast detection, without having to scan through\ndatabase/logs. That is, we want to maintain a running list of frequent\nitems that appear in a stream.\n4\n\napproximate frequent elements\nIssue: No algorithm using o(n) space can output just the items with\nfrequency \u2265n/k. Hard to tell between an item with frequency n/k\n(should be output) and n/k \u22121 (should not be output).\n(\u270f, k)-Frequent Items Problem: Consider a stream of n items\nx1, . . . , xn. Return a set F of items, including all items that appear \u2265n\nk\ntimes and no items that appear < (1 \u2212\u270f) \u00b7 n\nk times.\n\u2022 An example of relaxing to a \u2018promise problem\u2019: for items with\nfrequencies in [(1 \u2212\u270f) \u00b7 n\nk , n\nk ] no output guarantee.\n5\n\ncount-min sketch algorithm\nAndrew McGregor\n0\nAl3\n\u2193\n\ncount-min sketch\nCount-Min Sketch: A stream algorithm for estimating how many times\nelements has appeared in a stream x1, . . . , xn. Uses random hashing and\nis closely related to Bloom \ufb01lters.\n1\n\ncount-min sketch\nCount-Min Sketch: A stream algorithm for estimating how many times\nelements has appeared in a stream x1, . . . , xn. Uses random hashing and\nis closely related to Bloom \ufb01lters.\n1\n\ncount-min sketch\nCount-Min Sketch: A stream algorithm for estimating how many times\nelements has appeared in a stream x1, . . . , xn. Uses random hashing and\nis closely related to Bloom \ufb01lters.\n1\n\ncount-min sketch\nCount-Min Sketch: A stream algorithm for estimating how many times\nelements has appeared in a stream x1, . . . , xn. Uses random hashing and\nis closely related to Bloom \ufb01lters.\n1\n\ncount-min sketch\nCount-Min Sketch: A stream algorithm for estimating how many times\nelements has appeared in a stream x1, . . . , xn. Uses random hashing and\nis closely related to Bloom \ufb01lters.\n1\n\ncount-min sketch\nCount-Min Sketch: A stream algorithm for estimating how many times\nelements has appeared in a stream x1, . . . , xn. Uses random hashing and\nis closely related to Bloom \ufb01lters.\n1\n\ncount-min sketch\nCount-Min Sketch: A stream algorithm for estimating how many times\nelements has appeared in a stream x1, . . . , xn. Uses random hashing and\nis closely related to Bloom \ufb01lters.\nWill use A[h(x)] to estimate f (x) = |{i : xi = x}|, i.e., the frequency of x\nin the stream.\n1\n\ncount-min sketch accuracy\nUse A[h(x)] to estimate f (x).\nClaim 1: We always have A[h(x)] \u2265f (x). Why?\nf (x): frequency of x in the stream (i.e., number of items equal to x). h: random hash\nfunction. m: size of Count-min sketch array.\n2\n\ncount-min sketch accuracy\nUse A[h(x)] to estimate f (x).\nClaim 1: We always have A[h(x)] \u2265f (x). Why?\n\u2022 A[h(x)] counts the number of occurrences of any y with h(y) = h(x),\nincluding x itself.\nf (x): frequency of x in the stream (i.e., number of items equal to x). h: random hash\nfunction. m: size of Count-min sketch array.\n2\n\ncount-min sketch accuracy\nUse A[h(x)] to estimate f (x).\nClaim 1: We always have A[h(x)] \u2265f (x). Why?\n\u2022 A[h(x)] counts the number of occurrences of any y with h(y) = h(x),\nincluding x itself.\n\u2022 A[h(x)] = f (x) + P\ny6=x:h(y)=h(x) f (y).\nf (x): frequency of x in the stream (i.e., number of items equal to x). h: random hash\nfunction. m: size of Count-min sketch array.\n2\n\u2467\n\ncount-min sketch accuracy\nA[h(x)] = f (x) +\nX\ny6=x:h(y)=h(x)\nf (y)\n|\n{z\n}\nerror in frequency estimate\n.\nExpected Error:\nE\n2\n4\nX\ny6=x:h(y)=h(x)\nf (y)\n3\n5 =\nX\ny6=x\nPr(h(y) = h(x)) \u00b7 f (y)\n\uf8ff\nX\ny6=x\n1\nm \u00b7 f (y) = 1\nm \u00b7 (n \u2212f (x)) \uf8ffn\nm\nWhat is a bound on probability that the error is \u22652n\nm ?\nMarkov\u2019s inequality: Pr\nhP\ny6=x:h(y)=h(x) f (y) \u22652n\nm\ni\n\uf8ff1\n2.\nWhat property of h is required to show this bound? a) fully random\nb)\npairwise independent\nc) 2-universal\nd) locality sensitive\nf (x): frequency of x in the stream (i.e., number of items equal to x). h: random hash\nfunction. m: size of Count-min sketch array.\n3\n\ncount-min sketch accuracy\nClaim: For any x, with probability at least 1/2,\nf (x) \uf8ffA[h(x)] \uf8fff (x) + 2n\nm .\nHow can we improve the success probability?Repetition.\nf (x): frequency of x in the stream (i.e., number of items equal to x). h: random hash\nfunction. m: size of Count-min sketch array.\n4\n\ncount-min sketch accuracy\n5\n\ncount-min sketch accuracy\n5\n\ncount-min sketch accuracy\n5\n\ncount-min sketch accuracy\nEstimate f (x) with \u02dcf (x) = mini2[t] Ai[hi(x)]. (count-min sketch)\n5\n\ncount-min sketch accuracy\nEstimate f (x) with \u02dcf (x) = mini2[t] Ai[hi(x)]. (count-min sketch)\nWhy min instead of mean or median? The minimum estimate is always\nthe most accurate since they are all overestimates of the true frequency!\n5\nA\n\ncount-min sketch analysis\nEstimate f (x) by \u02dcf (x) = mini2[t] Ai[hi(x)]\n\u2022 For each x and i 2 [t], with probability \u22651/2:\nf (x) \uf8ffAi[hi(x)] \uf8fff (x) + 2n\nm .\n\u2022 What is Pr[f (x) \uf8ff\u02dcf (x) \uf8fff (x) + 2n\nm ]?\n1 \u22121/2t.\n\u2022 To get a good estimate with probability \u22651 \u2212\u03b4, set t = log(1/\u03b4).\n6\n\ncount-min application\nAndrew McGregor\n0\nA\n\nrecap: count-min sketch\nEstimate the number of occurrences of x, i.e., f (x) by\n\u02dcf (x) = min\ni2[t] Ai[hi(x)]\nIf t = log(1/\u03b4) then\nPr[f (x) \uf8ff\u02dcf (x) \uf8fff (x) + 2n/m] \u22651 \u2212\u03b4\n1\n\n(\u270f, k)-frequent items problem\nGiven stream of n items x1, . . . , xn where each xi 2 U. Return a set F,\nsuch that for every x 2 U:\n1. If f (x) \u2265n/k then x 2 F\n2. If f (x) < (1 \u2212\u270f)n/k then x 62 F\nwhere f (x) is the number of times x appears in the stream.\nRelationship to Frequency Estimation. If m = 2k/\u270fthen\nf (x) \uf8ff\u02dcf (x) \uf8fff (x) + \u270fn/k\nand outputting x if \u02dcf (x) \u2265n/k ensures both requirements satis\ufb01ed.\nBut do we need to query all possible x 2 U? That\u2019s time consuming and\nincreases the chance we\u2019d make some mistakes.\n2\n\nidentifying frequent elements\nOne approach:\n\u2022 Maintain set F of elements, initially empty, while processing stream\n\u2022 At step i:\n\u2022 Add ith stream element to F if its estimated frequency is \u2265i/k and it\nisn\u2019t already in F.\n\u2022 Remove any element from F whose estimated frequency is < i/k.\n\u2022 Stores O(k) items at any one time and F includes all elements with\nestimated frequency \u2265n/k at the end.\nError Probability: We query the Count-Min sketch at t = O(nk) times\nso setting \u03b4 = 0.001/t ensures the probability that a query answer is\ninsu\ufb03ciently accurate is at most 0.001 (by the Union bound).\n3\n\ndistinct elements\nAndrew McGregor\n0\nAl5\n\u2193\n\ndistinct elements\nDistinct Elements (Count-Distinct) Problem: Given a stream\nx1, . . . , xn, estimate the number of distinct elements in the stream. E.g.,\n1, 5, 7, 5, 2, 1 ! 4 distinct elements\nApplications:\n\u2022 Distinct IP addresses clicking on an ad or visiting a site.\n\u2022 Number of distinct search engine queries.\n\u2022 Counting distinct motifs in large DNA sequences.\n\u2022 Implemented in Google Sawzall, Facebook Presto, Apache Drill,\nTwitter Algebird\n1\n\nhashing for distinct elements\nDistinct Elements Problem: Given a stream x1, . . . , xn, estimate the\nnumber of distinct elements.\nMin-Hashing for Distinct Elements (variant of Flajolet-Martin):\n\u2022 Let h : U ! [0, 1] be a random hash function (with real valued output)\n\u2022 s := 1\n\u2022 For i = 1, . . . , n\n\u2022 s := min(s, h(xi))\n\u2022 Return \u02c6d = 1\ns \u22121\n2\n\nhashing for distinct elements\nDistinct Elements Problem: Given a stream x1, . . . , xn, estimate the\nnumber of distinct elements.\nMin-Hashing for Distinct Elements (variant of Flajolet-Martin):\n\u2022 Let h : U ! [0, 1] be a random hash function (with real valued output)\n\u2022 s := 1\n\u2022 For i = 1, . . . , n\n\u2022 s := min(s, h(xi))\n\u2022 Return \u02c6d = 1\ns \u22121\n2\n\nhashing for distinct elements\nDistinct Elements Problem: Given a stream x1, . . . , xn, estimate the\nnumber of distinct elements.\nMin-Hashing for Distinct Elements (variant of Flajolet-Martin):\n\u2022 Let h : U ! [0, 1] be a random hash function (with real valued output)\n\u2022 s := 1\n\u2022 For i = 1, . . . , n\n\u2022 s := min(s, h(xi))\n\u2022 Return \u02c6d = 1\ns \u22121\n2\n\nhashing for distinct elements\nDistinct Elements Problem: Given a stream x1, . . . , xn, estimate the\nnumber of distinct elements.\nMin-Hashing for Distinct Elements (variant of Flajolet-Martin):\n\u2022 Let h : U ! [0, 1] be a random hash function (with real valued output)\n\u2022 s := 1\n\u2022 For i = 1, . . . , n\n\u2022 s := min(s, h(xi))\n\u2022 Return \u02c6d = 1\ns \u22121\n2\n\nhashing for distinct elements\nDistinct Elements Problem: Given a stream x1, . . . , xn, estimate the\nnumber of distinct elements.\nMin-Hashing for Distinct Elements (variant of Flajolet-Martin):\n\u2022 Let h : U ! [0, 1] be a random hash function (with real valued output)\n\u2022 s := 1\n\u2022 For i = 1, . . . , n\n\u2022 s := min(s, h(xi))\n\u2022 Return \u02c6d = 1\ns \u22121\n2\n\nhashing for distinct elements\nDistinct Elements Problem: Given a stream x1, . . . , xn, estimate the\nnumber of distinct elements.\nMin-Hashing for Distinct Elements (variant of Flajolet-Martin):\n\u2022 Let h : U ! [0, 1] be a random hash function (with real valued output)\n\u2022 s := 1\n\u2022 For i = 1, . . . , n\n\u2022 s := min(s, h(xi))\n\u2022 Return \u02c6d = 1\ns \u22121\n2\n\nhashing for distinct elements\nDistinct Elements Problem: Given a stream x1, . . . , xn, estimate the\nnumber of distinct elements.\nMin-Hashing for Distinct Elements (variant of Flajolet-Martin):\n\u2022 Let h : U ! [0, 1] be a random hash function (with real valued output)\n\u2022 s := 1\n\u2022 For i = 1, . . . , n\n\u2022 s := min(s, h(xi))\n\u2022 Return \u02c6d = 1\ns \u22121\n2\n\nperformance in expectation\n\u2022 s is the minimum of d points chosen uniformly at random on [0, 1].\n\u2022 Can show via calculus that E[s] =\n1\nd+1 since E(s) =\nR 1\n0\nPr(s > x)dx\nand similarly that Var[s] \uf8ff1/(d + 1)2.\n\u2022 Hence, d = 1/E[s] \u22121. Does s \u21e1E[s] imply bd = 1/s \u22121 \u21e1d?\n\u2022 Exercise: For any \u270f2 (0, 1/2), of\n|s \u2212E[s]| \uf8ff\u270f\u00b7 E[s] =) (1 \u22124\u270f)d \uf8ffbd \uf8ff(1 + 4\u270f)d\n3\n\nconcentration bound\nHow well is s concentrates around its mean?\nE[s] =\n1\nd + 1 and Var[s] \uf8ff\n1\n(d + 1)2\nNow apply the Median Trick!\n\u2022 Repeat process r = r1r2 times in parallel.\n\u2022 Partition the estimates into r1 groups of size r2. Let si be the average\nof the ith group of estimates:\nPr [|si \u2212E[s]| \u2265\u270fE[s]] \uf8ffVar[si]\n(\u270fE[s])2 \uf8ffVar[s]/r2\n(\u270fE[s])2 = 1/4\nwhere the last step follow if r2 = 4/\u270f2.\n\u2022 Using Median Trick, if r1 = 12 log(2\u03b4\u22121) then median(s1, . . . , sr1) is\nbetween 1\u2212\u270f\nd+1 and 1+\u270f\nd+1 with probability at least 1 \u2212\u03b4.\n4\n\ndistinct elements in practice\nAndrew McGregor\n0\nA16\n\u2193\n\ndistinct elements in practice\nOur algorithm uses continuous valued fully random hash functions. Can\u2019t\nbe implemented...\n\u2022 The idea of using the minimum hash value of x1, . . . , xn to estimate\nthe number of distinct elements naturally extends to when the hash\nfunctions map to discrete values.\n\u2022 Flajolet-Martin (LogLog) algorithm and HyperLogLog.\nEstimate # distinct elements based\non max number of trailing zeros m.\nThe more distinct hashes we see, the\nhigher we expect the max to be.\n1\n\nloglog counting of distinct elements\nFlajolet-Martin (LogLog) algorithm and HyperLogLog.\nEstimate # distinct elements based on\nmaximum number of trailing zeros m.\nWith d distinct elements, roughly what do we expect m to be?\nPr(h(xi) has log d trailing zeros) =\n1\n2log d = 1\nd .\nSo with d distinct hashes, expect to see 1 with log d trailing zeros. Expect\nm \u21e1log d. m takes log log d bits to store.\nTotal Space: O\n! log log d\n\u270f2\n+ log d\n\"\nfor an \u270fapproximate count.\nNote: Careful averaging of estimates from multiple hash functions.\n2\n\nloglog space guarantees\nUsing HyperLogLog to count 1 billion distinct items with 2% accuracy:\nspace used = O\n\u2713log log d\n\u270f2\n+ log d\n\u25c6\n= 1.04 \u00b7 dlog2 log2 de\n\u270f2\n+ dlog2 de bits1\n= 1.04 \u00b7 5\n.022\n+ 30 = 13030 bits \u21e11.6 kB!\nMergeable Sketch: Consider the case (essentially always in practice) that the\nitems are processed on di\u21b5erent machines.\n\u2022 Given data structures (sketches) HLL(x1, . . . , xn), HLL(y1, . . . , yn) it is easy\nto merge them to give HLL(x1, . . . , xn, y1, . . . , yn). How?\n\u2022 Set the maximum # of trailing zeros to the maximum in the two sketches.\n1. 1.04 is the constant in the HyperLogLog analysis. Not important!\n3\n\nSummary\nSlides\n112-A16 :\n\n\nlocality sensitive hashing i\nAndrew McGregor\n0\nAl7\n\u2193\n\nanother fundamental problem\nJaccard Index: A similarity measure between two sets.\nJ(A, B) = |A \\ B|\n|A [ B| = # shared elements\n# total elements .\nAlso a natural measure for similarity between bit strings: interpret an n\nbit string as a set, containing the elements corresponding the positions of\nits ones. J(x, y) = # shared ones\ntotal ones\n.\n1\n\nsearch with jaccard similarity\nJ(A, B) = |A \\ B|\n|A [ B| = # shared elements\n# total elements .\nWant Fast Implementations For:\n\u2022 Near Neighbor Search: Have a database of n sets/bit strings and\ngiven a set A, want to \ufb01nd if it has high Jaccard similarity to anything\nin the database. \u2326(n) time with a linear scan.\n\u2022 All-pairs Similarity Search: Have n di\u21b5erent sets/bit strings and\nwant to \ufb01nd all pairs with high Jaccard similarity. \u2326(n2) time if we\ncheck all pairs explicitly.\nWill speed up via randomized locality sensitive hashing.\n2\n\napplications\nDocument Similarity:\n\u2022 E.g., detecting plagiarism, copyright infringement, spam.\n\u2022 Use Shingling (aka n-grams or k-mers) + Jaccard similarity.\n3\n\napplication: collaborative filtering\nOnline recommendation systems are often based on collaborative \ufb01ltering.\nSimplest approach: \ufb01nd similar users and make recommendations based on\nthose users.\n\u2022 Twitter: represent a user as the set of accounts they follow. Match similar\nusers based on the Jaccard similarity of these sets. Recommend that you\nfollow accounts followed by similar users. Net\ufb02ix: look at sets of movies\nwatched. Amazon: look at products purchased, etc.\n4\n\napplication: entity resolution\nEntity Resolution Problem: Want to combine records from multiple data\nsources that refer to the same entities.\n\u2022 E.g., data on individuals from voting registrations, property records, and\nsocial media accounts. Names and addresses may not exactly match, due to\ntypos, nicknames, moves, etc.\n\u2022 Still want to match records that all refer to the same person using all pairs\nsimilarity search.\nSee Section 3.8.2 of Mining Massive Datasets for a discussion of a real world\nexample involving 1 million customers. Naively this would be\n!1000000\n2\n\"\n\u21e1500\nbillion pairs of customers to check!\n5\n\napplication: spam and fraud detection\nMany applications to spam/fraud detection. E.g.\n\u2022 Fake Reviews: Common on websites like Amazon. Detection often\nlooks for (near) duplicate reviews on similar products, which have been\ncopied. \u2018Near duplicate\u2019 measured with shingles + Jaccard similarity.\n\u2022 Lateral phishing: Phishing emails sent to addresses at a business\ncoming from a legitimate email address at the same business that has\nbeen compromised.\n\u2022 One method of detection looks at the recipient list of an email and checks\nif it has small Jaccard similarity with any previous recipient lists. If not,\nthe email is \ufb02agged as possible spam.\n6\n\nlocality sensitive hashing\nA hash function is a locality sensitive hash function if the collision\nprobability is higher when two inputs are more similar (can design\ndi\u21b5erent functions for di\u21b5erent similarity metrics.)\n7\n\nlsh for similarity search\nHow does locality sensitive hashing help for similarity search?\n\u2022 Near Neighbor Search: Given item x, compute h(x). Only search for\nsimilar items in the h(x) bucket of the hash table.\n\u2022 All-pairs Similarity Search: Scan through all buckets of the hash\ntable and look for similar pairs within each bucket.\n8\n\nlocality sensitive hashing 2\nAndrew McGregor\n0\nAIS\n\u2193\n\nminhashing\nGoal: Speed up Jaccard similarity search.\nStrategy: Use random hashing to map each set to a very compressed\nrepresentation. Jaccard similarity can be estimated from these.\nMinHash(A): [Andrei Broder, 1997 at Altavista]\n\u2022 Let h : U ! [0, 1] be a random hash\nfunction\n\u2022 s := 1\n\u2022 For x1, . . . , x|A| 2 A\n\u2022 s := min(s, h(xk))\n\u2022 Return s\nIdentical to our distinct elements sketch!\n1\n\nminhash\nFor two sets A and B, what is Pr(MinHash(A) = MinHash(B))?\n\u2022 Since we are hashing into the continuous range [0, 1], we will never\nhave h(x) = h(y) for x 6= y (i.e., no spurious collisions)\n\u2022 MH(A) = MH(B) i\u21b5an item in A \\ B has the minimum hash value in\nboth sets. Therefore,\nPr(MH(A) = MH(B)) =\nX\nx2A\\B\nPr(MH(A) = h(x) \\ MH(B) = h(x))\n=\nX\nx2A\\B\nPr(x = arg min\ny2A[B\nh(y))\n=\nX\nx2A\\B\n1\n|A [ B| = |A \\ B|\n|A [ B| = J(A, B)\n2\n\nlsh with minhash\nGoal: Given a document y, identify all documents x in a database with\nJaccard similarity (of their shingle sets) J(x, y) \u22651/2.\nOur Approach:\n\u2022 Create a hash table of size m, choose a random hash function\ng : [0, 1] ! [m], and insert each item x into bucket g(MH(x)). Search\nfor items similar to y in bucket g(MH(y)).\n\u2022 What is Pr [g(MH(z)) = g(MH(y))] assuming J(z, y) \uf8ff1/3 and g is\ncollision free? At most 1/3\n\u2022 For each document x in your database with J(x, y) \u22651/2 what is the\nprobability you will \ufb01nd x in bucket g(MH(y))? At least 1/2\n3\n\nreducing false negatives\nWith a simple use of MinHash, we miss a match x with J(x, y) = 1/2 with\nprobability 1/2. How can we reduce this false negative rate?\nRepetition: Run MinHash t times independently, to produce hash values\nMH1(x), . . . , MHt(x). Apply random hash function g to map all these values to\nlocations in t hash tables.\n\u2022 To search for items similar to y, look at all items in bucket g(MH1(y)) of\nthe 1st table, bucket g(MH2(y)) of the 2nd table, etc.\n\u2022 What is the probability that x with J(x, y) = 1/2 is in at least one of these\nbuckets, assuming for simplicity g has no collisions?\n1\u2212(probability in no buckets) = 1 \u2212\n! 1\n2\n\"t \u21e1.99 for t = 7.\n\u2022 What is the probability that x with J(x, y) = 1/4 is in at least one of these\nbuckets, assuming for simplicity g has no collisions?\n1\u2212(probability in no buckets) = 1 \u2212\n! 3\n4\n\"t \u21e1.87 for t = 7.\nPotential for a lot of false positives! Slows down search time.\n4\n\nbalancing hit rate and query time\nWe want to balance a small probability of false negatives (a high hit rate) with\na small probability of false positives (a small query time.)\nCreate t hash tables. Each is indexed into not with a single MinHash value, but\nwith r values, appended together. A length r signature.\n5\n\nbalancing hit rate and query time\nConsider searching for matches in t hash tables, using MinHash signatures of\nlength r. For x and y with Jaccard similarity J(x, y) = s:\n\u2022 Probability that a single hash matches.\nPr [MHi,j(x) = MHi,j(y)] = J(x, y) = s.\n\u2022 Probability that x and y having matching signatures in repetition i.\nPr [MHi,1(x), . . . , MHi,r(x) = MHi,1(y), . . . , MHi,r(y)] = sr.\n\u2022 Probability that x and y don\u2019t match in repetition i: 1 \u2212sr.\n\u2022 Probability that x and y don\u2019t match in all repetitions: (1 \u2212sr)t.\n\u2022 Probability that x and y match in at least one repetition:\nHit Probability: 1 \u2212(1 \u2212sr)t.\n6\n\nlocality sensitive hashing 3\nAndrew McGregor\n01\n\nthe s-curve\nUsing t repetitions each with a signature of r MinHash values, the probability\nthat x and y with Jaccard similarity J(x, y) = s match in at least one\nrepetition is: 1 \u2212(1 \u2212sr)t.\n0\n0.2\n0.4\n0.6\n0.8\n1\nJaccard Similarity s\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nHit Probability\nr = 5, t = 30\nr and t are tuned depending on application. One approach: set t as large as\nyou can a\u21b5ord (this will increase the gradient around the cut-o\u21b5) and then\ntweak r such that the cut-o\u21b5appears in the right place.\n1\n\ns-curve example\nFor example: Consider a database with 10, 000, 000 audio clips. You are given\na clip x and want to \ufb01nd any y in the database with J(x, y) \u2265.9.\n\u2022 There are 10 true matches in the database with J(x, y) \u2265.9.\n\u2022 There are 10, 000 near matches with J(x, y) 2 [.7, .9].\nWith signature length r = 25 and repetitions t = 50, hit probability for\nJ(x, y) = s is 1 \u2212(1 \u2212s25)50.\n\u2022 Hit probability for J(x, y) \u2265.9 is \u22651 \u2212(1 \u2212.925)50 \u21e1.98\n\u2022 Hit probability for J(x, y) 2 [.7, .9] is \uf8ff1 \u2212(1 \u2212.925)50 \u21e1.98\n\u2022 Hit probability for J(x, y) \uf8ff.7 is \uf8ff1 \u2212(1 \u2212.725)50 \u21e1.007\nExpected Number of Items Scanned: (proportional to query time)\n\uf8ff10 + .98 \u21e410, 000 + .007 \u21e49, 989, 990 \u21e180, 000 \u232710, 000, 000.\n2\n\ngeneralizing locality sensitive hashing\nRepetition and s-curve tuning can be used for fast similarity search with other\nsimilarity metrics:\n\u2022 LSH schemes exist for many similarity/distance measures: hamming\ndistance, cosine similarity, etc.\nCosine Similarity: cos(\u2713(x, y)) =\nhx,yi\nkxk2\u00b7kyk2 .\n\u2022 cos(\u2713(x, y)) = 1 when \u2713(x, y) = 0\u25e6and cos(\u2713(x, y)) = 0 when\n\u2713(x, y) = 90\u25e6, and cos(\u2713(x, y)) = \u22121 when \u2713(x, y) = 180\u25e6\n3\n\nsimhash for cosine similarity\nSimHash Algorithm: LSH for cosine similarity.\n4\n\nsimhash for cosine similarity\nSimHash Algorithm: LSH for cosine similarity.\nSimHash(x) = sign(hx, ti) for a random vector t.\nWhat is Pr [SimHash(x) = SimHash(y)]?\n4\n\nsimhash for cosine similarity\nWhat is Pr [SimHash(x) = SimHash(y)]?\nSimHash(x) 6= SimHash(y) when the plane separates x from y.\n\u2022 Pr [SimHash(x) 6= SimHash(y)] = \u2713(x,y)\n180\n\u2022 Pr [SimHash(x) = SimHash(y)] = 1 \u2212\u2713(x,y)\n180\n\u21e1cos \u2713for small \u2713.\n5\n\ndimensionality reduction\nAndrew McGregor\n0\nA 20\n\u2193\n\nhigh dimensional data\n\u201cBig Data\u201d means not just many data points, but many measurements per\ndata point. I.e., very high dimensional data.\n\u2022 Twitter has 321 million active monthly users. Records (tens of) thousands of\nmeasurements per user: who they follow, who follows them, when they last\nvisited the site, timestamps for speci\ufb01c interactions, how many tweets they\nhave sent, the text of those tweets, etc.\n\u2022 A 3 minute Youtube clip with a resolution of 500 \u21e5500 pixels at 15\nframes/second with 3 color channels is a recording of \u22652 billion pixel values.\nEven a 500 \u21e5500 pixel color image has 750, 000 pixel values.\n\u2022 The human genome contains 3 billion+ base pairs. Genetic datasets often\ncontain information on 100s of thousands+ mutations and genetic markers.\n1\n\ndata as vectors and matrices\nIn data analysis and machine learning, data points with many attributes\nare often stored, processed, and interpreted as high dimensional vectors,\nwith real valued entries.\nSimilarities/distances between vectors (e.g.,\nhx, yi, kx \u2212yk2) have meaning for\nunderlying data points.\n2\n\ndatasets as vectors and matrices\nData points are interpreted as high dimensional vectors, with real valued\nentries. Data set is interpreted as a matrix.\nData Points: ~x1, ~x2, . . . , ~xn 2 Rd.\nData Set: X 2 Rn\u21e5d with ith rows equal to ~xi.\nMany data points n =) tall. Many dimensions d =) wide.\n3\n\nlow distortion embedding\nLow Distortion Embedding: Given ~x1, . . . , ~xn 2 Rd, distance function D, and\nerror parameter \u270f\u22650, \ufb01nd \u02dcx1, . . . , \u02dcxn 2 Rm (where m \u2327d) and distance\nfunction \u02dcD such that for all i, j 2 [n]:\n(1 \u2212\u270f)D(~xi, ~xj) \uf8ff\u02dcD(\u02dcxi, \u02dcxj) \uf8ff(1 + \u270f)D(~xi, ~xj).\nWe\u2019ll focus on the case where D and \u02dcD are Euclidean distances. I.e., the\ndistance between two vectors x and y is de\ufb01ned as\nk~x \u2212~yk2 =\nsX\ni\n(~x(i) \u2212~y(i))2\nThis is related to the Euclidean norm, k~zk2 =\npPn\ni=1 ~z(i)2.\n4\n\nthe johnson-lindenstrauss lemma\nJohnson-Lindenstrauss Lemma: For any set of points ~x1, . . . , ~xn 2 Rd\nand \u270f> 0 there exists a linear map M : Rd ! Rm such that m =\nO\n% log n\n\u270f2\n&\nand letting \u02dcxi = M~xi:\nFor all i, j : (1 \u2212\u270f)k~xi \u2212~xjk2 \uf8ffk\u02dcxi \u2212\u02dcxjk2 \uf8ff(1 + \u270f)k~xi \u2212~xjk2.\nFurther, if M 2 Rm\u21e5d has each entry chosen independently from\nN(0, 1/m), it satis\ufb01es the guarantee with high probability.\nFor d = 1 trillion, \u270f= .05, and n = 100, 000, m \u21e16600.\nVery surprising! Powerful result with a simple construction: applying a random\nlinear transformation to a set of points preserves distances between all those\npoints with high probability.\n5\n\nrandom projection\nFor any ~x1, . . . , ~xn and M 2 Rm\u21e5d with each entry chosen independently from\nN(0, 1/m), with high probability, letting \u02dcxi = M~xi:\nFor all i, j : (1 \u2212\u270f)k~xi \u2212~xjk2 \uf8ffk\u02dcxi \u2212\u02dcxjk2 \uf8ff(1 + \u270f)k~xi \u2212~xjk2.\n\u2022 M is known as a random projection. It is a random linear function, mapping\nlength d vectors to length m vectors.\n\u2022 M is data oblivious. Stark contrast to methods like PCA.\n6\n\nalgorithmic considerations\n\u2022 Alternative constructions: \u00b11 entries, sparse (most entries 0), Fourier\nstructured, etc. =) e\ufb03cient computation of \u02dcxi = M~xi.\n\u2022 Data oblivious property means that once M is chosen, \u02dcx1, . . . ,\u02dcxn can\nbe computed in a stream with little memory.\n\u2022 Storage is just O(nm) rather than O(nd).\n\u2022 Compression can be performed in parallel on di\u21b5erent servers.\n\u2022 When new data points are added, can be easily compressed, without\nupdating existing points.\n7\n\njohnson lindenstrauss: part 1\nAndrew McGregor\n0\nA21\n\u2193\n\ndistributional jl\nThe Johnson-Lindenstrauss Lemma is a direct consequence of:\nDistributional JL Lemma: Let M 2 Rm\u21e5d have each entry chosen\ni.i.d. as N(0, 1/m). If we set m = O\n\u21e3\nlog(1/\u03b4)\n\u270f2\n\u2318\n, then for any ~y 2 Rd,\nwith probability \u22651 \u2212\u03b4\n(1 \u2212\u270f)k~yk2 \uf8ffkM~yk2 \uf8ff(1 + \u270f)k~yk2\nI.e., applying a random matrix M to any vector ~y preserves the norm with high\nprobability. Like a low-distortion embedding, but for the length of a compressed\nvector rather than distances between vectors.\n2\n\ndistributional jl =) jl\nDistributional JL Lemma =) JL Lemma: Distributional JL show that a\nrandom projection M preserves the norm of any y. The main JL Lemma says\nthat M preserves distances between vectors. Since M is linear these are the\nsame thing!\nProof: Given x1, . . . , xn, de\ufb01ne\n#n\n2\n$\nvectors yij where yij = xi \u2212xj.\n\u2022 If we choose M with m = O\n#\n\u270f\u22122log 1/\u03b40$\n, for each yij with probability at\nleast 1 \u2212\u03b40 we have:\n(1 \u2212\u270f)kxi \u2212xjk2 \uf8ffkMxi \u2212Mxjk2 \uf8ff(1 + \u270f)kxi \u2212xjk2\n\u2022 Union Bound: Every distance preserved with probability 1 \u2212\n#n\n2\n$\n\u00b7 \u03b40.\n\u2022 Setting \u03b40 = \u03b4/\n#n\n2\n$\nensures all distances preserved with probability 1 \u2212\u03b4 and\nm = O\n\u2713log(1/\u03b40)\n\u270f2\n\u25c6\n= O\n \nlog(\n#n\n2\n$\n/\u03b4)\n\u270f2\n!\n= O\n\u2713log(n/\u03b4)\n\u270f2\n\u25c6\n3\n\njohnson lindenstrauss: part 2\nAndrew McGregor\n0\nA22\n\u2193\n\ndistributional jl proof (part 1 of 3)\nDistributional JL Lemma: Let M 2 Rm\u21e5d have independent N(0, 1/m)\nentries. If we set m = O\n\u21e3\nlog(1/\u03b4)\n\u270f2\n\u2318\n, then for any y 2 Rd, with probability\nat least 1 \u2212\u03b4\n(1 \u2212\u270f)kyk2 \uf8ffkMyk2 \uf8ff(1 + \u270f)kyk2.\n\u2022 Let \u02dcy = My and Mj be the jth row of M\n\u2022 For any j, \u02dcyj = hMj, yi = Pd\ni=1 gi \u00b7 yi where gi \u21e0N(0, 1/m).\n\u2022 By linearity of expectation:\nE[\u02dcyj] =\nd\nX\ni=1\nE[gi] \u00b7 yi = 0 .\n\u2022 Since E[\u02dcyj] = 0 we have E[\u02dcy 2\nj ] = Var[\u02dcyj]. Then, by linearity of variance:\nE[\u02dcy 2\nj ] = Var[\u02dcyj] =\nd\nX\ni=1\nVar[gi \u00b7 yi] =\nX\ni\ny 2\ni /m = kyk2\n2/m .\n\u2022 Hence E[k\u02dcyk2\n2] = E[P\nj \u02dcy 2\nj ] = kyk2\n2. Remains to show k\u02dcyk2\n2 is concentrated.\n2\n\ndistributional jl proof (part 2 of 3)\nStability of Gaussian Random Variables. For independent a \u21e0\nN(\u00b51, \u03c32\n1) and b \u21e0N(\u00b52, \u03c32\n2) we have:\na + b \u21e0N(\u00b51 + \u00b52, \u03c32\n1 + \u03c32\n2)\nLetting \u02dcy = My, we have:\n\u02dcyj =\nd\nX\ni=1\ngi \u00b7 yi where gi \u00b7 yi \u21e0N(0, y 2\ni /m).\nThus, \u02dcyj \u21e0N(0, Pd\ni=1 y 2\ni /m) = N(0, kyk2\n2/m).\n3\n\ndistributional jl proof (part 3 of 3)\nSo Far: Each entry of our compressed vector \u02dcy is Gaussian with :\n\u02dcyj \u21e0N(0, kyk2\n2/m) and E[k\u02dcyk2\n2] = kyk2\n2\nk\u02dcyk2\n2 = Pm\ni=1 \u02dcy 2\nj a Chi-Squared random variable with m degrees of freedom (a\nsum of m squared independent Gaussians)\nLemma: (Chi-Squared Concentration) Letting Z be a Chi-Squared ran-\ndom variable with m degrees of freedom,\nPr [|Z \u2212EZ| \u2265\u270fEZ] \uf8ff2e\u2212m\u270f2/8.\nIf we set m = 8\u270f\u22122 log(2/\u03b4) and Z = k\u02dcyk2\n2, then Chi-Squared Concentration\nimplies that with probability at least 1 \u2212\u03b4:\n(1 \u2212\u270f)kyk2\n2 \uf8ffk\u02dcyk2\n2 \uf8ff(1 + \u270f)kyk2\n2.\n4\n\njohnson lindenstrauss: part 3\nAndrew McGregor\n0\n\u21b3\n\nexample application: k-means clustering\nGoal: Separate n points in d dimensional space into k groups C1, . . . , Ck.\nk-means Objective: Cost(C1, . . . , Ck) =\nk\nX\nj=1\nX\n~x2Cj\nk~x \u2212\u00b5jk2\n2 where\n\u00b5j =\n1\n|Cj|\nX\n~x2Cj\n~x\nis the average of the points in Cj.\nExercise: Can be rewritten as Cost(C1, . . . , Ck) =\nk\nX\nj=1\nX\n~x1,~x22Cj\nk~x1 \u2212~x2k2\n2\n|Cj|\n1\n\nexample application: k-means clustering\nk-means Objective: Cost(C1, . . . , Ck) = Pk\nj=1\nP\n~x1,~x22Cj\nk~x1\u2212~x2k2\n2\n|Cj |\nIf we randomly project to m = O\n#\n\u270f\u22122 log n\n$\ndimensions, for all pairs ~x1, ~x2,\n(1 \u2212\u270f)k~x1 \u2212~x2k2\n2 \uf8ffk\u02dcx1 \u2212\u02dcx2k2\n2 \uf8ff(1 + \u270f)k~x1 \u2212~x2k2\n2\nLetting Cost(C1, . . . , Ck) = Pk\nj=1\nP\n~x1,~x22Cj\nk\u02dcx1\u2212\u02dcx2k2\n2\n|Cj |\n(1 \u2212\u270f)Cost(C1, . . . , Ck) \uf8ffCost(C1, . . . , Ck) \uf8ff(1 + \u270f)Cost(C1, . . . , Ck).\nUpshot: Can cluster in m dimensional space (much more e\ufb03ciently) and\nminimize Cost(C1, . . . , Ck).\n2\n\njl lemma is almost optimal\n\northogonal vectors\n\u2022 Recall that we say two vectors x, y are orthogonal if hx, yi = 0.\n\u2022 What is the largest set of mutually orthogonal unit vectors in\nd-dimensional space? Answer: d.\n\u2022 How large can a set of unit vectors in d-dimensional space be that\nhave all pairwise dot products |hx, yi| \uf8ff\u270f? Answer: 2\u2326(\u270f2d).\nAn exponentially large set of random vectors will be nearly pairwise\northogonal with high probability!\n4\n\northogonal vectors proof\nClaim: 2O(\u270f2d) random d-dimensional unit vectors will have all pairwise\ndot products |hx, yi| \uf8ff\u270f(be nearly orthogonal).\nProof: Let x1, . . . , xt 2 Rd have independent random entries \u00b1 1\np\nd .\n\u2022 What is kxik2? Every xi is always a unit vector.\n\u2022 What is E[hxi, xji]? E[hxi, xji] = 0\n\u2022 By a Bernstein bound, Pr[|hxi, xji| \u2265\u270f] \uf8ff2e\u2212\u270f2d/6.\n\u2022 If t = 1\n2e\u270f2d/12, using a union bound over\n!t\n2\n\"\n\uf8ff1\n8e\u270f2d/6 possible pairs,\nwith probability \u22653/4 all will be nearly orthogonal.\nWe won\u2019t prove it but this is essentially optimal: In d dimensions, there\ncan be at most 2O(\u270f2d) nearly orthogonal unit vectors.\n5\n\nconnection to dimensionality reduction\nRecall: The Johnson Lindenstrauss lemma states that if M 2 Rm\u21e5d is a\nrandom matrix (linear map) with m = O\n\u21e3\nlog n\n\u270f2\n\u2318\n, for x1, . . . , xn 2 Rd with\nhigh probability, for all i, j:\n(1 \u2212\u270f)kxi \u2212xjk2\n2 \uf8ffkMxi \u2212Mxjk2\n2 \uf8ff(1 + \u270f)kxi \u2212xjk2\n2.\nImplies: If x1, . . . , xn are nearly orthogonal unit vectors in d-dimensions\n(with pairwise dot products bounded by \u270f/8), then\nMx1\nkMx1k2\n, . . . ,\nMxn\nkMxnk2\nare nearly orthogonal unit vectors in m-dimensions (with pairwise dot\nproducts bounded by \u270f). Algebra is a bit messy but a good exercise to\npartially work through. Proof uses the fact that\nkx \u2212yk2\n2 = kxk2\n2 + kyk2\n2 \u22122hx, yi .\n6\n\nconnection to dimensionality reduction\nClaim 1: n nearly orthogonal unit vectors can be projected to\nm = O\n\u21e3\nlog n\n\u270f2\n\u2318\ndimensions and still be nearly orthogonal.\nClaim 2: In m dimensions, there can be at most 2O(\u270f2m) nearly\northogonal unit vectors.\n\u2022 For both of these to hold it must be that n \uf8ff2O(\u270f2m).\n\u2022 I.e., n = 2log n \uf8ff2O(\u270f2m) and so m = \u2326\n\u21e3\nlog n\n\u270f2\n\u2318\n.\n\u2022 Tells us that the JL lemma is optimal up to constants.\n7\n\npart ii: overview\nAndrew McGregor\n0\no\n\ndatasets as vectors and matrices\nData points are interpreted as high dimensional vectors, with real valued\nentries. Data set is interpreted as a matrix.\nData Points: ~x1, ~x2, . . . , ~xn 2 Rd.\nData Set: X 2 Rn\u21e5d with ith rows equal to ~xi.\nMany data points n =) tall. Many dimensions d =) wide.\n1\n\nlow rank embedding\n\nwhen can we better than jl?\n\u2022 Goal of JL was to reduce the dimension of data points such that\nproperties of the original data set were preserved.\n\u2022 In JL, the compression is linear, i.e., by applying a matrix. The matrix\nwas chosen randomly and without regard to the data set.\n\u2022 What if we chose matrix taking into account structure of dataset? Can\ngive better compression than random projection?\n\u2022 Short Answer: There\u2019s a lot we can do if there\u2019s a structure in the data.\nThis section will be need a lot of linear algebra. Today we\u2019ll use:\n\u2022 A set of vectors B is a basis for a set of vectors A, if every vector in A\nis a linear combination of vectors in B.\n\u2022 The dimension of A is the size of its smallest basis.\n3\n\nexamples of data having structure\n\u2022 Data points might be approximately reconstructed from a basis of\nk \u2327d vectors.\n4\n\ndual view of low-rank approximation\n\u2022 The columns of X might be approximately spanned by k vectors.\n5\n\nsingular value decomposition (svd)\nAny matrix X 2 Rn\u21e5d can be written as X = U\u2303VT.\n\u2022 U has orthonormal columns ~u1, . . . , ~ur 2 Rn (left singular vectors).\n\u2022 V has orthonormal columns ~v1, . . . , ~vr 2 Rd (right singular vectors).\n\u2022 \u2303is diagonal with elements \u03c31 \u2265\u03c32 \u2265. . . \u2265\u03c3r > 0 (singular values).\nThe \u2018swiss army knife\u2019 of modern linear algebra.\n7\n\napplications of low-rank approximation\n\u2022 We\u2019ll look at applications including Low-Rank Matrix Completion and\nLatent Semantic Analysis.\n\u2022 Classic example of Matrix Completion: the Net\ufb02ix prize problem. The\nentries of the data matrix correspond to ratings that di\u21b5erent users\nhave given to di\u21b5erent movies. But the data matrix has missing entries!\n\u2022\n8\n\nspectral clustering\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nCommunity detection in naturally occurring networks.\n10\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nCommunity detection in naturally occurring networks.\n10\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nLinearly separable data.\n10\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nNon-linearly separable data k-nearest neighbor graph.\n10\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nNon-linearly separable data k-nearest neighbor graph.\n10\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nNon-linearly separable data k-nearest neighbor graph.\n10\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nNon-linearly separable data k-nearest neighbor graph.\nCan \ufb01nd this cut using eigenvectors!\n10\n\nthe laplacian matrix\nFor a graph with adjacency matrix A and degree matrix D, L = D \u2212A is the\ngraph Laplacian.\nFor any vector ~v, its \u2018smoothness\u2019 over the graph is given by:\nX\n(i,j)2E\n(~v(i) \u2212~v(j))2 = ~v TL~v.\nSpectral clustering is related to \ufb01nding a particular eigenvector of the Laplacian\nmatrix and then partitioning the nodes of based on the values in this vector.\n11\n\nstochastic block model\n\nstochastic block model\n\u2022 For many algorithms in data science, such as spectral clustering, there\nare no worst-case guarantees.\n\u2022 Common Approach:\n\u2022 Propose a natural randomized way the input could have been generated.\n\u2022 Analyze how the algorithm performs on inputs generated in this way.\nIs used to justify `2 linear regression, k-means and spectral clustering...\n13\n\nstochastic block model\nStochastic Block Model (Planted Partition Model): Split n nodes\nrandomly into two groups B and C, each with n/2 nodes.\n\u2022 Node pairs in the same group are connected with probability p.\n\u2022 Node pairs in di\u21b5erent groups are connected with probability q < p.\n\u2022 Connections are independent.\nWe\u2019ll see whether spectral clustering can recover A and B from the graph.\n14\n\nfinding eigenvectors\n\npower method\n\u2022 Power Method: The most fundamental iterative method for \ufb01nding\nthe eigenvectors that arise in many of the above applications.\n\u2022 Goal: Given symmetric A 2 Rd\u21e5d \ufb01nd ~z such that\nA~z \u21e1\u03bb~z\nfor some large \u03bb.\n\u2022 Algorithm:\n\u2022 Initialize: Pick random starting vector ~z(0).\n\u2022 Loop: For i = 1, . . . , t\n\u2022 ~z(i) := A \u00b7 ~z(i\u22121)\n\u2022 ~zi :=\n~z(i)\nk~z(i)k2\n\u2022 Return ~zt\nWill show (under mild conditions) that as t ! 1, the output converges\nto the desired output. We will also analyze the rate of convergence.\n16\n\nbasic linear algebra definitions\nAndrew McGregor\n0i\n\nvector definitions\n\u2022 The Euclidean norm of a vector v 2 Rd is de\ufb01ned as\nkvk2 =\nsX\nj\nv 2\nj .\n\u2022 The dot product (or \u201cscalar product\u201d) between u, v 2 Rd is de\ufb01ned as\nhu, vi =\nX\nj2[d]\nujvj\nand note kvk2\n2 = hv, vi.\n\u2022 A set of vectors {v1, v2, v3, . . .} is orthonormal if\nhvi, vji =\n(\n1\nif i = j\n0\nif i 6= j\n1\n\nbasis and dimension\n\u2022 u 2 Rn is in the span of a set of vectors B = {v1, v2, . . . , vk} if u can\nbe written as a linear combination of vectors in B, i.e.,\nu = \u21b51v1 + \u21b52v2 + . . . \u21b5kvk\nfor some \u21b51, . . . , \u21b5k 2 R.\n\u2022 Given two sets of vectors A and B, say B is a basis for A if every u 2 A\nis in the span of B. A \ufb01nite set B could be a basis for an in\ufb01nite set A.\n\u2022 The dimension of A is the size of its smallest basis.\n2\n\nmatrix multiplication\n\u2022 If A 2 R`\u21e5m and B 2 Rm\u21e5n are matrices then the (i, j)-th entry of the\nproduct C = AB 2 R`\u21e5n is\nCi,j =\nX\nh2[m]\nAi,hBh,j .\n\u2022 The rows of C are linear combinations of the rows of B. The columns\nof C are linear combinations of the columns of A.\n3\n\neigenvalues and eigenvectors\n\u2022 Given a square matrix A 2 Rn\u21e5n, we say v 2 Rn is an eigenvector of A\nwith eigenvalue \u03bb 2 R if\nAv = \u03bbv\n\u2022 The trace of a square matrix is the sum of the diagonal entries. It can\nalso be shown that this equals the sum of the eigenvalues of the matrix.\n\u2022 Eigendecomposition: If A is a n \u21e5n symmetric matrix, it has n\northonormal eigenvectors. It can be written as\nA = VDV T\nwhere the columns of V are the orthonormal eigenvectors and D is a\ndiagonal matrix whose ith diagonal entry is the the eigenvalue\ncorresponding to the ith eigenvector.\n4\n\nmatrix transpose and frobenius norm\n\u2022 The transpose of an n \u21e5m matrix M, is the m \u21e5n matrix whose\n(i, j)-th entry is the same as the (j, i)-th entry of M. It is denoted MT.\n\u2022 For matrices A and B,\n(AB)T = BTAT\n\u2022 If the columns of a matrix M are orthonormal then MTM is the identity\nmatrix, i.e., the diagonal matrix where every diagonal entry is 1.\n\u2022 The Frobenius norm kXkF of a matrix X is the square root of the sum\nof its squared entires, i.e.,\nkXkF =\nqX\nX 2\ni,j\n\u2022 kXk2\nF is equal to the trace of X TX which equals the sum of the\neigenvalues of X TX.\n5\n\nlow dimensional data\nAndrew McGregor\n0\n\u21b3\n\nembedding with assumptions\n\u2022 Let V be a k-dimensional subspace of Rd, i.e., the in\ufb01nite set of vectors\nformed by taking linear combinations of some set of k orthonormal vectors.\n\u2022 Assume that data points ~x1, . . . , ~xn 2 V .\nClaim: Let ~v1, . . . , ~vk be an orthonormal basis for V and V 2 Rd\u21e5k be the\nmatrix with these vectors as its columns. For all ~xi, ~xj:\nkVT~xi \u2212VT~xjk2 = k~xi \u2212~xjk2.\nThat is, VT 2 Rk\u21e5d is a linear embedding of ~x1, . . . , ~xn into k dimensions with\nno distortion.\n1\n\ndot product transformation\nWe\u2019ll prove a more general result about any ~y 2 V, not just ~y = ~xi \u2212~xj.\nClaim: Let ~v1, . . . , ~vk be an orthonormal basis for V and V 2 Rd\u21e5k be\nthe matrix with these vectors as its columns. For all ~y 2 V:\nkVT~yk2 = k~yk2.\nProof:\n\u2022 If ~y = P\ni ci~vi then ~y = V~c where ~cT = (c1, . . . , ck)\n\u2022 k~yk2\n2 = ~y T~y = (V~c)T(V~c) = ~cTVTV~c\n\u2022 kVT~yk2\n2 = (VT~y)T(VT~y) = ~y TVVT~y = ~cTVTVVTV~c\n\u2022 But VTV = I since\n[VTV]i,j = ~v T\ni ~vj =\n(\n1\ni = j\n0\ni 6= j\n\u2022 So k~yk2\n2 = ~cT~c = kVT~yk2\n2.\n2\n\nlow-rank factorization\n\u2022 Suppose every data point ~xi (row of data matrix X) can be written as\n~xi = ci,1 \u00b7 ~v1 + . . . + ci,k \u00b7 ~vk\nand let V have columns are ~v1, . . . , ~vk.\n\u2022 X can be represented by (n + d) \u00b7 k parameters vs. n \u00b7 d.\n\u2022 The rows of X are spanned by the k rows of VT and the columns of X\nare spanned by the k columns of C.\n3\n\nlow-rank factorization\nClaim: If ~x1, . . . , ~xn lie in a k-dimensional subspace with orthonormal\nbasis V 2 Rd\u21e5k, the data matrix can be written as X = XVVT.\nProof: Since the columns of V are orthonormal, VTV is the identity\nmatrix and so:\nX = CVT =) XV = CVTV =) XV = C\nand so X = XVVT\n4\n\nprojection matrix\n\u2022 VVT is a projection matrix, which projects the rows of X (the data\npoints ~x1, . . . , ~xn) onto the subspace V.\n\u2022 We just showed that X = XVVT when the rows of X are already in V.\n\u2022 Next we\u2019ll show that XVVT is the \u201cclosest\u201d matrix to X that has the\nproperty that every row is in V.\n5\n\nmapping to a fixed low dimensional space\nAndrew McGregor\n0\n\u21b3\n\nbest approximation of x in a given subspace\n\u2022 Let X be the matrix whose rows correspond to data points ~x1, . . . , ~xn.\n\u2022 Let ~v1, . . . , ~vk be orthonormal basis of k-dimensional subspace V of Rd.\n\u2022 What matrix B is \u201cclosest\u201d to X that has the property that every row\nof B is in V? Speci\ufb01cally,\n\u2022 Want to minimize kB \u2212Xk2\nF where k \u00b7 kF is the Frobenius norm, i.e., the\nsquare root of the sum of squared entries.\n\u2022 Need to every row of B to be a linear combination of ~v1, . . . , ~vk.\nTheorem:\nXVVT =\narg min\nB with rows in V\nkX \u2212Bk2\nF.\nRecall VVT is called the projection matrix.\n1\n\nthree properties of projection matrices\nProperty 1 : Show that VVT is idempotent, i.e. ,\n(VVT)(VVT) = (VVT)\nProof: (VVT)(VVT) = V(VTV)VT = VVT\n2\n\nthree properties of projection matrices\nProperty 2: The projection is orthogonal to its complement: For any\n~y 2 Rd,\nhVVT~y, ~y \u2212VVT~yi = 0\nProof:\nhVVT~y, ~y \u2212VVT~yi\n=\nhVVT~y, ~yi \u2212hVVT~y, VVT~yi\n=\n~y TVVT~y \u2212~y TVVTVVT~y\n=\n0\nsince VVT is idempotent.\n3\n\nthree properties of projection matrices\nProperty 3 (Pythagorean Theorem): For any ~y 2 Rd,\nk~yk2\n2 = k(VVT)~yk2\n2 + k~y \u2212(VVT)~yk2\n2.\nProof: Write ~y = (~y \u2212(VVT)~y) + (VVT)~y and then use\nk~a + ~bk2\n2 = k~ak2\n2 + k~bk2\n2 + 2h~a, ~bi\nand the fact that the projection is orthogonal to its complement\n4\n\nprojection vector is closest point in subspace\nLet V 2 Rn\u21e5k have orthonormal columns and let ~y 2 Rn. Then the\nPythagorean Theorem proves that VVT~y is the closest vector to ~y that\ncan be expressed as a linear combination of the columns of V\n\u2022 Apply Pythagorus to ~y \u2212~z for arbitrary ~z 2 Rn:\nk~y \u2212~zk2\n2 = kVVT(~y \u2212~z)k2\n2 + k~y \u2212~z \u2212VVT(~y \u2212~z)k2\n2\n= kVVT~y \u2212VVT~zk2\n2 + k~y \u2212VVT~y \u2212~z + VVT~zk2\n2.\n\u2022 If ~z = V~c for some ~c 2 Rk, then VVT~z = VVTV~c = V~c = ~z and the\nabove simpli\ufb01es to\nkVVT~y \u2212~zk2\n2 + k~y \u2212VVT~yk2\n2\n\u2022 To minimize this, set ~z = VVT~y.\n5\n\nproof of theorem\nTheorem:\nXVVT =\narg min\nB with rows in V\nkX \u2212Bk2\nF.\nProof:\n\u2022 kX \u2212Bk2\nF = Pn\ni=1 k~xi \u2212~bik2\n2 where ~bi is the ith row of B.\n\u2022 Minimized by setting ~bi to be the closest vector to ~xi that is in V.\n\u2022 From the previous slide, ~bi = ~xiVVT since we\u2019re dealing with row\nvectors rather than column vectors.\n\u2022 So the optimal B is XVVT.\n6\n\neigenvalues and symmetric matrices\nAndrew McGregor\n0\n4\n\neigenvectors and eigendecomposition\n\u2022 Eigenvector: ~x 2 Rd is an eigenvector of a matrix A 2 Rd\u21e5d if\nA~x = \u03bb~x\nfor some scalar \u03bb called the eigenvalue corresponding to ~x. That is, A\njust \u2018stretches\u2019 x.\n\u2022 Spectral Theorem: If A is symmetric, has d orthonormal\neigenvectors.\n\u2022 Eigendecomposition: Let ~v1, . . . , ~vd be the orthonormal eigenvectors.\nLet V 2 Rd\u21e5d be the matrix with these vectors as columns and \u21e4be\nthe diagonal matrix with corresponding eigenvalues on the diagonal.\nAV =\n2\n64\n|\n|\n|\n|\nA~v1\nA~v2\n\u00b7 \u00b7 \u00b7\nA~vd\n|\n|\n|\n|\n3\n75 =\n2\n64\n|\n|\n|\n|\n\u03bb1~v1\n\u03bb2~v2\n\u00b7 \u00b7 \u00b7\n\u03bb~vd\n|\n|\n|\n|\n3\n75 = V\u21e4\nand so AVVT = A = V\u21e4VT where the \ufb01rst inequality follows since\nrows of A are in span of the eigenvectors.\n1\n\neigenvectors and eigendecomposition\nTypically order the eigenvectors in decreasing order:\n\u03bb1 \u2265\u03bb2 \u2265. . . \u2265\u03bbd\n2\n\ncourant-fischer theorem\nCourant-Fischer Theorem: For symmetric A, the eigenvectors are given via\nthe greedy optimization:\n~v1 =\narg max\n~v with kvk2=1\n~v TA~v.\n~v2 =\narg max\n~v with kvk2=1, h~v,~v1i=0\n~v TA~v.\n. . .\n~vd =\narg max\n~v with kvk2=1, h~v,~vj i=0 8j<d\n~v TA~v.\n\u2022 ~v T\nj A~vj = \u03bbj \u00b7 ~v T\nj ~vj = \u03bbj, the jth largest eigenvalue.\n3\n\nbest fit subspace\nAndrew McGregor\n0\n65\n\u2193\n\nbest fit subspace\n\u2022 So far we know, given X whose rows are data points in Rd and\nsubspace V \u21e2Rd\nXVVT =\narg min\nB with rows in V\nkX \u2212Bk2\nF\nwhere V is the matrix whose columns are an orthonormal basis V.\n\u2022 Therefore, kX \u2212XVVTk2\nF is the smallest error achievable when\napproximating X by a matrix with rows in V.\n\u2022 Here we investigate how to choose V (or equivalently V) such that we\nminimize:\nkX \u2212XVVTk2\nF ,\nsubject to the constraint that V is k-dimensional, i.e., V has only k\ncolumns.\n1\n\nbest fit subspace\nWant orthonormal V 2 Rd\u21e5k that minimizes\nkX \u2212XVVTk2\nF\n=\nkXT \u2212VVTXTk2\nF\n=\nn\nX\ni=1\nk~xi \u2212VVT~xik2\n2\n=\nn\nX\ni=1\nk~xik2\n2 \u2212kVVT~xik2\n2\nwhere the last line follows from Pythagoras.\nSo minimizing kX \u2212XVVTk2\nF is the same as maximizing\nX\ni\nkVVT~xik2\n2 =\nX\ni\n~xT\ni VVTVVT~xi =\nX\ni\n~xT\ni VVT~xi =\nX\ni\nkVT~xik2\n2\n2\n\nsolution via eigendecomposition\nV minimizing kX \u2212XVVTk2\nF is given by:\narg max\northonormal V2Rd\u21e5k\nn\nX\ni=1\nkVT~xik2\n2 =\nk\nX\nj=1\nn\nX\ni=1\nh~vj, ~xii2 =\nk\nX\nj=1\nkX~vjk2\n2\nSurprisingly, can \ufb01nd the columns of V, ~v1, . . . , ~vk greedily.\n~v1 =\narg max\n~v with kvk2=1\n~v TXTX~v.\n~v2 =\narg max\n~v with kvk2=1, h~v,~v1i=0\n~v TXTX~v.\n. . .\n~vk =\narg max\n~v with kvk2=1, h~v,~vj i=0 8j<k\n~v TXTX~v.\nBy Courant-Fischer Theorem, these are exactly the top k eigenvectors of XTX!\n3\n\nlow-rank approx via eigendecomposition\nUpshot: Letting Vk have columns ~v1, . . . , ~vk corresponding to the top k\neigenvectors of the covariance matrix XTX, Vk is the orthogonal matrix\nminimizing\nkX \u2212XVkVT\nk k2\nF,\nThis is principal component analysis (PCA).\nNext: How accurate is this low-rank approximation? Can understand via\neigenvalues of XTX.\n4\n\nerror of best fit subspace\nAndrew McGregor\n0\u00b7\n\nsummary so far...\n\u2022 Given X whose rows data points in Rd and subspace V \u21e2Rd\nXVVT =\narg min\nB with rows in V\nkX \u2212Bk2\nF\nwhere V is the matrix whose columns are an orthonormal basis V.\n\u2022 Therefore, kX \u2212XVVTk2\nF is the smallest error achievable when\napproximating X by a matrix with rows spanned by the V.\n\u2022 The orthonormal V 2 Rd\u21e5k that minimizes:\nkX \u2212XVVTk2\nF ,\nis the matrix Vk whose columns are the top k eigenvectors of XTX.\n\u2022 How large is the error kX \u2212XVkVT\nk k2\nF?\n1\n\nmain ingredients for the analysis\n\u2022 By applying the Pythagorus Theorem on each row:\nkXk2\nF = kX \u2212XVkVT\nk k2\nF + kXVkVT\nk k2\nF\n\u2022 Because Vk is orthonormal,\nkXVkVT\nk k2\nF = kXVkk2\nF\n\u2022 For any matrix A,\nkAk2\nF =\nd\nX\ni=1\nk~aik2\n2 = tr(ATA) = sum of diagonal entries = sum eigenvalues.\n2\n\nspectrum analysis\nApproximation error is:\nkX \u2212XVkVT\nk k2\nF = tr(XTX) \u2212tr(VT\nk XTXVk)\n=\nd\nX\ni=1\n\u03bbi \u2212\nk\nX\ni=1\n~v T\ni XTX~vi\n=\nd\nX\ni=1\n\u03bbi \u2212\nk\nX\ni=1\n\u03bbi =\nd\nX\ni=k+1\n\u03bbi\n3\n\nspectrum analysis\nClaim: The error in approximating X with the best rank k approximation\n(projecting onto the top k eigenvectors of XTX) is:\nkX \u2212XVkVT\nk k2\nF =\nd\nX\ni=k+1\n\u03bbi\n4\n\nspectrum analysis\nClaim: The error in approximating X with the best rank k approximation\n(projecting onto the top k eigenvectors of XTX) is:\nkX \u2212XVkVT\nk k2\nF =\nd\nX\ni=k+1\n\u03bbi\n4\n\nspectrum analysis\nPlotting the spectrum of the covariance matrix XTX (its eigenvalues) shows\nhow compressible X is using low-rank approximation (i.e., how close ~x1, . . . , ~xn\nare to a low-dimensional subspace).\n5\n\nspectrum analysis\nPlotting the spectrum of the covariance matrix XTX (its eigenvalues) shows\nhow compressible X is using low-rank approximation (i.e., how close ~x1, . . . , ~xn\nare to a low-dimensional subspace).\n5\n\nspectrum analysis\nPlotting the spectrum of the covariance matrix XTX (its eigenvalues) shows\nhow compressible X is using low-rank approximation (i.e., how close ~x1, . . . , ~xn\nare to a low-dimensional subspace).\n5\n\nspectrum analysis\nNote the eigenvalues of XTX are always positive. Follows since\n\u03bbj = ~v T\nj XTX~vj = kX~vjk2\n2 \u22650 .\n6\n\nsingular value decomposition\nAndrew McGregor\n0\n\u21b3\n\npreviously...\n\u2022 Eigendecomposition: For any symmetric square matrix A, we can write\nA = V\u21e4VT where columns of V are orthonormal eigenvectors and \u21e4is the\ndiagonal matrix of eigenvalues.\n\u2022 The best k-dimensional approximation of a data matrix X 2 Rn\u21e5d is\nXVkVT\nk\nwhere Vk is the matrix whose columns are the top k eigenvectors of XTX.\n\u2022 The error is\nkX \u2212XVkVT\nk k2\nF = \u03bbk+1 + \u03bbk+2 + . . . + \u03bbd\nwhere \u03bbi is the i-th biggest eigenvalue of XTX.\n1\n\nsingular value decomposition\nThe Singular Value Decomposition (SVD) generalizes the eigendecomposition\nto asymmetric (even rectangular) matrices.\nAny matrix X 2 Rn\u21e5d with\nrank(X) = r can be written as X = U\u2303VT.\n\u2022 U has orthonormal columns ~u1, . . . , ~ur 2 Rn (left singular vectors).\n\u2022 V has orthonormal columns ~v1, . . . , ~vr 2 Rd (right singular vectors).\n\u2022 \u2303is diagonal with elements \u03c31 \u2265\u03c32 \u2265. . . \u2265\u03c3r > 0 (singular values).\nThe \u2018swiss army knife\u2019 of modern linear algebra.\n2\n\nconnection of the svd to eigendecomposition\nWriting X 2 Rn\u21e5d in its singular value decomposition X = U\u2303VT:\nXTX = V\u2303UTU\u2303VT = V\u23032VT (the eigendecomposition)\nSimilarly: XXT = U\u2303VTV\u2303UT = U\u23032UT.\nThe right and left singular vectors are the eigenvectors of the covariance matrix\nXTX and the gram matrix XXT respectively.\nSo, letting Vk 2 Rd\u21e5k have columns equal to ~v1, . . . , ~vk, we know that XVkVT\nk\nis the best rank-k approximation to X (given by PCA).\nWhat about UkUT\nk X where Uk 2 Rn\u21e5k has columns equal to ~u1, . . . , ~uk?\nExercise: UkUT\nk X = XVkVT\nk = Uk\u2303kVT\nk\nX 2 Rn\u21e5d: data matrix, U 2 Rn\u21e5rank(X): matrix with orthonormal columns ~u1, ~u2, . . .\n(left singular vectors), V 2 Rd\u21e5rank(X): matrix with orthonormal columns ~v1, ~v2, . . . (right\nsingular vectors), \u23032 Rrank(X)\u21e5rank(X): positive diagonal matrix containing singular values\nof X.\n3\n\nthe svd and optimal low-rank approximation\nThe best low-rank approximation to X, i.e.,\nXk =\narg min\nrank-k B2Rn\u21e5d kX \u2212BkF\nis given by Xk = XVkVT\nk = UkUT\nk X = Uk\u2303kVT\nk\nCorresponds to projecting the rows (data points) onto the span of Vk or\nthe columns (features) onto the span of Uk\n5\n\nthe svd and optimal low-rank approximation\nThe best low-rank approximation to X, i.e.,\nXk =\narg min\nrank-k B2Rn\u21e5d kX \u2212BkF\nis given by Xk = XVkVT\nk = UkUT\nk X = Uk\u2303kVT\nk\nCorresponds to projecting the rows (data points) onto the span of Vk or\nthe columns (features) onto the span of Uk\n5\n\nthe svd and optimal low-rank approximation\nThe best low-rank approximation to X, i.e.,\nXk =\narg min\nrank-k B2Rn\u21e5d kX \u2212BkF\nis given by Xk = XVkVT\nk = UkUT\nk X = Uk\u2303kVT\nk\nCorresponds to projecting the rows (data points) onto the span of Vk or\nthe columns (features) onto the span of Uk\n5\n\nthe svd and optimal low-rank approximation\nThe best low-rank approximation to X, i.e.,\nXk =\narg min\nrank-k B2Rn\u21e5d kX \u2212BkF\nis given by Xk = XVkVT\nk = UkUT\nk X = Uk\u2303kVT\nk\nCorresponds to projecting the rows (data points) onto the span of Vk or\nthe columns (features) onto the span of Uk\n5\n\nbasic idea to prove existence of svd\n\u2022 Let ~v1, ~v2, . . . 2 Rd be orthonormal eigenvectors of XTX.\n\u2022 Let \u03c3i = kX~vik2 and de\ufb01ne unit vector ~ui = X~vi\n\u03c3i .\n\u2022 Exercise: Show ~u1, ~u2, . . . are orthonormal eigenvectors of XXT.\n\u2022 This establishes that XV = U\u2303and that V and U have the required\nproperties.\n\u2022 Full proof isn\u2019t in the scope of COMPSCI 514 but if you\u2019re interested:\nhttps://math.mit.edu/classes/18.095/2016IAP/lec2/SVD_Notes.pdf\n6\n\napplications: matrix completion\nAndrew McGregor\n0\n\u21b3\n\npuzzle\nConsider a matrix X 2 Rn\u21e5d which we cannot fully observe. What are\nthe missing entries?\n2\n6664\n1\n1\n3\n6\n2\n3\n1\n9\n3\n7775\nAn impossible question! But what if we knew the rank was only 1?\n1\n\nmatrix completion\nConsider a matrix X 2 Rn\u21e5d which we cannot fully observe but believe is\nclose to rank-k (i.e., well approximated by a rank k matrix). Classic\nexample: the Net\ufb02ix prize problem.\nSolve: Y = arg min\nrank \u2212k B\nX\nobserved (j,k)\n[Xj,k \u2212Bj,k]2\n2\n\nmatrix completion\nConsider a matrix X 2 Rn\u21e5d which we cannot fully observe but believe is\nclose to rank-k (i.e., well approximated by a rank k matrix). Classic\nexample: the Net\ufb02ix prize problem.\nSolve: Y = arg min\nrank \u2212k B\nX\nobserved (j,k)\n[Xj,k \u2212Bj,k]2\nUnder certain assumptions, can show that Y well approximates X on\nboth the observed and (most importantly) unobserved entries.\n2\n\nidealized approach\n\u2022 Given include matrix X, \ufb01ll in the missing values with the row or\ncolumn average of the non-missing values.\n\u2022 Call the resulting matrix Z.\n\u2022 Let Vk be the matrix formed by the top k eigenvectors of ZTZ\n\u2022 Return ZVkVT\nk\n\u2022 It\u2019s a little more complicated in practice and the above approach\nwouldn\u2019t work well unless there were relatively few missing values.\n3\n\napplications: lsa and word embeddings\nAndrew McGregor\n0\n\u00b7\n\nentity embeddings\nDimensionality reduction embeds d-dimensional vectors into k \u2327d\ndimensions. But what about when you want to embed objects other than\nvectors?\n\u2022 Documents (for topic-based search and classi\ufb01cation)\n\u2022 Words (to identify synonyms, translations, etc.)\n\u2022 Nodes in a social network\nUsual Approach: Convert each item into a high-dimensional feature\nvector and then apply low-rank approximation.\n1\n\nexample: latent semantic analysis\n2\n\nexample: latent semantic analysis\n2\n\nexample: latent semantic analysis\n\u2022 If the error kX \u2212YZTkF is small, then on average,\nXi,a \u21e1(YZT)i,a = h~yi, ~zai.\n\u2022 I.e., h~yi, ~zai \u21e11 when doci contains worda.\n3\n\nexample: latent semantic analysis\nIf doci and docj both contain worda, h~yi, ~zai \u21e1h~yj, ~zai \u21e11 If doci and docj\nboth don\u2019t contain worda, h~yi, ~zai \u21e1h~yj, ~zai \u21e10\nSince this applies for all words, documents with that involve a similar set of\nwords tend to have high dot product with each other.\nAnother View: Column of Y represent \u2018topics\u2019. ~yi(j) indicates how much doci\nbelongs to topic j. ~za(j) indicates how much worda associates with that topic.\n4\n\nexample: latent semantic analysis\n\u2022 Just like with documents, ~za and ~zb will tend to have high dot product if\nworda and wordb appear in many of the same documents.\n\u2022 In an SVD decomposition we set ZT = \u2303kVT\nk where columns of Vk are the\ntop k eigenvectors of XTX.\n5\n\nexample: word embedding\nLSA gives a way of embedding words into k-dimensional space.\n\u2022 Embedding is via low-rank approximation of XTX: where (XTX)a,b is the\nnumber of documents that both worda and wordb appear in.\n\u2022 Think about XTX as a similarity matrix with entry (a, b) being the similarity\nbetween worda and wordb.\n\u2022 Many ways to measure similarity: number of sentences both occur in,\nnumber of times both appear in the same window of w words, in similar\npositions of documents in di\u21b5erent languages, etc.\n\u2022 Replacing XTX with these di\u21b5erent metrics (sometimes appropriately\ntransformed) leads to popular word embedding algorithms: word2vec, GloVe,\nfastText, etc.\n6\n\nexample: word embedding\nNote: word2vec is typically described as a neural-network method, but it\nis really just low-rank approximation of a speci\ufb01c similarity matrix. Neural\nword embedding as implicit matrix factorization, Levy and Goldberg.\n7\n\ngraph embeddings\nAndrew McGregor\n0\u00b7\n\nnon-linear dimensionality reduction\nIs this set of points compressible? Does it lie close to a low-dimensional\nsubspace? (A 1-dimensional subspace of Rd.)\n1\n\nnon-linear dimensionality reduction\nIs this set of points compressible? Does it lie close to a low-dimensional\nsubspace? (A 1-dimensional subspace of Rd.)\n1\n\nnon-linear dimensionality reduction\nIs this set of points compressible? Does it lie close to a low-dimensional\nsubspace? (A 1-dimensional subspace of Rd.)\nA common way of automatically identifying this non-linear structure is to\nconnect data points in a graph. E.g., a k-nearest neighbor graph.\n\u2022 Connect items to similar items, possibly with higher weight edges when they\nare more similar.\n1\n\nlinear algebraic representation of a graph\nOnce we have connected n data points x1, . . . , xn into a graph, we can\nrepresent that graph by its (weighted) adjacency matrix.\nA 2 Rn\u21e5n with Ai,j = edge weight between nodes i and j\n2\n\nadjacency matrix eigenvectors\nHow do we compute an optimal low-rank approximation of A?\n\u2022 Project onto the top k eigenvectors of ATA = A2. Note these are just\nthe eigenvectors of A.\n1. A \u21e1AVkVT\nk where Vk is the matrix with top k eigenvectors as columns.\n2. Rows of AVk are an embedding of the nodes into Rk.\n\u2022 Similar vertices (close with regards to graph proximity) should have\nsimilar embeddings since\nk(A)i \u2212(A)jk2\n\u21e1\nk(AVkVT\nk )i \u2212(AVkVT\nk )jk2\n=\nk(AiVkVT\nk \u2212AjVkVT\nk k2\n=\nkAiVk \u2212AjVkk2\n=\nk(AVk)i \u2212(AVk)jk2\nwhere the second equality follows because for any ~y,\nk~y TVT\nk k2 = ~y TVT\nk Vk~y = k~yk2 .\n3\n\nspectral embedding\nStep 1: Produce a nearest neighbor graph\nbased on your input data in Rd.\nStep 2: Apply low-rank approximation to\nthe graph adjacency matrix to produce\nembeddings in Rk.\nStep 3: Work with the data in the\nembedded space. Where distances\napproximate distances in your original\n\u2018non-linear space.\u2019\n4\n\ngraph laplacian\nAndrew McGregor\n0\nb11\n\u2193\n\nthe laplacian\nFor a graph with adjacency matrix A and degree matrix D, L = D \u2212A is the\ngraph Laplacian.\nLemma: For any vector ~v,\nX\n(i,j)2E\n(~v(i) \u2212~v(j))2 = ~v TL~v.\n1\n\nrewriting laplacian\nLemma:\nX\n(i,j)2E\n(~v(i) \u2212~v(j))2 = ~v TL~v\nProof:\n\u2022 Let Le be the Laplacian for graph just containing edge e.\n\u2022 If e = (i, j), then ~v TLe~v = (v(i) \u2212v(j))2\n\u2022 By linearity,\n~v TL~v =\nX\ne2E\n~v TLe~v\n2\n\nother useful properties\n\u2022 Property 1: All the eigenvalues of the L are non-negative.\n\u2022 Proof: If ~v is a eigenvalue of L with eigenvalue \u03bb then\n0 \uf8ff\nX\n(i,j)2E\n(~v(i) \u2212~v(j))2 = ~v TL~v = \u03bbk~vk2\n2\n\u2022 Property 2: Let ~v 2 {\u22121, 1} and A = {i : ~v(i) = 1}. Then,\n~v TL~v = 4 \u21e5\u201cnumber of edges with exactly one node in A\u201d\n\u2022 Proof:\n~v TL~v =\nX\n(i,j)2E\n(~v(i)\u2212~v(j))2 = 4\u21e5\u201cnumber of edges with exactly one node in A\u201d\n3\n\nother useful properties\n\u2022 Property 3: Suppose the eigenvalues of the adjacency matrix A are\n\u03bb1 \u2265\u03bb2 \u2265. . . \u2265\u03bbn\nand every node has degree d. Then the eigenvalues of L are\n\u00b51 \u2265\u00b52 \u2265. . . \u2265\u00b5n\nwhere \u00b51 = d \u2212\u03bbn, \u00b52 = d \u2212\u03bbn\u22121, . . . , \u00b5n = d \u2212\u03bb1.\n\u2022 Proof: If ~v is the jth eigenvector of A then\nL~v = D~v \u2212A~v = d~v \u2212\u03bbj~v = (d \u2212\u03bbj)~v\ni.e., ~v is also an eigenvector of L and has the eigenvalue d \u2212\u03bbj.\n4\n\nspectral clustering\nAndrew McGregor\n0\nb12\n\u2193\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nCommunity detection in naturally occurring networks.\n1\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nCommunity detection in naturally occurring networks.\n1\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nLinearly separable data.\n1\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nNon-linearly separable data k-nearest neighbor graph.\n1\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nNon-linearly separable data k-nearest neighbor graph.\n1\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nNon-linearly separable data k-nearest neighbor graph.\nCan \ufb01nd this cut using eigendecomposition!\n1\n\ncut minimization\nSimple Idea: Partition clusters along minimum cut in graph.\n2\n\ncut minimization\nSimple Idea: Partition clusters along minimum cut in graph.\nSmall cuts are often not informative.\n2\n\ncut minimization\nSimple Idea: Partition clusters along minimum cut in graph.\nSmall cuts are often not informative.\nSolution: Encourage cuts that separate large sections of the graph.\n2\n\ncut minimization\nSimple Idea: Partition clusters along minimum cut in graph.\nSmall cuts are often not informative.\nSolution: Encourage cuts that separate large sections of the graph.\n\u2022 Let ~v 2 Rn be a cut indicator: ~v(i) = 1 if i 2 S. ~v(i) = \u22121 if i 2 T. Want\n~v to have roughly equal numbers of 1s and \u22121s. I.e., ~v T~1 \u21e10.\n2\n\nthe laplacian view\nFor a graph with adjacency matrix A and degree matrix D, L = D \u2212A is\nthe graph Laplacian.\nFor a cut indicator vector ~v 2 {\u22121, 1}n with ~v(i) = \u22121 for i 2 S and\n~v(i) = 1 for i 2 T:\n1. ~v TL~v = P\n(i,j)2E(~v(i) \u2212~v(j))2 = 4 \u00b7 cut(S, T).\n2. ~v T~1 = |T| \u2212|S|.\nWant to minimize both ~v TL~v (cut size) and ~v T~1 (imbalance).\nThis can be solved by eigendecomposition!\n3\n\nsecond smallest laplacian eigenvector\nBy Courant-Fischer, the second smallest eigenvector is given by:\n~vn\u22121 =\narg min\nv2Rn with k~vk=1, ~v T\nn ~v=0\n~v TL~v\nIf ~vn\u22121 were in\nn\n\u22121\npn,\n1\npn\non\nit would have:\n\u2022 ~v T\nn\u22121L~vn\u22121 = 4\nn \u00b7 cut(S, T) as small as possible given that\n~v T\nn\u22121~vn =\n1\npn~v T\nn\u22121~1 = |T| \u2212|S|\npn\n= 0 .\n\u2022 I.e., ~vn\u22121 would indicate the smallest perfectly balanced cut.\n\u2022 The eigenvector ~vn\u22121 2 Rn is not generally binary, but still satis\ufb01es a\n\u2018relaxed\u2019 version of this property.\n5\n\ncutting with the second laplacian eigenvector\nFind a good partition of the graph by computing\n~vn\u22121 =\narg min\nv2Rdwith k~vk=1, ~v T\nn\u22121~1=0\n~v TL~v\nSet S to be all nodes with ~vn\u22121(i) < 0, T to be all with ~vn\u22121(i) \u22650.\n6\n\nspectral clustering with guarantees\n\u2022 Summary: To partition a graph, \ufb01nd the eigenvector of the Laplacian\nwith the second smallest eigenvalue. Partition nodes based on whether\ncorresponding value in eigenvector is positive/negative.\n~vn\u22121 =\narg min\n~v2Rn,k~vk=1,~v T~1=0\n~v TL~v\n\u2022 We argued this \u201cshould\u201d partition graph along a small cut that\nseparates the graph into large pieces.\n\u2022 Haven\u2019t given formal guarantees. . . but later we\u2019ll consider the\nstochastic block model as a way to justify this approach.\n7\n\nstochastic block model: part 1\nAndrew McGregor\n0\nb13\n\nspectral clustering\n\u2022 To partition a graph, \ufb01nd the eigenvector of the Laplacian with the\nsecond smallest eigenvalue. Partition nodes based on whether\ncorresponding value in eigenvector is positive/negative.\n~vn\u22121 =\narg min\n~v2Rn,k~vk=1,~v T~1=0\n~v TL~v\n\u2022 We argued this \u201cshould\u201d partition graph along a small cut that\nseparates the graph into large pieces.\n\u2022 Haven\u2019t given formal guarantees; it\u2019s di\ufb03cult for general input graphs.\nBut can consider randoms \u201cnatural\u201d graphs. . .\n\u2022 Common Approach: Give a natural generative model for random\ninputs and analyze how the algorithm performs on such inputs. Can be\nused to justify `2 linear regression, k-means clustering, etc.\n1\n\nstochastic block model\nStochastic Block Model (Planted Partition Model): Let Gn(p, q) be a\ndistribution over graphs on n nodes, split randomly into two groups B and C,\neach with n/2 nodes.\n\u2022 Any two nodes in the same group are connected with probability p (including\nself-loops).\n\u2022 Any two nodes in di\u21b5erent groups are connected with prob. q < p.\n\u2022 Connections are independent.\n2\n\nlinear algebraic view\nLet G be a stochastic block model graph drawn from Gn(p, q).\n\u2022 Let A 2 Rn\u21e5n be the adjacency matrix of G.\n\u2022 For sake of analysis assume rows/columns of A are ordered such that\nnodes in B come \ufb01rst. In reality, we the ordering could be arbitrary.\n3\n\nexpected adjacency spectrum\nLetting G be a stochastic block model graph drawn from Gn(p, q) and\nA 2 Rn\u21e5n be its adjacency matrix. (E[A])i,j = p for i, j in same group,\n(E[A])i,j = q otherwise.\nWhat is rank(E[A])? What\nare the eigenvectors and\neigenvalues of E[A]?\n4\n\nexpected adjacency spectrum\nIf we compute ~v2 then we recover the communities B and C!\n\u2022 Can show that for G \u21e0Gn(p, q), A is \u201cclose\u201d to E[A] in some\nappropriate sense (matrix concentration inequality).\n\u2022 Second eigenvector of A is close to [1, 1, 1, . . . , \u22121, \u22121, \u22121] and gives a\ngood estimate of the communities.\nWhen rows/columns aren\u2019t sorted by ID, second eigenvector is e.g.,\n[1, \u22121, 1, \u22121, . . . , 1, 1, \u22121] and entries give community ids.\n5\n\nexpected laplacian spectrum\nLetting G be a stochastic block model graph drawn from Gn(p, q),\nA 2 Rn\u21e5n be its adjacency matrix and L be its Laplacian, what are the\neigenvectors and eigenvalues of E[L]?\nE[L] = E[D] \u2212E[A] =\n\u2713n(p + q)\n2\n\u25c6\nI \u2212E[A]\nand so if E[A]~x = \u03bb~x then\nE[L]~x = (n(p + q)/2 \u2212\u03bb)~x\nTherefore, second eigenvalue of E[A] is second last eigenvector of E[L].\n6\n\nexpected laplacian spectrum\nUpshot: The second smallest eigenvector of E[L] encodes the cut\nbetween the communities.\n\u2022 If the matrices A and L were exactly equal to their expectation,\npartitioning using this eigenvector (i.e., spectral clustering) would\nexactly recover the two communities B and C.\nHow do we show that a matrix is close to its expectation? Matrix\nconcentration inequalities.\n\u2022 Analogous to scalar concentration inequalities like Cherno\u21b5etc.\n\u2022 Random matrix theory is a very recent and cutting edge sub\ufb01eld of\nmathematics that is being actively applied in computer science,\nstatistics, and ML.\n7\n\nstochastic block model: part 2\nAndrew McGregor\n0\nb14 !\n\nrecap\n\u2022 Suppose G is a random graph generated according to the stochastic\nblock model. Let B and C be the groups in the graph. Let A be the\nadjacency matrix.\n\u2022 If ~v be the second largest eigenvector of E[A] then\n~v(i) =\n(\n1\nif i 2 B\n\u22121\nif i 2 C\nThat is, we can work out B and C based on ~v. Note if we normalize\nthe vector, the values are \u00b11/pn.\n1\n\nrecap\n\u2022 Suppose G is a random graph generated according to the stochastic\nblock model. Let B and C be the groups in the graph. Let A be the\nadjacency matrix.\n\u2022 If ~v be the second largest eigenvector of E[A] then\n~v(i) =\n(\n1\nif i 2 B\n\u22121\nif i 2 C\nThat is, we can work out B and C based on ~v. Note if we normalize\nthe vector, the values are \u00b11/pn.\n\u2022 Next we argue that the second largest eigenvector of A is su\ufb03ciently\nsimilar to the second largest eigenvector of E[A] such that we can work\nout B and C approximately the second largest eigenvector of A.\n1\n\neigenvector perturbation\nDavis-Kahan Eigenvector Perturbation Theorem:\nSuppose\nA, A 2 Rd\u21e5d are symmetric with kA \u2212Ak2 \uf8ff\u270fand eigenvec-\ntors v1, v2, . . . , vd and \u00afv1, \u00afv2, . . . , \u00afvd. Letting \u2713(vi, \u00afvi) denote the\nangle between vi and \u00afvi, for all i:\nsin[\u2713(vi, \u00afvi)] \uf8ff\n\u270f\nminj6=i |\u03bbi \u2212\u03bbj|\nwhere \u03bb1, . . . , \u03bbd are the eigenvalues of A.\nThe errors get large if there are eigenvalues with similar magnitudes.\n3\n\napplication to stochastic block model\nClaim 1 (Matrix Concentration): For p \u2265O\n\u21e3\nlog4 n\nn\n\u2318\n,\nkA \u2212E[A]k2 \uf8ffO(ppn).\nClaim 2 (Davis-Kahan): For p \u2265O\n\u21e3\nlog4 n\nn\n\u2318\n,\nsin \u2713(v2, \u00afv2) \uf8ff\nO(ppn)\nminj6=2 |\u03bb2 \u2212\u03bbj| \uf8ff\nO(ppn)\n(p \u2212q)n/2 = O\n\u2713\npp\n(p \u2212q)pn\n\u25c6\nRecall: E[A] has eigenvalues \u03bb1 = (p+q)n\n2\n, \u03bb2 = (p\u2212q)n\n2\n, \u03bbi = 0 for i \u22653.\nmin\nj6=2 |\u03bb2 \u2212\u03bbj| = min\n\u2713\nqn, (p \u2212q)n\n2\n\u25c6\n.\nTypically, (p\u2212q)n\n2\nwill be the minimum of these two gaps.\nA adjacency matrix of random stochastic block model graph. p: connection probability\nwithin clusters. q < p: connection probability between clusters. n: number of nodes.\nv2, \u00afv2: second eigenvectors of A and E[A] respectively.\n4\n\napplication to stochastic block model\nSo Far: sin \u2713(v2, \u00afv2) \uf8ffO\n\u21e3\npp\n(p\u2212q)pn\n\u2318\n. What does this give us?\n\u2022 Can show that this implies kv2 \u2212\u00afv2k2\n2 \uf8ffO\n\u21e3\np\n(p\u2212q)2n\n\u2318\n(exercise).\n\u2022 \u00afv2 is\n1\npn\u03c7B,C: the community indicator vector.\n\u2022 Every i where v2(i), \u00afv2(i) di\u21b5er in sign contributes \u22651\nn to kv2 \u2212\u00afv2k2\n2.\n\u2022 So they di\u21b5er in sign in at most O\n\u21e3\np\n(p\u2212q)2\n\u2318\npositions.\nA adjacency matrix of random stochastic block model graph. p: connection probability\nwithin clusters. q < p: connection probability between clusters. n: number of nodes.\nv2, \u00afv2: second eigenvectors of A and E[A] respectively.\n5\n\napplication to stochastic block model\nUpshot: If G is a stochastic block model graph with adjacency matrix A,\nif we compute its second large eigenvector v2 and assign nodes to\ncommunities according to the sign pattern of this vector, we will correctly\nassign all but O\n\u21e3\np\n(p\u2212q)2\n\u2318\nnodes.\n6\n\npower method for finding eigenvectors\nAndrew McGregor\n0\nb15]\n\npower method\nPower Method: The most fundamental iterative method for\napproximate SVD/eigendecomposition. Applies to computing k = 1\neigenvectors, but can be generalized to larger k.\nGoal: Given symmetric A 2 Rd\u21e5d, an approximation to the top\neigenvector ~v1 of A.\n\u2022 Initialize ~z(0) randomly, e.g. ~z(0)(i) \u21e0N(0, 1).\n\u2022 For i = 1, . . . , t\n\u2022 ~z(i) := A \u00b7 ~z(i\u22121)\n\u2022 ~zi :=\n~z(i)\nk~z(i)k2\n\u2022 Return ~zt\nNote that mathematically it doesn\u2019t matter if we normalize in each\niteration of only normalize at the end.\n3\n\npower method analysis\n\u2022 Write ~z(0) in A\u2019s eigenvector basis:\n~z(0) = c1~v1 + c2~v2 + . . . + cd~vd.\n\u2022 Applying A gives:\nA~z(0) = c1A~v1 + c2A~v2 + . . . + cdA~vd\n= c1\u03bb1~v1 + c2\u03bb2~v2 + . . . + cd\u03bbd~vd\n\u2022 Applying A again gives:\nAA~z(0) = c1\u03bb1A~v1 + c2\u03bb2A~v2 + . . . + cd\u03bbdA~vd\n= c1\u03bb2\n1~v1 + c2\u03bb2\n2~v2 + . . . + cd\u03bb2\nd~vd\n\u2022 Applying A t times gives:\nAt~z(0) = c1\u03bbt\n1~v1 + c2\u03bbt\n2~v2 + . . . + cd\u03bbt\nd~vd\n4\n\npower method convergence\nAfter t iterations, we have \u2018powered\u2019 up the eigenvalues, making the\ncomponent in the direction of v1 much larger, relative to the other components.\n~z(0) = c1~v1 + c2~v2 + . . . + cd~vd =) At~z(0) = c1\u03bbt\n1~v1 + c2\u03bbt\n2~v2 + . . . + cd\u03bbt\nd~vd\nIteration 0\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nc8\nc9\nc10\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n5\n\npower method convergence\nAfter t iterations, we have \u2018powered\u2019 up the eigenvalues, making the\ncomponent in the direction of v1 much larger, relative to the other components.\n~z(0) = c1~v1 + c2~v2 + . . . + cd~vd =) At~z(0) = c1\u03bbt\n1~v1 + c2\u03bbt\n2~v2 + . . . + cd\u03bbt\nd~vd\nIteration 1\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nc8\nc9\nc10\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n5\n\npower method convergence\nAfter t iterations, we have \u2018powered\u2019 up the eigenvalues, making the\ncomponent in the direction of v1 much larger, relative to the other components.\n~z(0) = c1~v1 + c2~v2 + . . . + cd~vd =) At~z(0) = c1\u03bbt\n1~v1 + c2\u03bbt\n2~v2 + . . . + cd\u03bbt\nd~vd\nIteration 13\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nc8\nc9\nc10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nWhen will convergence be slow?\n5\n\npower method slow convergence\nSlow Case: A has eigenvalues: \u03bb1 = 1, \u03bb2 = .99, \u03bb3 = .9, \u03bb4 = .8, . . .\n~z(0) = c1~v1 + c2~v2 + . . . + cd~vd =) ~z(t) = c1\u03bbt\n1~v1 + c2\u03bbt\n2~v2 + . . . + cd\u03bbt\nd~vd\nIteration 2\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nc8\nc9\nc10\n-0.2\n-0.1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n6\n\npower method slow convergence\nSlow Case: A has eigenvalues: \u03bb1 = 1, \u03bb2 = .99, \u03bb3 = .9, \u03bb4 = .8, . . .\n~z(0) = c1~v1 + c2~v2 + . . . + cd~vd =) ~z(t) = c1\u03bbt\n1~v1 + c2\u03bbt\n2~v2 + . . . + cd\u03bbt\nd~vd\nIteration 13\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nc8\nc9\nc10\n-0.1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n6\n\npower method convergence rate\n~z(0) = c1~v1 + c2~v2 + . . . + cd~vd =) ~z(t) = c1\u03bbt\n1~v1 + c2\u03bbt\n2~v2 + . . . + cd\u03bbt\n2~vd\nWrite |\u03bb2| = (1 \u2212\u03b3)|\u03bb1| for \u2018gap\u2019 \u03b3 = |\u03bb1|\u2212|\u03bb2|\n|\u03bb1|\n.\nHow many iterations t does it take to have |\u03bb2|t \uf8ff\u03b4 \u00b7 |\u03bb1|t?\nln(1/\u03b4)\n\u03b3\n.\nWill have for all i > 1, |\u03bbi|t \uf8ff|\u03bb2|t \uf8ff\u03b4 \u00b7 |\u03bb1|t.\nHow small must we set \u03b4 to ensure that c1\u03bbt\n1 dominates all other components\nand so ~z(t) is very close to ~v1?\nA 2 Rd\u21e5d: input matrix with eigendecomposition A = V\u21e4VT . ~v1: top eigenvector, being\ncomputed, ~z(i): iterate at step i, converging to ~v1.\n7\n\nrandom initialization\nClaim: When z(0) is chosen with random Gaussian entries, writing\nz(0) = c1~v1 + c2~v2 + . . . + cd~vd, with high probability, for all i:\nO(1/d2) \uf8ff|ci| \uf8ffO(log d)\nCorollary:\nmax\nj\n!!!!\ncj\nc1\n!!!! \uf8ffO(d2 log d).\nA 2 Rd\u21e5d: input matrix with eigendecomposition A = V\u21e4VT . ~v1: top eigenvector, being\ncomputed, ~z(i): iterate at step i, converging to ~v1.\n8\n\ntechnical preliminaries\n\u2022 Claim: For 0 < c < kxk2:\n!!!!\nx\nkxk2\n\u2212\ny\nkyk2\n!!!!\n2\n\uf8ff\n!!!!\nx\nc \u2212\ny\nkyk2\n!!!!\n2\n\u2022 Proof by geometry: Try drawing a picture.\n\u2022 Claim: For any vector z 2 Rd,\nkzk2 \uf8ffkzk1 := |z(1)| + |z(2)| + . . . + |z(d)|\n\u2022 Proof follows from kzk2\n1 = (|z(1)| + . . . + |z(d)|)2 \u2265kzk2\n2\n9\n\nrandom initialization\nClaim 1: If z(0) is chosen with random Gaussian entries, writing\nz(0) = c1~v1 + . . . + cd~vd, with high probability, maxj\n!!!\ncj\nc1\n!!! \uf8ffO(d2 log d).\nClaim 2: For gap \u03b3 = |\u03bb1|\u2212|\u03bb2|\n|\u03bb1|\n, and t = ln(1/\u03b4)\n\u03b3\n,\n!!!\n\u03bbt\ni\n\u03bbt\n1\n!!! \uf8ff\u03b4 for all i.\n~z(t) :=\nc1\u03bbt\n1~v1 + . . . + cd\u03bbt\nd~vd\nkc1\u03bbt\n1~v1 + . . . + cd\u03bbt\nd~vdk2 =)\nk~z(t) \u2212~v1k2 \uf8ff\n\"\"\"\"\nc1\u03bbt\n1~v1 + . . . + cd\u03bbt\nd~vd\nkc1\u03bbt\n1~v1k2\n\u2212~v1\n\"\"\"\"\n2\n=\n\"\"\"\"\nc2\u03bbt\n2\nc1\u03bbt\n1\n~v2 + . . . + cd\u03bbt\nd\nc1\u03bbt\n1\n~vd\n\"\"\"\"\n2\n\uf8ff\n!!!!\nc2\u03bbt\n2\nc1\u03bbt\n1\n!!!! + . . . +\n!!!!\ncd\u03bbt\nd\nc1\u03bbt\n1\n!!!! \uf8ff\u03b4 \u00b7 O(d2 log d) \u00b7 d\nSetting \u03b4 = O\n\u21e3\n\u270f\nd3 log d\n\u2318\ngives k~z(t) \u2212~v1k2 \uf8ff\u270f.\nA 2 Rd\u21e5d: input matrix with eigendecomposition A = V\u21e4VT . ~v1: top eigenvector, being\ncomputed, ~z(i): iterate at step i, converging to ~v1.\n10\n\npower method theorem\nTheorem (Basic Power Method Convergence)\nLet \u03b3 = |\u03bb1|\u2212|\u03bb2|\n|\u03bb1|\nbe the relative gap between the \ufb01rst and second eigenvalues.\nIf Power Method is initialized with a random Gaussian vector ~v (0) then, with\nhigh probability, after t = O\n\u21e3\nln(d/\u270f)\n\u03b3\n\u2318\nsteps:\nk~z(t) \u2212~v1k2 \uf8ff\u270f.\nTotal runtime: O(t) matrix-vector multiplications. If A = XTX:\nO\n\u2713\nnnz(X) \u00b7 ln(d/\u270f)\n\u03b3\n\u00b7\n\u25c6\n= O\n\u2713\nnd \u00b7 ln(d/\u270f)\n\u03b3\n\u25c6\n.\nwhere nnz() is the number of non-zero entries in the matrix.\n11\n\nfinding second (etc.) eigenvector\n\u2022 If A has eigenvectors v1, . . . , vn with eigenvalues \u03bb1, . . . , \u03bbn\n(|\u03bb1| \u2265. . . \u2265|\u03bbn|) then\nB = A \u2212\u03bb1v1v T\n1\nhas eigenvectors v2, . . . , vn, v1 with eigenvectors \u03bb2, . . . , \u03bbn, 0\n\u2022 Hence, to \ufb01nd the second eigenvector of A, just apply the previous\nmethod to B.\n12\n\npower method vs. random walks\nAndrew McGregor\n0\nD16\n\nconnection to random walks\nConsider a random walk on a graph G with adjacency matrix A.\n1\n\nconnection to random walks\nLet ~p(t) 2 Rn have ith entry ~p(t)\ni\n= Pr(walk at node i at step t).\n\u2022 Initialize: ~p(0) = [1, 0, 0, . . . , 0].\n\u2022 Update:\nPr(walk at i at step t) =\nX\nj2neigh(i)\nPr(walk at j at step t \u22121) \u00b7\n1\ndegree(j)\n= ~zT ~p(t\u22121)\nwhere ~z(j) =\n1\ndegree(j) for all j 2 neigh(i), ~z(j) = 0 for all j /2 neigh(i).\n\u2022 ~z is the ith row of the right normalized adjacency matrix AD\u22121.\n\u2022 ~p(t) = AD\u22121~p(t\u22121) = AD\u22121AD\u22121 . . . AD\u22121\n|\n{z\n}\nt times\n~p(0)\n2\n\nrandom walking as power method\nClaim: After t steps, the probability that a random walk is at node i is given\nby the ith entry of\n~p(t) = AD\u22121AD\u22121 . . . AD\u22121\n|\n{z\n}\nt times\n~p(0).\nD\u22121/2~p(t) = (D\u22121/2AD\u22121/2)(D\u22121/2AD\u22121/2) . . . (D\u22121/2AD\u22121/2)\n|\n{z\n}\nt times\n(D\u22121/2~p(0)).\n\u2022 D\u22121/2~p(t) is exactly what would obtained by applying t iterations of power\nmethod to D\u22121/2~p(0)!\n\u2022 Will converge to the top eigenvector of the normalized adjacency matrix\nD\u22121/2AD\u22121/2. Stationary distribution.\n\u2022 Like the power method, the time a random walk takes to converge to its\nstationary distribution (mixing time) is dependent on the gap between the\ntop two eigenvalues of D\u22121/2AD\u22121/2. The spectral gap.\n3\n\nsection 3: optimization overview\nAndrew McGregor\n0\n001\n\nsummary of part iii: optimization\n\u2022 In this last section, we will investigate general iterative algorithms for\noptimization, speci\ufb01cally gradient descent and its variants.\n\u2022 What are these methods, when are they applied, and how do you analyze\ntheir performance?\n\u2022 Small taste of what you can \ufb01nd in COMPSCI 651: Optimization in\nComputer Science.\n1\n\nmathematical setup\nGiven some function f : Rd ! R, \ufb01nd ~\u2713? with:\nf (~\u2713?) = min\n~\u27132Rd f (~\u2713) + \u270f\nup to some small additive approximation term \u270f.\nOften under some constraints, e.g.,\nk~\u2713k2 \uf8ff1 , k~\u2713k1 \uf8ff1 , A~\u2713\uf8ff~b , ~\u2713TA~\u2713\u22650 , or\nd\nX\ni=1\n~\u2713(i) \uf8ffc .\n2\n\ncontinuous optimization examples\nThe methods we consider work for the functions on the left (these are\n\u201cconvex\u201d) but not necessarily for those with multiple local minimum.\n3\n\nwhy continuous optimization?\nModern machine learning centers around continuous optimization.\nTypical Set Up: (supervised machine learning)\n\u2022 Have a model, which is a function mapping inputs to predictions (neural\nnetwork, linear function, low-degree polynomial etc).\n\u2022 The model is parameterized by a parameter vector (weights in a neural\nnetwork, coe\ufb03cients in a linear function or polynomial)\n\u2022 Want to train this model on input data, by picking a parameter vector such\nthat the model does a good job mapping inputs to predictions on your\ntraining data.\nThis training step is typically formulated as a continuous optimization problem.\n5\n\noptimization in ml\nExample 1: Linear Regression, e.g., predicting house prices based on d\nfeatures (sq. footage, average price of houses in neighborhood. . . )\nModel: M~\u2713: Rd ! R with M~\u2713(~x)\ndef\n= ~\u2713(1) \u00b7 ~x(1) + . . . + ~\u2713(d) \u00b7 ~x(d).\nParameter Vector: ~\u27132 Rd (the regression coe\ufb03cients)\nOptimization Problem: Given data points (training points) ~x1, . . . , ~xn (the\nrows of data matrix X 2 Rn\u21e5d) and labels y1, . . . , yn 2 R, \ufb01nd ~\u2713\u21e4minimizing\nthe loss function:\nLX,y(~\u2713) =\nn\nX\ni=1\n`(M~\u2713(~xi), yi)\nwhere ` is some measurement of how far M~\u2713(~xi) is from yi.\n\u2022 Least Squares Regression: `(M~\u2713(~xi), yi) =\n\"\nM~\u2713(~xi) \u2212yi\n#2\n\u2022 Logistic Regression: yi 2 {\u22121, 1} and\n`(M~\u2713(~xi), yi) = ln\n\"\n1 + exp(\u2212yiM~\u2713(~xi))\n#\n6\n\noptimization in ml\nExample 2: Neural Networks\nModel: M~\u2713: Rd ! R. M~\u2713(~x) = h~wout, \u03c3(W2\u03c3(W1~x))i.\nParameter Vector: ~\u27132 R(# edges) (the weights on every edge)\nOptimization Problem: Given data points ~x1, . . . , ~xn and labels z1, . . . , zn 2 R,\n\ufb01nd ~\u2713\u21e4minimizing the loss function:\nLX,~y(~\u2713) =\nn\nX\ni=1\n`(M~\u2713(~xi), zi)\n7\n\noptimization in ml\nLX,~y(~\u2713) =\nn\nX\ni=1\n`(M~\u2713(~xi), yi)\n\u2022 Supervised means we have labels y1, . . . , yn for the training points.\n\u2022 Solving the \ufb01nal optimization problem has many di\u21b5erent names: likelihood\nmaximization, empirical risk minimization, minimizing training loss, etc.\n\u2022 Continuous optimization is also very common in unsupervised learning.\n(PCA, spectral clustering, etc.)\n\u2022 Generalization tries to explain why minimizing the loss LX,~y(~\u2713) on the\ntraining points minimizes the loss on future test points. I.e., makes us have\ngood predictions on future inputs.\n8\n\noptimization algorithms\nChoice of optimization algorithm for minimizing f (~\u2713) will depend on:\n\u2022 The form of f (in ML, depends on the model & loss function).\n\u2022 Any constraints on ~\u2713(e.g., k~\u2713k < c).\n\u2022 Computational constraints, such as memory constraints.\n9\n\noptimization algorithms\nChoice of optimization algorithm for minimizing f (~\u2713) will depend on:\n\u2022 The form of f (in ML, depends on the model & loss function).\n\u2022 Any constraints on ~\u2713(e.g., k~\u2713k < c).\n\u2022 Computational constraints, such as memory constraints.\nIn this section, we will introduce the gradient descent algorithm (with and\nwithout constraints), the stochastic gradient descent algorithm, and the online\ngradient descent algorithm.\n9\n\ngradient descent in one dimension\nAndrew McGregor\n0\n27\n!\n\ngradient descent\nGradient descent (and some important variants)\n\u2022 An extremely simple greedy iterative method, that can be applied to almost\nany continuous function we care about optimizing.\n\u2022 Often not the \u2018best\u2019 choice for a given function, but it is the approach of\nchoice in ML since it is simple, general, and often works very well.\n\u2022 At each step, tries to move towards the lowest nearby point in the function\nthat is can \u2013 in the opposite direction of the gradient.\n1\n\nconvexity in 1d\nDe\ufb01nition: A function f : R ! R is convex i\u21b5, for any \u27131, \u27132 2 R:\nf (\u27132) \u2265f (\u27131) + f 0(\u27131)(\u27132 \u2212\u27131)\n\u2022 Let \u2713\u21e4= arg min\u2713f (\u2713). Then f 0(\u2713) < 0 when \u2713< \u2713\u21e4and f 0(\u2713) \u22650 when\n\u2713> \u2713\u21e4. Furthermore, |f 0(\u2713)| gets smaller as \u2713gets closer to \u2713\u21e4.\n\u2022 Exercise: If f is convex, the for all \u27131, \u27132 2 R\nf (\u27132) \u2212f (\u27131) \uf8fff 0(\u27132)(\u27132 \u2212\u27131)\nand\nf (\u27131/2 + \u27132/2) \uf8fff (\u27131)/2 + f (\u2713)2/2 .\n2\n\nbasic idea of gradient descent\nGradient Descent Update in 1D:\n\u2022 Set \u27131 arbitrarily.\n\u2022 For i = 1 to t \u22121:\n\u2713i+1 = \u2713i \u2212\u2318f 0(\u2713i)\ni.e., increase \u2713if negative derivative and decrease \u2713if positive\nderivative. \u2318is small \ufb01xed value.\n\u2022 Return \u2713= arg min\u27131,...\u2713t f (\u2713i).\nExample: f (x) = (x \u22121)2, \u27131 = 2, and \u2318= 0.2\n\u2022 Compute derivative f 0(x) = 2(x \u22121)\n\u2022 \u27132 = \u27131 \u2212\u2318f 0(\u27131) = 2 \u22120.2 \u21e5f 0(2) = 2 \u22120.2 \u21e52 = 1.6.\n\u2022 \u27133 = \u27132 \u2212\u2318f 0(\u27132) = 1.6 \u22120.2 \u21e5f 0(1.6) = 1.6 \u22120.2 \u21e51.2 = 1.36.\n3\n\ngd analysis proof for d = 1\nTheorem: For convex function f : R ! R where |f 0(\u2713)| \uf8ffG for all \u2713,\nGD run with t \u2265R2G2\n\u270f2\niterations, \u2318=\nR\nGpt , and starting point within R\nof \u2713\u21e4, outputs \u02c6\u2713satisfying f (\u02c6\u2713) \uf8fff (\u2713\u21e4) + \u270f.\n\u2022 Substituting \u2713i+1 = \u2713i \u2212\u2318f 0(\u2713i) and letting ai = \u2713i \u2212\u2713\u21e4gives:\na2\ni+1 = (\u2713i+1 \u2212\u2713\u21e4)2 = (ai \u2212\u2318f 0(\u2713i))2 = a2\ni \u22122\u2318f 0(\u2713i)ai + (\u2318f 0(\u2713i))2\n\u2022 Rearrange and use convexity to show:\nf (\u2713i) \u2212f (\u2713\u21e4) \uf8fff 0(\u2713i)ai =\n1\n2\u2318\n!\na2\ni \u2212a2\ni+1\n\"\n+ \u2318(f 0(\u2713i))2/2\n\u2022 Summing over i and using the fact |f 0(\u2713i)| \uf8ffG,\n1\nt\nPt\ni=1 (f (\u2713i) \u2212f (\u2713\u21e4)) \uf8ff\n\u21e3\n1\n2t\u2318\nPt\ni=1(a2\ni \u2212a2\ni+1)\n\u2318\n+ \u2318G2\n2\n\uf8ff\na2\n1\n2t\u2318+ \u2318G2\n2\n\u2022 Using a2\n1 \uf8ffR2 and f (\u02c6\u2713) \u2212f (\u2713\u21e4) \uf8ff1\nt\nPt\ni=1 (f (\u2713i) \u2212f (\u2713\u21e4))\nf (\u02c6\u2713) \uf8fff (\u2713\u21e4) + R2\n2t\u2318+ \u2318G2\n2\n\uf8fff (\u2713\u21e4) + \u270f\n4\n\nmultivariate convexity and calculus\nAndrew McGregor\n0\nI\n\nmultivariate calculus review\nLet ~ei 2 Rd denote the ith standard basis vector,\n~ei = [0, 0, 1, 0, 0, . . . , 0]\n|\n{z\n}\n1 at position i\n.\nPartial Derivative: For f : Rd ! R,\n@f\n@~\u2713(i)\n= lim\n\u270f!0\nf (~\u2713+ \u270f\u00b7 ~ei) \u2212f (~\u2713)\n\u270f\n.\nGradient: A vector of the partial derivatives.\n~rf (~\u2713) =\n2\n666664\n@f\n@~\u2713(1)\n@f\n@~\u2713(2)\n...\n@f\n@~\u2713(d)\n3\n777775\n1\n\nmultivariate calculus review\nDirectional Derivative: For vector ~v,\nD~v f (~\u2713) = lim\n\u270f!0\nf (~\u2713+ \u270f~v) \u2212f (~\u2713)\n\u270f\n.\nDirectional Derivative in Terms of the Gradient:\nD~v f (~\u2713) = h~v, ~rf (~\u2713)i.\n2\n\nexample of gradients\n\u2022 Suppose f : R3 ! R where f (~\u2713) = \u27133\n1 + \u27132\u27133 + \u27132\n3 then\nrf (~\u2713) =\n0\nB\n@\n3\u27132\n1\n\u27133\n\u27132 + 2\u27133\n1\nC\nA\nand\nkrf (~\u2713)k2 =\nq\n(3\u27132\n1)2 + (\u27133)2 + (\u27132 + 2\u27133)2\n\u2022 Suppose f : R3 ! R where f (~\u2713) = 3\u27131 + \u27132 + 5\u27133 then\nrf (~\u2713) =\n0\nB\n@\n3\n1\n5\n1\nC\nA\nand krf (~\u2713)k2 =\np\n32 + 12 + 52.\n3\n\nconvexity\nConvex Function: A function f : Rd ! R is convex i\u21b5, for any ~\u27131, ~\u27132 2\nRd and \u03bb 2 [0, 1]:\n(1 \u2212\u03bb) \u00b7 f (~\u27131) + \u03bb \u00b7 f (~\u27132) \u2265f\n\u21e3\n(1 \u2212\u03bb) \u00b7 ~\u27131 + \u03bb \u00b7 ~\u27132\n\u2318\n4\n\nconvexity\nCorollary: A function f : R ! R is convex i\u21b5, for any \u27131, \u27132 2 R:\nf (\u27132) \u2265f (\u27131) + f 0(\u27131)(\u27132 \u2212\u27131)\nMore generally, a function f : Rd ! R is convex if and only if, for any\n~\u27131, ~\u27132 2 Rd: f (~\u27132) \u2212f (~\u27131) \u2265~rf (~\u27131)T \u21e3\n~\u27132 \u2212~\u27131\n\u2318\n5\n\nequivalent definitions for convexity\nWe de\ufb01ned convexity of f : Rd ! R in two ways:\n(1) For all x, y 2 Rd, \u03bb 2 [0, 1], \u03bbf (x) + (1 \u2212\u03bb)f (y) \u2265f (\u03bbx + (1 \u2212\u03bb)y).\n(2) For all x, y 2 Rd, f (x) \uf8fff (y) + hrf (x), x \u2212yi\nTo see (1) implies (2)\nhrf (x), y \u2212xi = Dy\u2212xf (x) = lim\n\u270f!0\nf (x + \u270f(y \u2212x)) \u2212f (x)\n\u270f\n= lim\n\u270f!0\nf ((1 \u2212\u270f)x + \u270fy) \u2212f (x)\n\u270f\n\uf8fflim\n\u270f!0\n(1 \u2212\u270f)f (x) + \u270ff (y) \u2212f (x)\n\u270f\n=f (y) \u2212f (x)\n6\n\nequivalent definitions for convexity\nWe de\ufb01ned convexity of f : Rd ! R in two ways:\n(1) For all x, y 2 Rd, \u03bb 2 [0, 1], \u03bbf (x) + (1 \u2212\u03bb)f (y) \u2265f (\u03bbx + (1 \u2212\u03bb)y).\n(2) For all x, y 2 Rd, f (x) \uf8fff (y) + hrf (x), x \u2212yi\nTo see (2) implies (1)\nf (\u03bbx + (1 \u2212\u03bb)y) \uf8fff (x) + hrf (\u03bbx + (1 \u2212\u03bb)y), \u03bbx + (1 \u2212\u03bb)y \u2212xi\nf (\u03bbx + (1 \u2212\u03bb)y) \uf8fff (y) + hrf (\u03bbx + (1 \u2212\u03bb)y), \u03bbx + (1 \u2212\u03bb)y \u2212yi\n\u03bb times the \ufb01rst equation plus (1 \u2212\u03bb) times the second equation gives\nf (\u03bbx + (1 \u2212\u03bb)y) \uf8ff\u03bbf (x) + (1 \u2212\u03bb)f (y)\n7\n\nhigher dimensional gradient descent\nAndrew McGregor\n0\n23/\n\nrecap: gradient descent in 1d\nLet f : R ! R be a convex function with \u2713\u21e4= arg min\u2713f (\u2713).\nGradient Descent Update in 1D:\n\u2022 Set \u27131 arbitrarily.\n\u2022 For i = 1 to t \u22121:\n\u2713i+1 = \u2713i \u2212\u2318f 0(\u2713i)\ni.e., increase \u2713if negative derivative and decrease \u2713if positive\nderivative. \u2318is small \ufb01xed value.\n\u2022 Return \u2713= arg min\u27131,...\u2713t f (\u2713i).\nWe proved that if |f 0(\u2713)| \uf8ffG for all \u2713, t \u2265R2G 2\n\u270f2 , \u2318=\nR\nGpt , and\n|\u27131 \u2212\u2713\u21e4| \uf8ffR then the output satis\ufb01es \u02c6\u2713satisfying f (\u02c6\u2713) \uf8fff (\u2713\u21e4) + \u270f.\n1\n\ngradient descent greedy approach\nGradient descent is a greedy iterative optimization algorithm: Starting at ~\u2713(1),\nin each iteration let\n~\u2713(i) = ~\u2713(i\u22121) + \u2318~v\nwhere \u2318is the \u2018step size\u2019 and ~v is a direction chosen to make f (~\u2713(i\u22121) + \u2318~v)\nsmall. How should we choose ~v?\nD~v f (~\u2713(i\u22121)) = lim\n\u270f!0\nf (~\u2713(i\u22121) + \u270f~v) \u2212f (~\u2713(i\u22121))\n\u270f\n.\nSo for small \u2318:\nf (~\u2713(i)) \u2212f (~\u2713(i\u22121)) = f (~\u2713(i\u22121) + \u2318~v) \u2212f (~\u2713(i\u22121)) \u21e1\u2318\u00b7 D~vf (~\u2713(i\u22121))\n= \u2318\u00b7 h~v, ~rf (~\u2713(i\u22121))i.\nWe want to minimize h~v, ~rf (~\u2713(i\u22121))i, so choose ~v pointing in the opposite\ndirection of ~rf (~\u2713(i\u22121)), i.e., \u2212rf (~\u2713(i\u22121)).\n2\n\nGradient Descent analysis for convex, Lipschitz functions.\n4\n\nlipschitz functions\nGradient Descent Update:\n~\u2713i+1 = ~\u2713i \u2212\u2318rf (~\u2713i)\nFor fast convergence, need to assume that the function is Lipschitz, i.e.,\nsize of gradient k~rf (~\u2713)k2 is bounded. We\u2019ll assume\n8~\u27131, ~\u27132 :\n|f (~\u27131) \u2212f (~\u27132)| \uf8ffG \u00b7 k~\u27131 \u2212~\u27132k2\n5\n\ngd analysis \u2013 convex functions\nAssume that:\n\u2022 f is convex.\n\u2022 f is G Lipschitz, i.e., k~rf (~\u2713)k2 \uf8ffG for all ~\u2713.\n\u2022 k~\u27131 \u2212~\u2713\u21e4k2 \uf8ffR where ~\u27131 is the initialization point.\nGradient Descent\n\u2022 Choose some initialization ~\u27131 and set \u2318=\nR\nGpt .\n\u2022 For i = 1, . . . , t \u22121\n\u2022 ~\u2713i+1 = ~\u2713i \u2212\u2318rf (~\u2713i)\n\u2022 Return \u02c6\u2713= arg min~\u27131,...~\u2713t f (~\u2713i).\n6\n\ngd analysis proof\nTheorem: For convex G-Lipschitz function f : Rd ! R, GD run with\nt \u2265R2G2\n\u270f2\niterations, \u2318=\nR\nGpt , and starting point within radius R of ~\u2713\u21e4,\noutputs \u02c6\u2713satisfying f (\u02c6\u2713) \uf8fff (~\u2713\u21e4) + \u270f.\n\u2022 Step 1: ~rf (~\u2713i)T~ai \uf8ff\nk~ai k2\n2\u2212k~ai+1k2\n2\n2\u2318\n+ \u2318G2\n2\nwhere ~ai = ~\u2713i \u2212~\u2713\u21e4.\nProof:\nk~ai+1k2\n2\n=\nk~ai \u2212\u2318~rf (~\u2713i)k2\n2\n=\nk~aik2\n2 \u22122\u2318~rf (~\u2713i)T~ai + k\u2318~rf (\u2713i)k2\n2\n\uf8ff\nk~aik2\n2 \u22122\u2318~rf (~\u2713i)T~ai + \u23182G 2\nusingka \u2212bk2\n2 = kak2\n2 \u22122aTb + kbk2\n2. Then rearrange.\n\u2022 Step 2: By convexity, for all i,\nf (~\u2713i) \u2212f (~\u2713\u21e4) \uf8ff~rf (~\u2713i)T~ai \uf8ffk~aik2\n2 \u2212k~ai+1k2\n2\n2\u2318\n+ \u2318G 2\n2\n.\n7\n\ngd analysis proof\nTheorem: For convex G-Lipschitz function f : Rd ! R, GD run with\nt \u2265R2G2\n\u270f2\niterations, \u2318=\nR\nGpt , and starting point within radius R of ~\u2713\u21e4,\noutputs \u02c6\u2713satisfying f (\u02c6\u2713) \uf8fff (~\u2713\u21e4) + \u270f.\n\u2022 So far: For all i, f (~\u2713i) \u2212f (~\u2713\u21e4) \uf8ff\nk~ai k2\n2\u2212k~ai+1k2\n2\n2\u2318\n+ \u2318G2\n2\n\u2022 Sum over i:\nt\nX\ni=1\n\u21e3\nf (~\u2713i) \u2212f (~\u2713\u21e4)\n\u2318\n\uf8ff\nt\u2318G 2\n2\n+ 1\n2\u2318\nt\u22121\nX\ni=0\n\u21e3\nk~aik2\n2 \u2212k~ai+1k2\n2\n\u2318\n\uf8ff\nt\u2318G 2\n2\n+ 1\n2\u2318k~\u27130 \u2212~\u2713\u21e4k2\n2 \uf8fft\u2318G 2\n2\n+ R2\n2\u2318\n\u2022 Substitute \u2318and t: And using 1\nt\nPt\ni=1 f (~\u2713i) \u2265f (\u02c6\u2713) implies\nf (\u02c6\u2713) \u2212f (~\u2713\u21e4) \uf8ff1\nt\nt\nX\ni=1\nf (~\u2713i) \u2212f (~\u2713\u21e4) \uf8ff\nR2\n2\u2318\u00b7 t + \u2318G 2\n2\n\uf8ff\u270f.\n8\n\nfunction access\nOften the functions we are trying to optimize are very complex (e.g., a\nneural network). We will assume access to:\nFunction Evaluation: Can compute f (~\u2713) for any ~\u2713.\nGradient Evaluation: Can compute ~rf (~\u2713) for any ~\u2713.\nIn neural networks:\n\u2022 Function evaluation is called a forward pass (propogate an input\nthrough the network).\n\u2022 Gradient evaluation is called a backward pass (compute the gradient\nvia chain rule, using backpropagation).\n9\n\nprojected gradient descent\nAndrew McGregor\n0\n-1\n\nconstrained convex optimization\nOften want to perform convex optimization with convex constraints.\n~\u2713\u21e4= arg min\n~\u27132S\nf (~\u2713),\nwhere S is a convex set.\nDe\ufb01nition (Convex Set): A set S \u2713Rd is convex if and only if, for any\n~\u27131, ~\u27132 2 S and \u03bb 2 [0, 1]: (1 \u2212\u03bb)~\u27131 + \u03bb \u00b7 ~\u27132 2 S\nFor any convex set let PS(\u00b7) denote the projection function onto S:\nPS(~y) = arg min\n~\u27132S\nk~\u2713\u2212~yk2\n\u2022 For S = {~\u27132 Rd : k~\u2713k2 \uf8ff1} what is PS(~y)?\n\u2022 For S being a k dimensional subspace of Rd, what is PS(~y)?\n1\n\nprojected gradient descent\nProjected Gradient Descent\n\u2022 Choose some initialization ~\u27131 and set \u2318=\nR\nGpt .\n\u2022 For i = 1, . . . , t \u22121\n\u2022 ~\u2713(out)\ni+1\n= ~\u2713i \u2212\u2318\u00b7 ~rf (~\u2713i)\n\u2022 ~\u2713i+1 = PS(~\u2713(out)\ni+1 ).\n\u2022 Return \u02c6\u2713= arg min~\u2713i f (~\u2713i).\n2\n\nconvex projections\nAnalysis of projected gradient descent is almost identifcal to gradient descent\nanalysis! Just need to appeal to following geometric result:\nTheorem (Projection to a convex set): For any convex set S \u2713Rd,\n~y 2 Rd, and ~\u27132 S,\nkPS(~y) \u2212~\u2713k2 \uf8ffk~y \u2212~\u2713k2.\n3\n\nprojected gradient descent analysis\nTheorem (Projected GD): For convex G-Lipschitz function f , and con-\nvex set S, Projected GD run with t \u2265\nR2G2\n\u270f2\niterations, \u2318=\nR\nGpt , and\nstarting point within radius R of ~\u2713\u21e4= min~\u27132S f (~\u2713), outputs \u02c6\u2713satisfying\nf (\u02c6\u2713) \uf8fff (~\u2713\u21e4) + \u270f\nRecall: ~\u2713(out)\ni+1\n= ~\u2713i \u2212\u2318\u00b7 ~rf (~\u2713i) and ~\u2713i+1 = PS(~\u2713(out)\ni+1 ).\nProof from earlier establishes that for all i,\nf (~\u2713i) \u2212f (~\u2713\u21e4) \uf8ffk~\u2713i \u2212\u2713\u21e4k2\n2 \u2212k~\u2713(out)\ni+1 \u2212~\u2713\u21e4k2\n2\n2\u2318\n+ \u2318G 2\n2\n.\nBut Projection Lemma then ensures that for all i,\nf (~\u2713i) \u2212f (~\u2713\u21e4) \uf8ffk~\u2713i \u2212~\u2713\u21e4k2\n2 \u2212k~\u2713i+1 \u2212~\u2713\u21e4k2\n2\n2\u2318\n+ \u2318G 2\n2\nRest of proof unchanged: f (\u02c6\u2713) \u2212f (~\u2713\u21e4) \uf8ff1\nt\nPt\ni=1 f (~\u2713i) \u2212f (~\u2713\u21e4) \uf8ff\nR2\n2\u2318\u00b7t + \u2318G2\n2 .\n4\n\nonline gradient descent\nAndrew McGregor\n0I\n\nonline gradient descent\nIn reality many learning problems are online.\n\u2022 Websites optimize ads or recommendations to show users, given\ncontinuous feedback from these users.\n\u2022 Spam \ufb01lters are incrementally updated and adapt as they see more\nexamples of spam over time.\n\u2022 Face recognition systems, other classi\ufb01cation systems, learn from\nmistakes over time.\nWant to minimize some global loss L(~\u2713, X) = Pn\ni=1 `(~\u2713, ~xi), when data\npoints are presented in an online fashion ~x1, ~x2, . . . , ~xn (similar to\nstreaming algorithms)\n1\n\nonline optimization formal setup\nOnline Optimization: In place of a single function f , we see a di\u21b5erent\nobjective function at each step:\nf1, f2, . . . , ft : Rd ! R\n\u2022 At each step, \ufb01rst pick (play) a parameter vector ~\u2713(i).\n\u2022 Then are told fi and incur cost fi(~\u2713(i)).\n\u2022 Goal: Minimize total cost Pt\ni=1 fi(~\u2713(i)).\nOur analysis will make no assumptions on how f1, . . . , ft are related to\neach other!\n2\n\nonline optimization example\nHome pricing tools.\n\u2022 Parameter vector ~\u2713(i): coe\ufb03cients of linear model at step i.\n\u2022 Functions f1, . . . , ft: fi(~\u2713(i)) = (h~xi, ~\u2713(i)i \u2212pricei)2 revealed when\nhomei is listed or sold.\n\u2022 Want to minimize total squared error Pt\ni=1 fi(~\u2713(i)) (same as classic\nleast squares regression).\n3\n\nonline optimization example\nUI design via online optimization.\n\u2022 Parameter vector ~\u2713(i): some encoding of the layout at step i.\n\u2022 Functions f1, . . . , ft: fi(~\u2713(i)) = 1 if user does not click \u2018add to cart\u2019 and\nfi(~\u2713(i)) = 0 if they do click.\n\u2022 Want to maximize number of purchases, i.e., minimize Pt\ni=1 fi(~\u2713(i)).\n4\n\nregret\nIn normal optimization, we seek \u02c6\u2713satisfying:\nf (\u02c6\u2713) \uf8ffmin\n~\u2713\nf (~\u2713) + \u270f.\nIn online optimization we want:\nt\nX\ni=1\nfi(~\u2713(i)) \uf8ff\nt\nX\ni=1\nfi(~\u2713o\u21b5) + \u270f\nwhere ~\u2713o\u21b5= arg min~\u2713\nPt\ni=1 fi(~\u2713) and \u270fis called the regret and \u270f/t is the\naverage regret.\n\u2022 This error metric is a bit unusual: Comparing online solution to best\n\ufb01xed \u201conline\u201d solution in hindsight. \u270fcan be negative!\n5\n\nintuition check\nWhat if for i = 1, . . . , t, fi(\u2713) = |\u2713\u22121000| or fi(\u2713) = |\u2713+ 1000| in an\nalternating pattern?\nHow small can the regret \u270fbe? Pt\ni=1 fi(~\u2713(i)) \uf8ffPt\ni=1 fi(~\u2713o\u21b5) + \u270f.\nWhat if for i = 1, . . . , t, fi(\u2713) = |\u2713\u22121000| or fi(\u2713) = |\u2713+ 1000| in no\nparticular pattern? How can any online learning algorithm hope to\nachieve small regret?\n6\n\nonline gradient descent\nAssume that:\n\u2022 f1, . . . , ft are all convex.\n\u2022 Each fi is G-Lipschitz, i.e., k~rfi(~\u2713)k2 \uf8ffG for all ~\u2713.\n\u2022 k~\u2713(1) \u2212~\u2713o\u21b5k2 \uf8ffR where \u2713(1) is the \ufb01rst vector chosen.\nOnline Gradient Descent\n\u2022 Pick some initial ~\u2713(1).\n\u2022 Set step size \u2318=\nR\nGpt .\n\u2022 For i = 1, . . . , t\n\u2022 Play ~\u2713(i) and incur cost fi(~\u2713(i)).\n\u2022 ~\u2713(i+1) = ~\u2713(i) \u2212\u2318\u00b7 ~rfi(~\u2713(i))\n7\n\nonline gradient descent analysis\nTheorem: For convex G-Lipschitz f1, . . . , ft, OGD initialized with start-\ning point \u2713(1) within radius R of \u2713o\u21b5, using step size \u2318=\nR\nGpt , has regret\nbounded by:\n\"\nt\nX\ni=1\nfi(\u2713(i)) \u2212\nt\nX\ni=1\nfi(\u2713o\u21b5)\n#\n\uf8ffRG\np\nt\nAverage regret goes to 0 and t ! 1.\nNo assumptions on f1, . . . , ft!\nStep 1: For all i,\nrfi(\u2713(i))T(\u2713(i) \u2212\u2713o\u21b5) \uf8ffk\u2713(i) \u2212\u2713o\u21b5k2\n2 \u2212k\u2713(i+1) \u2212\u2713o\u21b5k2\n2\n2\u2318\n+ \u2318G 2\n2\nStep 2: Convexity implies that for all i,\nfi(\u2713(i)) \u2212fi(\u2713o\u21b5) \uf8ffk\u2713(i) \u2212\u2713o\u21b5k2\n2 \u2212k\u2713(i+1) \u2212\u2713o\u21b5k2\n2\n2\u2318\n+ \u2318G 2\n2 .\n8\n\nonline gradient descent analysis\nTheorem: For convex G-Lipschitz f1, . . . , ft, OGD initialized with start-\ning point \u2713(1) within radius R of \u2713o\u21b5, using step size \u2318=\nR\nGpt , has regret\nbounded by:\n\"\nt\nX\ni=1\nfi(\u2713(i)) \u2212\nt\nX\ni=1\nfi(\u2713o\u21b5)\n#\n\uf8ffRG\np\nt\nStep 2: For all i,\nfi(\u2713(i)) \u2212fi(\u2713o\u21b5) \uf8ffk\u2713(i) \u2212\u2713o\u21b5k2\n2 \u2212k\u2713(i+1) \u2212\u2713o\u21b5k2\n2\n2\u2318\n+ \u2318G 2\n2\nStep 3:\n\"\nt\nX\ni=1\nfi(\u2713(i)) \u2212\nt\nX\ni=1\nfi(\u2713o\u21b5)\n#\n\uf8ff\nt\nX\ni=1\nk\u2713(i) \u2212\u2713o\u21b5k2\n2 \u2212k\u2713(i+1) \u2212\u2713o\u21b5k2\n2\n2\u2318\n+ t \u00b7 \u2318G 2\n2\n=\nk\u2713(1) \u2212\u2713o\u21b5k2\n2 \u2212k\u2713(t+1) \u2212\u2713o\u21b5k2\n2\n2\u2318\n+ t \u00b7 \u2318G 2\n2\n\uf8ff\nR2/(2\u2318) + t\u2318G 2/2 = RG\np\nt\n9\n\nstochastic gradient descent\nAndrew McGregor\n0\u00b7\n\nstochastic gradient descent\nStochastic gradient descent is an e\ufb03cient o\u270fine optimization method,\nseeking \u02c6\u2713with\nf (\u02c6\u2713) \uf8ffmin\n~\u2713\nf (~\u2713) + \u270f\n\u2022 The most popular optimization method in modern machine learning.\nEasily analyzed as a special case of online gradient descent!\n\u2022 Basic Idea: In gradient descent, we set ~\u2713i+1 = ~\u2713i \u2212\u2318\u00b7 ~rf (~\u2713i). In\nstochastic gradient descent we don\u2019t compute ~rf (~\u2713i) exactly but\ninstead do something random that is correct in expectation. This saves\ntime per step but might increase the number of steps.\n1\n\nstochastic gradient descent\nAssume that:\n\u2022 f is convex and decomposable as f (~\u2713) = Pn\nj=1 fj(~\u2713).\n\u2022 For example, trying to minimize a loss function over a data set X,\nL(~\u2713, X) = Pn\nj=1 `(~\u2713, ~xj) that is a sum of losses of element in data set.\n\u2022 Each fj is G\nn -Lipschitz:\nkrf (~\u2713)k2 \uf8ffk Pn\nj=1 rfj(~\u2713)k2 \uf8ffPn\nj=1 krfj(~\u2713)k2 \uf8ffG .\n\u2022 Initialize with \u2713(1) satisfying k~\u2713(1) \u2212~\u2713\u21e4k2 \uf8ffR.\nStochastic Gradient Descent\n\u2022 Pick some initial ~\u2713(1).\n\u2022 Set step size \u2318=\nR\nGpt .\n\u2022 For i = 1, . . . , t\n\u2022 Pick random ji 2 1, . . . , n.\n\u2022 ~\u2713(i+1) = ~\u2713(i) \u2212\u2318\u00b7 ~rfji (~\u2713(i))\n\u2022 Return \u02c6\u2713= 1\nt\nPt\ni=1 ~\u2713(i).\n2\n\nexample\nIf f (x, y) = (x2 + 3xy) + (x + y) then gradient descent updates\n\u2713i+1 = \u2713i \u2212\u2318\n \n2\u2713i\n1 + 3\u2713i\n2 + 1\n3\u2713i\n1 + 1\n!\nWith probability 1/2, stochastic gradient descent updates\n\u2713i+1 = \u2713i \u2212\u2318\n \n2\u2713i\n1 + 3\u2713i\n2\n3\u2713i\n1\n!\nand with probability 1/2 the update is:\n\u2713i+1 = \u2713i \u2212\u2318\n \n1\n1\n!\n3\n\nstochastic gradient descent\n~\u2713(i+1) = ~\u2713(i) \u2212\u2318\u00b7 ~rfji(~\u2713(i))\nvs.\n~\u2713(i+1) = ~\u2713(i) \u2212\u2318\u00b7 ~rf (~\u2713(i))\nNote that: E[~rfji(~\u2713(i))] = 1\nn ~rf (~\u2713(i)).\nAnalysis extends to any algorithm that takes the gradient step in\nexpectation (minibatch SGD, randomly quantized, measurement noise,\ndi\u21b5erentially private, etc.)\n4\n\nstochastic gradient descent analysis\nTheorem \u2013 SGD on Convex Lipschitz Functions: SGD run with t \u2265\nR2G2\n\u270f2\niterations, \u2318=\nR\nGpt , and starting point within radius R of \u2713\u21e4,\noutputs \u02c6\u2713satisfying: E[f (\u02c6\u2713)] \uf8fff (\u2713\u21e4) + \u270f.\nStep 1: f (\u02c6\u2713) \u2212f (\u2713\u21e4) \uf8ff1\nt\nPt\ni=1[f (\u2713(i)) \u2212f (\u2713\u21e4)] since\nf (\u02c6\u2713) = f (\nt\nX\ni=1\n\u2713(i)/t) \uf8ff1\nt\nt\nX\ni=1\nf (\u2713(i)) by convexity\nStep 2: E[f (\u02c6\u2713) \u2212f (\u2713\u21e4)] \uf8ffn\nt \u00b7 E\nhPt\ni=1[fji (\u2713(i)) \u2212fji (\u2713\u21e4)]\ni\nsince\nE[fji (~\u2713)] = 1\nn f (~\u2713) since f (~\u2713) = Pn\nj=1 fj(~\u2713)\nStep 3: E[f (\u02c6\u2713) \u2212f (\u2713\u21e4)] \uf8ffn\nt \u00b7 R \u00b7 G\nn \u00b7\np\nt\n|\n{z\n}\nOGD bound\n= RG\npt .\n5\n\nsgd vs. gd: time per iteration\nStochastic gradient descent generally makes more iterations than\ngradient descent.\nEach iteration is much cheaper (by a factor of n).\n~r\nn\nX\nj=1\nfj(~\u2713) vs. ~rfj(~\u2713)\n6\n",
        "added_at": "2025-11-30T01:36:06.679922"
      }
    ]
  },
  "a2c009bf-003c-4d43-8dbb-29a38d7657b1": {
    "created_at": "2025-11-30T01:36:42.836146",
    "files": [
      {
        "name": "514 Lectures  3.pdf",
        "type": "document",
        "text": "ALGORITHMS\nFOR\nDATA\nSCIENCE\nMotivations\n-\n1) People are intested in analyzing + learning from massive dataset\n2) Data Science is highly interdisciplinary\nTextbooks\n-\n1) foundations of Data Science\nof Massive Datasets\n2) Mining\n\nLecture A1 :\nmunun\n! PROBABILITY\nREVIEW !\nconditional prob :\nfor events A and B,\nPr(AIB)=\nA\nindependence :\nA and B\nare\nindependent if\nPr(AIB) = Pr(A)\nor equivalently\nPr(ANB) = Pr(A)\n. Pr(B)\nindependent random variables :\nX & Y\nare\nindependent\nif the events\nEX = s3\nand EY = + 3\nare independent for all values St.\nex . Consider two\nindependent coin flips :\nEHH , HT, TH, TT3\n- let A be the event the first flip is heads -\n> EHH, HTS\n-\n- let is\nbe the\nevent that there\nis\n1 heads\nin\ntotal -\n>\nHT , TH3\n- let\nC be the\nevent that there\nare\n2 heads in total > \u00b7 H H3\n-> What is Pr(CIA) ?\nPr(A)\n= E\n,\nC1A = SHH3\n-\n>\nPr((1A)=\nPr(2(a)\n= P\n-\n> are A&B\nindependent ?\nPr(A)=, Pr(B) = E\n-\n>\nEHTS\nPr(AIB) = OA =\nsince\nPrCAIB)=\n= Pr(A)\nEn\nPrCANB) =\n= pas . PB)\n, the\nevents are independent.\nconsider a\nrandom X variable taking values\nin\nsome\ndiscrete set\nSCIR.\n- Expectation :\nthe weighted\naverage of the possible\nvalues\nE(x] = [ Pr(X = s)\n. S\nSES\nand for any function f : IR-IR ,\nE(f(x))=\nPr(x= s)\u00b7\n\n-variance:\nA measure of\nnow\nconcentrated the\nrandom variable is\nvar [x] = [E[(X-1E[X])3]\nX, X2X3\nP,\nP2\nPs\ne .g. if\nX takes the\nvalues\n1 , 2, 4 with probabilities\n13 , 12\n, 16 then\nIE[X] = (5 1) + (t2) + (t\n\u00b7 4) = 2\nvar [x] = (5 (1 - 2))\n+ (2(2 -2)] + (t (4-2)2] =\n/\nP,\nX\nP2X2\nPs\nY\n- X, , . ... Yn\nare\nK-wise independent\nit for\nany A c[n]\nwith 1AIK\nPr(1EXi\n= Sib)= Pr(i = Si)\nUSESA, .... SES\nitA\nI\nfull independence\nI\nK-wise\nex. Toss two independent coins.\n- let\nX\n,\n= 1 if the first\ncoin is heads and\nO otherwise.\n- let X2 = 1\nif the second\ncoin\nis heads and\nO otherwise\n- let Xz= (X, ++2) % 2\n. Show these\nrandom variables\nare\n2-wise independent but not\n3-wise,\noutcome\nY3\n\u00b7 X, &X2\nare obviously independent\nHH\nO\n\u00b7X& X3\n:\nPr(X, = 1)=\nPr(Xz = 1) = E\nI T\nI\njoint -\n> Pr(x = 1 , Xz = 1)=\nEHT3\nTi\nproduct\n-\n> I: -equal V pairwise\nTT\nI \"\n\u00b7 XzXz\n: (same reasoning as above\n-\n\n\u00b7\nX.,XzX3\n- for full independence , we'd need :\nPr(X,\n= 1\n, xz\n= 1 , xg\n= 1) = Pr(X,\n= 1) Pr(Xz\n= 1) Pr(Xz = 1)\nno three\ncolumns\n= .\n. t\nhave\nIs\n0\n= 1s\n-\n>\n2-wise\nbut not\n3-wise\nimm\nproperties\n- for scalar < . E(c\n. X] = a . #[x]\nand var[a\n. x] = var(X]\n- linearity of Expectation\n:\nGiven\nany set\nof\nrandom variables\nX,\nX2,\n.\n. ., Yn :\n#([Xi]\n= [E[Xi]\n;\n-Given any set of random variables\nX, Xz, ... Yn tudt\nare pairwise\nindependent\n:\nvar [i]\n= &var (xi]\ni\n- A random\nvariable\nX that only takes the\nvalues\nO and 1\nis called an\nIndicator\nRandom Variable\n-the expectation\nis #[x]\n= Pr(X = 1)\n= P\n-\n> E[X:] = P\n- the variance is var(ix] = #[x]\n- #(x] = Pr(X\n= 1)\n- Pr(X=1) =\np - p2\n- Suppose random variable\nX\ncan\nbe written as\nX= A + Az +... + An\nwhere\neach\nA :\nare\nindependent indicator variables v/ Pr(Ai) = p.\n- then\nX\nis\na Binomial\nRandom Variable\nwith\nparameters\nn\nand\np.\nPr[X = i) = (i)\n= pi(z\n- p)n\n- i\nncome fromsummingonaa trin)\n- Recall ECAi)= p and var[Ai] = p- p2\n\u2191\nW\n\u21b3by linearity of expectation\nand variance , E(X) = up\n& var[X]\n= np(1-p)\nGeometric\nRandom Variable\n- repeatedly toss a coin where the probability of\nheads is p\n. Let X\nbethe #\nof\ncoin tosses\nobserved up to including the\nfirst heads\n-then\nX\nis the\ngeometric\nrandom var\nwith parameter p : Pr(X+i)= (l-p): p .\n&\n#2x]=\n\nmmmme\nA2 :\nMarkov & Chebyshev\n- we often need to consider quantifies of the form\nPr(X]t) = [Pr[X= i)\nit\ne.g. the probability we get 360\nheads when we toss\n100 fair coins\n- if we know Pr[X=i) for each i, we\ncan work this out exactly\n&\n- the simplest concentration\nbound is\nMarkov's inequality.\n-\n> for any non-negative\nrandom variable\nX\nand any\nt30 :\nPr[XIt)\nA refer to slides for proofA\n(the larger the deviation t\n, the smaller the probability)\n- Chebyshev's\nInequality\n:\nfor any\nrandom variable X\nand any 730 :\nPr[/X-E(X)1]t) [Va(x)\nAlooke slides for proof\n- Law of Large\nNumbers : With enough samples n, the\nsample\naverage\nwill\nalways\nconcentrate to the\nmean\nummmmmm\nA3 : Application Ominmarkov\n&\nChebyshev\n\u2466\n\u2461\n\n-\n> #ii\n: Gintestis a\nhe a se\n/ -\nindicati\nrandom\n#[Dij) = 1\n\u00b7 Pr (Di ,j = 1) + 0\n. Pr (Dij =0)\nvariable\n-\nfor\ni\n. r . U\n. the\n= Pr (Di ,j =1)\nexpectation is\nthat the prob\n. is\n= 1\n- Since each CAPTCHA\nis chosen uniformly at\n+\n----- -- ;\nrandom from I , Pr(Dist\n= En\n\u00b7 Now , lets define the totalof\npairs that\nare\nduplicates\n-\nD = &\nDis\n-o this\ncounts the total # of\npairs that\nare duplicate\ni,je[m] ,i<j\n\u00b7\nex . if\nCAPTCHA \"abC\"\nshows\nupa times, that\ncreates (2)\n= 6 pairs\n\u00b7 Expected Value :\n- E(D)\n= [ELDi,i)\n\u246a\ni\n,je[m], i <]\nSub\n#[Di ,j]=\nY\n\u2193\n= (~).\n= mm-\n\u00b7S\nO you take\nm = 1000 samples.\nIf the database\nhas\nn =\n1,000, 000\nentries,\nthen the expected\nnumber of\nduplicates\nis :\nE(X) =ma\n=\n0 . 49\n\u2461You see\n10 pairwise duplicates\nand suspect something is\nup. But\nnow confident\ncan you be in your\ntests ?\n\u21b3concentration Inequalities/Tail Bounds :\nBounds\non the probability\nthat a random variable deviates\na\ncertain distance from the\nmean\n-\n> used in understanding how statistical tests perform,\nthe behavior ofrandomized\nalgorithms , etc.\n\u2193cont\n\n\u2462By Markov's inequality\n, if the\ndatabase\nis\n1 = 1,000, 000\nand\nyou\nobserve D= 10 duplicates, then the probability\nof this happening is :\nPr[D110) = ESD-00995\n\u2193\nThis is very small and you feel pretty sure that\nthe # of unique CAPTCHAS\nis much less than\n1,000,000.\nHow\nto calculate\nk in\nChebysher\nIneg.\nPr(1H-M/ = ko)1 z\n\u2460Express\nin\nterms of the mean\nH -M = a - M\n\u2461k=\nM\n\u2193\nchebyshev Inequality\n:\nPr(1H-M11K0) - k\n\u2193\n-> # of standard deviations away from the mean\nM + the expected value\no- variance\nunmunimme\nAt : Chernoff & Bernstein Bounds\n\nflipping coins\nWe \ufb02ip n = 100 independent coins, each are heads with probability 1/2\nand tails with probability 1/2. Let H be the number of heads.\nE[H] = n\n2 = 50 and Var[H] = n\n4 = 25\nMarkov:\nPr(H \u226560) \uf8ff.833\nPr(H \u226570) \uf8ff.714\nPr(H \u226580) \uf8ff.625\nChebyshev:\nPr(H \u226560) \uf8ff.25\nPr(H \u226570) \uf8ff.0625\nPr(H \u226580) \uf8ff.0278\nIn Reality:\nPr(H \u226560) = 0.0284\nPr(H \u226570) = .000039\nPr(H \u226580) < 10\u22129\n1\n-Markov chains are\nloose bounds since it only uses the mean\n-chebysher uses variance and\nis therefore tighter from\nMarkov , but still\nlove.\n= 0 2 0 = 25 = 5\nECH]\nz\na\nA\nM\nA\nu\nL\nSTOZE0\nI\nexact\nY\nBinomial\nk = AM-5\nTail\n\ntighter concentration bounds\nTo be fair... Markov and Chebyshev\u2019s inequalities apply much more generally\nthan to Binomial random variables like coin \ufb02ips.\nCan we obtain tighter bounds that still apply to many distributions?\n\u2022 Markov: Pr(X \u2265t) \uf8ffE[X]\nt . Sometimes called a First Moment approach.\n\u2022 Chebyshev:\nPr(|X \u2212E[X]| \u2265t) = Pr(|X \u2212E[X]|2 \u2265t2) \uf8ffVar[X]\nt2\nProved via analyzing the Second Moment.\n\u2022 What if we just apply Markov\u2019s inequality to even higher moments?\n2\n\na fourth moment bound\nConsider any random variable X:\nPr(|X \u2212E[X]| \u2265t) = Pr\n\u21e3\n(X \u2212E[X])4 \u2265t4\u2318\n\uf8ff\nE\nh\n(X \u2212E[X])4i\nt4\n.\nApplication to Coin Flips: Recall we have n = 100 independent fair coins and\nH is the number of heads.\n\u2022 Bound the fourth moment:\nE\nh\n(H \u2212E[H])4i\n= E\n2\n4\n 100\nX\ni=1\nHi \u221250\n!43\n5 =\nX\ni,j,k,`\ncijk`E[HiHjHkH`] = 1862.5\nwhere Hi = 1 if ith \ufb02ip is heads and 0 otherwise. Last calculations are messy!\n\u2022 Apply Fourth Moment Bound: Pr (|H \u2212E[H]| \u2265t) \uf8ff1862.5\nt4\n3\nH = %H ;\nwith\nH,30, 13 ,\nPrCH= 1) =2\nElY;] = 0\ni\n- E[Y]\n=t\nYi = Hi-t\n. the\nH-ACH) = [ , Vi\nECY?]=o\nS\nf\nxplanation\non\nnext\npage\n\nmarkov inequality\nPr(1X-E()((t)\n= pr ((X- E(X])\"It - EXE(X)9]\napplied to nonnegative\nE random variable\n(X-ECX3)!\n- for t = 0\n, the function >UP\nis increasing\non UIO\n- (X-E(X]) can\nnever be negative (to ath pur)\n- Markov\nsays ,\nfor\nanynonneg\nrandom\nvar\nY\nand\nany\naso\n, Pr(4(a) Es\n& Take\nY = (x-E(X)\"\nand\na = +0\n\u00b7 them\n:\nPr((X-E(x))\"It))--E)\nECH-E(H])\n= E((Hi-501: [rieE(HiHjHiHe)\n= 1862\n. 5\ni,j,k,l\n-This\nis the #((X-E(x))] calculation\nseer\nin the moment Markov bound\n1)Setup\n: -Hiis 1\nis flip i\nis heads,\nO Otherwise -\n>\nHiw Bernoulli (1/2)\n- totalof heade is\nH =&Hi ;\nMean : #(H) : [ECH)\n= 100\n\u00b7 E\n=50 .\ni = 1\n\ntighter bounds\nChebyshev\u2019s:\nPr(H \u226560) \uf8ff.25\nPr(H \u226570) \uf8ff.0625\nPr(H \u226580) \uf8ff.04\n4th Moment:\nPr(H \u226560) \uf8ff.186\nPr(H \u226570) \uf8ff.0116\nPr(H \u226580) \uf8ff.0023\nIn Reality:\nPr(H \u226560) = 0.0284\nPr(H \u226570) = .000039\nPr(H \u226580) < 10\u22129\n\u2022 We aren\u2019t restricted to applying Markov\u2019s to |X \u2212E[X]|k for some k.\n\u2022 Can use any \u201cmonotonic\u201d function f satisfying b > a i\u21b5f (b) > f (a):\nPr(|X \u2212E[X]| > t) = Pr(f (|X \u2212E[X]|) > f (t))\n4\n100 coin flips\n50/50 chance\n\u2193\nThree waysto upper bound events for He Bin (100, 0 . 5)\nPr(IH-50127)\nPr(H-501t)50\nt4\nt = 10\nt = 20\nt =30\nf(u) = u2-\n> Chebyshev\nflu)= % - 9th bound Markov\nflu) = e* -\n> Chernoff/Hoeffding/Bernstein\n\nexponential concentration bounds\n\u2022 Moment Generating Function: Consider for any r > 0:\nMr(X) = er\u00b7(X\u2212E[X]) =\n1\nX\nk=0\nr k(X \u2212E[X])k\nk!\nand note Mr(X) is monotonic for any r > 0 and so\nPr[X \u2212E[X] \u2265\u03bb] = Pr[Mr(X) \u2265er\u03bb] \uf8ffE[Mr(X)]\ner\u03bb\n\u2022 Weighted sum of all moments (r controls the weights) and choosing r\nappropriately gives various powerful exponential concentration bounds\nsuch as Cherno\u21b5, Bernstein, Hoe\u21b5ding, Azuma, Berry-Esseen, etc.\n5\n\nbernstein inequality\nBernstein\nInequality:\nConsider\nindependent\nrandom\nvariables\nX1, . . . , Xn 2 [-1,1]. Let \u00b5 = E[Pn\ni=1 Xi] and \u03c32 = Var[Pn\ni=1 Xi]. For\nany s \u22650:\nPr\n #####\nn\nX\ni=1\nXi \u2212\u00b5\n##### \u2265s\u03c3\n!\n\uf8ff2 exp\n\u2713\n\u2212s2\n4\n\u25c6\n.\nAssume that M = 1 and plug in t = s \u00b7 \u03c3 for s \uf8ff\u03c3.\nCompare to Chebyshev: Pr\n(##Pn\ni=1 Xi \u2212\u00b5\n## \u2265s\u03c3\n)\n\uf8ff\n1\ns2 .\n\u2022 An exponentially stronger dependence on s!\n6\n\ncomparision to chebyshev\nFor the number of heads, H, amongst n = 100 independent coin \ufb02ips:\nChebyshev:\nPr(H \u226560) \uf8ff.25\nPr(H \u226570) \uf8ff.0625\nPr(H \u226580) \uf8ff.04\nBernstein:\nPr(H \u226560) \uf8ff.0.412\nPr(H \u226570) \uf8ff.0108\nPr(H \u226580) \uf8ff0.0000907\nIn Reality:\nPr(H \u226560) = 0.0284\nPr(H \u226570) = .000039\nPr(H \u226580) < 10\u22129\nGetting much closer to the true probability.\n7\n\nexponential tail bounds\nBernstein Inequality:\nConsider independent random variables\nX1, . . . , Xn all falling in [\u2212M, M].\nLet \u00b5 = E[Pn\ni=1 Xi] and\n\u03c32 = Var[Pn\ni=1 Xi] = Pn\ni=1 Var[Xi]. For any t \u22650:\nPr\n $$$$$\nn\nX\ni=1\nXi \u2212\u00b5\n$$$$$ \u2265t\n!\n\uf8ff2 exp\n \n\u2212\nt2\n2\u03c32 + 4\n3Mt\n!\n.\nA useful variation for binary (indicator) random variables is:\nCherno\u21b5Bound (simpli\ufb01ed version): Consider independent random\nvariables X1, . . . , Xn taking values in {0, 1}. Let \u00b5 = E[Pn\ni=1 Xi]. For\nany \u03b4 \u22650\nPr\n #####\nn\nX\ni=1\nXi \u2212\u00b5\n##### \u2265\u03b4\u00b5\n!\n\uf8ff2 exp\n\u2713\n\u2212\u03b42\u00b5\n2 + \u03b4\n\u25c6\n.\n8\n\napplication: maximum load\nAndrew McGregor\n0\nA5\nV\n\nrandomized load balancing\n\u2022 n requests or tasks are randomly assigned to k servers.\n\u2022 Let Ri be the number requests assigned to the ith server.\n\u2022 Ri is a binomial random variable and hence has expectation:\nE[Ri] =\nn\nX\nj=1\nE[Irequest j assigned to i] =\nn\nX\nj=1\nPr [j assigned to i] = n\nk .\n\u2022 Variance:\nVar[Ri] = Var[\nn\nX\nj=1\nIrequest j assigned to i] =\nn\nX\nj=1\nVar[Ij assigned to i] = n\n\u27131\nk \u22121\nk2\n\u25c6\n< n/k\n1\n\nmaximum server load\nSuppose a server crashes if it\u2019s load exceeds twice the expected load.\nWhat\u2019s the probability some server has load greater than 2 \u00b7 E[Ri] = 2n\nk ?\n\u2022 By Markov\u2019s inequality, Pr[Ri \u22652E[Ri]] \uf8ff1/2.\n\u2022 By Chebyshev\u2019s inequality, Pr[Ri \u22652E[Ri]] \uf8ffVar[Ri]\nE[Ri]2 < k\nn.\nWe want to upper bound:\nPr\n\u2713\nmax\ni\n(Ri) \u22652n\nk\n\u25c6\n= Pr\n\u2713\uf8ff\nR1 \u22652n\nk\n%\nor . . . or\n\uf8ff\nRk \u22652n\nk\n%\u25c6\n= Pr\n k[\ni=1\n\uf8ff\nRi \u22652n\nk\n%!\nHow do we do this since R1, . . . , Rk are not independent?\n2\n\nthe union bound\nUnion Bound: For any random events A1, A2, ..., Ak,\nPr (A1 [ A2 [ . . . [ Ak) \uf8ffPr(A1) + Pr(A2) + . . . + Pr(Ak).\nProof:\nLet X = P\ni Xi where Xi = 1 if Ai happens and 0 otherwise.\nPr (A1 [ A2 [ . . . [ Ak) = Pr(X1 + . . . + Xk \u22651) \uf8ffE[X] (by Markov)\nand E[X] = P\ni E[Xi] = P\ni Pr(Xi = 1) = Pr(A1) + . . . + Pr(Ak).\n3\n\napplying the union bound\nWhat\u2019s the probability some server has load greater than 2 \u00b7 E[Ri] = 2n\nk ?\nPr\n\u2713\nmax\ni\n(Ri) \u22652n\nk\n\u25c6\n= Pr\n k[\ni=1\n\uf8ff\nRi \u22652n\nk\n%!\n\uf8ff\nk\nX\ni=1\nPr\n\u2713\uf8ff\nRi \u22652n\nk\n%\u25c6\n(Union Bound)\n\uf8ff\nk\nX\ni=1\nk\nn = k2\nn\n(Bound from Chebyshev)\nIf k \u2327pn, max load will be small compared to the expected load with\ngood probability. When k is smaller, each server gets more requests and\nlaw of large numbers implies they gets closer to the expected number.\n4\n\nalternative analysis via chernoff bound\nCherno\u21b5Bound: Since Ri is binomial distributed:\nPr(Ri \u22652E[Ri]) \uf8ffPr (|Ri \u2212E[Ri]| \u2265E[Ri]) \uf8ff2 exp\n\u2713\n\u221212 \u00b7 E[Ri]\n2 + 1\n\u25c6\n= 2 exp\n\u21e3\n\u2212n\n3k\n\u2318\nand so by the union bound\nPr(Ri \u22652E[Ri] for some i 2 [k]) \uf8ff2k exp\n\u21e3\n\u2212n\n3k\n\u2318\nBecomes very small (as n tends to in\ufb01nity) if k \u2327n/(log n) whereas for\nthe Chebyshev analysis we needed k \u2327pn.\nExercise: If k = n, use Cherno\u21b5to show that for su\ufb03ciently large n,\nPr[max Ri \uf8ff3 log n] > 1 \u22121/n .\n5\n\nhash functions\nAndrew McGregor\n0\nA6\nV\n\nhash functions\n\u2022 In maximum load example, we made a separate choice about which\nserver to send each request. What if we want identical requests sent to\nsame server?\n\u2022 Classic Solution: Hash Functions\n\u2022 Suppose U is set of possible requests and we have n servers\n\u2022 Before the requests start arriving, randomly pick a function\nh : U ! [n]\n\u2022 Send request x 2 U to server h(x).\n\u2022 Simplest construction is picking a random value in [n] for each x 2 U\nand de\ufb01ne h(x) to be this value. Is fully random but very ine\ufb03cient!\n\u2022 Next we will see more e\ufb03cient (but less random) construction. (Also,\nin practice, often su\ufb03ces to use hash functions like MD5, SHA-2, etc.)\n1\n\nefficiently computable hash function: example\n\u2022 For simplicity, assume U = {0, 1, . . . , |U| \u22121}\n\u2022 Let p \u2265|U| be a prime number.\n\u2022 Pick a 2 {1, . . . , p \u22121} and b 2 {0, 1, . . . , p \u22121} uniformly at random\n\u2022 For each x 2 U de\ufb01ne h(x) = ((ax + b) mod p) mod n\nThis function is \u201c2-Universal\u201d and it\u2019s often the only property you need!\n2-Universal Hash Function (low collision probability). A random\nhash function from h : U ! [n] is two universal if for all x 6= y 2 U:\nPr[h(x) = h(y)] \uf8ff1\nn.\n2\n\npairwise independence\nAnother common requirement for a hash function:\nPairwise Independent Hash Function. A random hash function from\nh : U ! [n] is pairwise independent if for all i, j 2 [n] and for all\nx 6= y 2 U:\nPr[h(x) = i \\ h(y) = j] = 1\nn2 .\nWhich is a more stringent requirement? 2-universal or pairwise independent?\nPr[h(x) = h(y)] =\nn\nX\ni=1\nPr[h(x) = i \\ h(y) = i] = n \u00b7 1\nn2 = 1\nn .\nA closely related (ax + b) mod p construction gives pairwise independence\non top of 2-universality.\nNote: A fully random hash function is both 2-universal and pairwise\nindependent. But it is not e\ufb03ciently implementable.\n3\n\none-level hash tables\nAndrew McGregor\n0\nA7\nV\n\nhash tables\nWant to store a set of items from some \ufb01nite but massive universe of\nitems, e.g., images, text documents, 128-bit IP addresses.\nGoal: support query(x) to check if x is in the set in O(1) time.\nClassic Solution: Hash Tables\n\u2022 Static hashing since we won\u2019t worry about insertion and deletion today.\n1\n\nhash tables\n\u2022 Use 2-wise independent hash function h : U ! [n] maps elements in\nuniverse U to indices of an array. Store x 2 U in location h(x)\n\u2022 Collisions: when we insert m items into the hash table we may have\nto store multiple items in the same location (typically as a linked list).\n\u2022 If there are no collisions then the query time is O(1). More generally it\nis O(L) where L is the length of the longest linked list.\n2\n\nexpected number of collisions\n\u2022 The number of pairwise collisions is\nC =\nX\ni,j2[m],i<j\nCi,j\nwhere Ci,j = 1 if items i and j collide, and 0 otherwise.\n\u2022 Since the hash function was 2-wise independent,\nE[Ci,j] = Pr[Ci,j = 1] = Pr[h(xi) = h(xj)] = 1\nn .\n\u2022 By linearity of expectation,\nE[C] =\nX\ni,j2[m],i<j\n1\nn =\n\"m\n2\n#\nn\n= m(m \u22121)\n2n\n.\n3\n\ncollision free hashing\n\u2022 For n = 4m2 we have\nE[C] = m(m \u22121)\n2n\n= m(m \u22121)\n8m2\n< 1\n8 .\n\u2022 Apply Markov\u2019s Inequality:\nPr[C \u22651] \uf8ffE[C]\n1\n< 1\n8 .\n\u2022 Therefore,\nPr[C = 0] = 1 \u2212Pr[C \u22651] > 1 \u22121\n8 = 7\n8 .\n\u2022 Hence we can ensure O(1) query time but we are using O(m2) space\nto store m items.\n4\n\ntwo-level hash tables\nAndrew McGregor\n0\nAf\nV\n\ntwo level hashing\n\u2022 One-Level Hashing enabled O(1) query time but required O(m2) space.\n\u2022 Can we achieve O(1) query time with only O(m) space?\n\u2022 Two-Level Hashing:\n\u2022 For the ith bucket, pick a collision free hash function mapping [si] ! [4s2\ni ]\nwhere si values in the ith bucket.\n\u2022 Previously showed a random function is collision free with probability \u22657\n8\nso just generate a random hash function and check it is collision free.\n1\n\nspace usage\nQuery time for two level hashing is O(1): requires evaluating two hash\nfunctions. What is the expected space usage?\nUp to constants, space used is: E[S] = n + 4 Pn\ni=1 E[s2\ni ]\nE[s2\ni ] = E\n\" m\nX\nj=1\nIh(xj )=i\n!2#\n= E\n2\n4 X\nj,k2[m]\nIh(xj )=i \u00b7 Ih(xk )=i\n3\n5 =\nX\nj,k2[m]\nE\n\u21e5\nIh(xj )=i \u00b7 Ih(xk )=i\n\u21e4\n.\n\u2022 For j = k, E\n\u21e5\nIh(xj )=i \u00b7 Ih(xk )=i\n\u21e4\n= E\nh.\nIh(xj )=i\n/2i\n= Pr[h(xj) = i] = 1\nn.\n\u2022 For j 6= k, E\n\u21e5\nIh(xj )=i \u00b7 Ih(xk )=i\n\u21e4\n= Pr[h(xj) = i \\ h(xk) = i] =\n1\nn2 .\nxj, xk: stored items, n: hash table size, h: random hash function, S: space usage of two\nlevel hashing, si: # items stored in hash table at position i.\n2\n\nspace usage\nE[s2\ni ] =\nX\nj,k2[m]\nE\n\u21e5\nIh(xj )=i \u00b7 Ih(xk )=i\n\u21e4\n= m \u00b7 1\nn + 2 \u00b7\n \nm\n2\n!\n\u00b7 1\nn2\n= m\nn + m(m \u22121)\nn2\n\uf8ff2 (If we set n = m.)\n\u2022 For j = k, E\n\u21e5\nIh(xj )=i \u00b7 Ih(xk )=i\n\u21e4\n= 1\nn.\n\u2022 For j 6= k, E\n\u21e5\nIh(xj )=i \u00b7 Ih(xk )=i\n\u21e4\n=\n1\nn2 .\nTotal Expected Space Usage: (if we set n = m)\nE[S] = n + 4\nn\nX\ni=1\nE[s2\ni ] \uf8ffn + 4n \u00b7 2 = 9n = 9m.\nNear optimal space with O(1) query time!\nxj, xk: stored items, m: # stored items, n: hash table size, h: random hash function, S:\nspace usage of two level hashing, si: # items stored at pos i.\n3\n\nbloom filters\nAndrew McGregor\n0\nan\n\napproximately maintaining a set\nWant to store a set S of items from a massive universe of possible items\n(e.g., images, text documents, IP addresses).\nGoal: Support insert(x) to add x to the set and query(x) to check if x is\nin the set. Both in constant time. Allow small probability \u03b4 > 0 of false\npositives: for any x /2 S,\nPr(query(x) = 1) \uf8ff\u03b4.\nSolution: Bloom \ufb01lters.\nSpace advantage over two-level hash tables: While O(m) slots were\nsu\ufb03cient to store m objects, if each object is b bits this takes O(mb)\nbits. Bloom \ufb01lters will only use O(m) bits assuming \u03b4 is constant.\n1\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n2\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nU ! [m]. For simplicity, assume each hash function is fully random.\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\nNo false negatives. False positives more likely with more insertions.\n2\n\napplications: determining when to cache\nAkamai (Boston-based company serving 15 \u221230% of all web tra\ufb03c) applies\nBloom \ufb01lters to prevent caching of \u2018one-hit-wonders\u2019 \u2013 pages only visited once\n\ufb01ll over 75% of cache.\n\u2022 Ideally, you\u2019d only cache a page if it\u2019s the second time it\u2019s been requested.\n\u2022 A Bloom Filter can be used to approximately track the url\u2019s you\u2019ve seen\nbefore without have to store them all! When url x comes in, if query(x) = 1,\ncache the page if it isn\u2019t already cached. If not, run insert(x) so that if it\ncomes in again, it will be cached.\n\u2022 False positive: If the Bloom \ufb01lter has a false positive rate of \u03b4 = .05, the\nnumber of cached one-hit-wonders will be reduced by at least 95%.\n3\n\nbloom filter analysis\nAndrew McGregor\n0\nAl\n\nbloom filters\nChoose k independent random hash functions h1, . . . , hk mapping the universe\nof elements U ! [m].\n\u2022 Maintain an array A containing m bits, all initially 0.\n\u2022 insert(x): set all bits A[h1(x)] = . . . = A[hk(x)] := 1.\n\u2022 query(x): return 1 only if A[h1(x)] = . . . = A[hk(x)] = 1.\n1\n\nanalysis\nFor a bloom \ufb01lter with m bits and k hash functions, the insertion and query\ntime is O(k). How does the false positive rate \u03b4 depend on m, k, and the\nnumber of items inserted?\nStep 1: What is the probability that after inserting n elements, the ith bit of\nthe array A is still 0? n \u21e5k total hashes must not hit bit i.\nPr(A[i] = 0) = Pr\n!\nh1(x1) 6= i \\ . . . \\ hk(x1) 6= i\n\\ h1(x2) 6= i . . . \\ hk(x2) 6= i \\ . . .\n\"\n= Pr\n!\nh1(x1) 6= i) \u21e5. . . \u21e5Pr\n!\nhk(x1) 6= i) \u21e5Pr\n!\nh1(x2) 6= i) . . .\n|\n{z\n}\nk\u00b7n events each occuring with probability 1\u22121/m\n=\n\u2713\n1 \u22121\nm\n\u25c6kn\n2\n\nanalysis\nHow does the false positive rate \u03b4 depend on m, k, and the number of items\ninserted?\nWhat is the probability that after inserting n elements, the ith bit of the array\nA is still 0?\nPr(A[i] = 0) =\n\u2713\n1 \u22121\nm\n\u25c6kn\n\u21e1e\u2212kn\nm\nLet T be the number of zeros in the array after n inserts. Then,\nE[T] = m\n\u2713\n1 \u22121\nm\n\u25c6kn\n\u21e1me\u2212kn\nm\nn: total number items in \ufb01lter, m: number of bits in \ufb01lter, k: number of random hash\nfunctions, h1, . . . hk: hash functions, A: bit array, \u03b4: false positive rate.\n3\n\ncorrect analysis sketch\nIf T is the number of 0 entries, for a non-inserted element w:\nPr(A[h1(w)] = . . . = A[hk(w)] = 1)\n= Pr(A[h1(w)] = 1) \u21e5. . . \u21e5Pr(A[hk(w)] = 1)\n= (1 \u2212T/m) \u21e5. . . \u21e5(1 \u2212T/m)\n= (1 \u2212T/m)k\n\u2022 How small is T/m? Note that T\nm \u2265m\u2212nk\nm\n\u21e1e\u2212kn\nm when kn \u2327m. More\ngenerally, it can be shown that T/m = \u2326\n\u21e3\ne\u2212kn\nm\n\u2318\nvia Theorem 2 of:\ncglab.ca/~morin/publications/ds/bloom-submitted.pdf\n4\n\nfalse positive rate\nFalse Positive Rate: with m bits of storage, k hash functions, and n items\ninserted \u03b4 \u21e1\n\u21e3\n1 \u2212e\n\u2212kn\nm\n\u2318k\n.\n0\n5\n10\n15\n20\n25\n30\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n\u2022 Can di\u21b5erentiate to show optimal number of hashes is k = ln 2 \u00b7 m\nn (rounded\nto the nearest integer). This gives \u03b4 \u21e11/2(m/n) ln 2.\n\u2022 Balances between \ufb01lling up the array with too many hashes and having\nenough hashes so that even when the array is pretty full, a new item is\nunlikely to have all its bits set (yield a false positive)\n5\n\ncombining estimates and the median trick\nAndrew McGregor\n0\nAl\n\ncombining noisy estimates\n\u2022 Suppose we have a randomized algorithm for estimating a quantity \u2713\nwhere the output of the algorithm is noisy.\n\u2022 Run the algorithm r times to get independent estimates X1, . . . , Xr\n\u2022 We can combine estimates to get a better estimate for \u2713.\n\u2022 \u201cCombine\u201d and \u201cBetter\u201d mean di\u21b5erent things in di\u21b5erent contexts. . .\n1\n\nexample 1\n\u2022 If estimates are never underestimates:\n\u2022 Return X = min(X1, . . . , Xr)\n\u2022 Suppose Pr[Xi > (1 + \u270f)\u2713] \uf8ff\u03b4 for each i.\n\u2022 Then\nPr[\u2713\uf8ffX \uf8ff(1 + \u270f)\u2713] = 1 \u2212Pr[Xi > (1 + \u270f)\u2713]r \u22651 \u2212\u03b4r\n2\n\nexample 2\n\u2022 If there are only two possible outputs and only one is correct:\n\u2022 Return X = majority(X1, . . . , Xr) where we assume r is odd to avoid ties.\n\u2022 Let Z be the number of answers that are incorrect.\nPr[X is incorrect] = Pr[Z > r/2]\n\u2022 If each Xi is incorrect with probability \u03b4 < 1/2 then Z \u21e0Bin(r, \u03b4) and we\ncan apply the Cherno\u21b5Bound:\nPr[Z > r/2] = Pr[Z > E[Z](1 + \u03b3)] \uf8ff2 exp(\u2212\u03b32E[Z]/(2 + \u03b3))\nwhere \u03b3 = 1/(2\u03b4) \u22121.\n\u2022 For example, if \u03b4 = 1/4 then \u03b3 = 1 and\nPr[X is incorrect ] \uf8ff2 exp(\u2212r/12) .\n3\n\nexample 3: averaging to reduce variance\n\u2022 Assuming estimates have correct expectation and small variance:\n\u2022 Return X = P\ni Xi/r, i.e., the mean of the estimates.\n\u2022 If E[Xi] = \u2713and Var[Xi] = \u03c32 for each i then\nE[X] = \u2713\nand\nVar[X] = \u03c32/r .\n\u2022 By the Chebyshev bound,\nPr[(1 \u2212\u270f)\u2713\uf8ffX \uf8ff(1 + \u270f)\u2713] = 1 \u2212Pr[|X \u2212\u2713| > \u270f\u2713] \u22651 \u2212\n\u03c32\nr\u270f2\u27132\n\u2022 E.g., setting r = 4\u03c32\u2713\u22122\u270f\u22122 ensures the probability is at least 3/4.\n\u2022 Could make 3/4 closer to 1 by increasing t but there\u2019s a better way. . .\n4\n\nexample 4: median trick\n\u2022 If estimates have the correct expectation and small variance:\n\u2022 Split r estimates into r1 = 12 log(2/\u03b4) groups of size r2 = 4\u03c32\u2713\u22122\u270f\u22122:\nX1,1\nX1,2\n. . .\nX1,r2\nX2,1\nX1,2\n. . .\nX2,r2\n...\n...\n...\n...\nXr1,1\nXr1,2\n. . .\nXr1,r2\n\u2022 Return Y = median(Y1, . . . , Yr1) where Yi is mean of ith row.\n\u2022 Example 3 analysis implies:\nPr[(1 \u2212\u270f)\u2713\uf8ffYi \uf8ff(1 + \u270f)\u2713] \u22651 \u2212\n\u03c32\nr2\u270f2\u27132 = 3/4\n\u2022 Example 2 analysis implies:\nPr[(1\u2212\u270f)\u2713\uf8ffmedian(Y1, . . . , Yr1) \uf8ff(1+\u270f)\u2713] \u22651\u22122 exp(\u2212r1/12) = 1\u2212\u03b4\n5\n\nstreams and frequent items\nAndrew McGregor\n0\n\u2191\n\nstreaming algorithms\nStream Processing: Have a massive dataset x1, x2, . . . , xn that arrive in\na continuous stream. Not nearly enough space to store all the items (in a\nsingle location).\n\u2022 Still want to analyze and learn from this data.\n\u2022 Typically must compress the data on the \ufb02y, storing a data structure\nfrom which you can still learn useful information.\n\u2022 Often the compression is randomized. E.g., Bloom \ufb01lters.\n\u2022 Compared to traditional algorithm design, which focuses on runtime,\nthe big question here is how much space is needed to answer a problem.\n1\n\nsome examples\n\u2022 Sensor data: images from telescopes (15 terabytes per night from the\nLarge Synoptic Survey Telescope), readings from seismometer arrays\nmonitoring and predicting earthquake activity, tra\ufb03c cameras and\ntravel time sensors (Smart Cities), electrical grid monitoring.\n\u2022 Internet Tra\ufb03c: 500 million Tweets per day, 5.6 billion Google\nsearches, billions of ad-clicks and other logs from instrumented\nwebpages, IPs routed by network switches, ...\n\u2022 Datasets in Machine Learning: When training e.g., a neural network\non a large dataset (ImageNet with 14 million images), the data is\ntypically processed in a stream due to storage limitations.\n2\n\nthe frequent items data stream problem\nk-Frequent Items (Heavy-Hitters) Problem: Consider a stream of n\nitems x1, . . . , xn (with possible duplicates). Return any item at appears at\nleast n/k times.\n\u2022 Trivial with O(n) space: Store the count for each item and return the\none that appears \u2265n/k times.\n\u2022 Can be at most k frequent items. Can we \ufb01nd them in o(n) space?\n3\n\nthe frequent items problem\nApplications of Frequent Items:\n\u2022 Finding top/viral items, e.g., products on Amazon, videos watched on\nYoutube, Google searches.\n\u2022 Finding very frequent IP addresses sending requests to detect DoS\nattacks/network anomalies.\n\u2022 \u2018Iceberg queries\u2019 for all items in a database with frequency above a\nthreshold.\nGenerally want very fast detection, without having to scan through\ndatabase/logs. That is, we want to maintain a running list of frequent\nitems that appear in a stream.\n4\n\napproximate frequent elements\nIssue: No algorithm using o(n) space can output just the items with\nfrequency \u2265n/k. Hard to tell between an item with frequency n/k\n(should be output) and n/k \u22121 (should not be output).\n(\u270f, k)-Frequent Items Problem: Consider a stream of n items\nx1, . . . , xn. Return a set F of items, including all items that appear \u2265n\nk\ntimes and no items that appear < (1 \u2212\u270f) \u00b7 n\nk times.\n\u2022 An example of relaxing to a \u2018promise problem\u2019: for items with\nfrequencies in [(1 \u2212\u270f) \u00b7 n\nk , n\nk ] no output guarantee.\n5\n\ncount-min sketch algorithm\nAndrew McGregor\n0\nAl3\n\u2193\n\ncount-min sketch\nCount-Min Sketch: A stream algorithm for estimating how many times\nelements has appeared in a stream x1, . . . , xn. Uses random hashing and\nis closely related to Bloom \ufb01lters.\n1\n\ncount-min sketch\nCount-Min Sketch: A stream algorithm for estimating how many times\nelements has appeared in a stream x1, . . . , xn. Uses random hashing and\nis closely related to Bloom \ufb01lters.\n1\n\ncount-min sketch\nCount-Min Sketch: A stream algorithm for estimating how many times\nelements has appeared in a stream x1, . . . , xn. Uses random hashing and\nis closely related to Bloom \ufb01lters.\n1\n\ncount-min sketch\nCount-Min Sketch: A stream algorithm for estimating how many times\nelements has appeared in a stream x1, . . . , xn. Uses random hashing and\nis closely related to Bloom \ufb01lters.\n1\n\ncount-min sketch\nCount-Min Sketch: A stream algorithm for estimating how many times\nelements has appeared in a stream x1, . . . , xn. Uses random hashing and\nis closely related to Bloom \ufb01lters.\n1\n\ncount-min sketch\nCount-Min Sketch: A stream algorithm for estimating how many times\nelements has appeared in a stream x1, . . . , xn. Uses random hashing and\nis closely related to Bloom \ufb01lters.\n1\n\ncount-min sketch\nCount-Min Sketch: A stream algorithm for estimating how many times\nelements has appeared in a stream x1, . . . , xn. Uses random hashing and\nis closely related to Bloom \ufb01lters.\nWill use A[h(x)] to estimate f (x) = |{i : xi = x}|, i.e., the frequency of x\nin the stream.\n1\n\ncount-min sketch accuracy\nUse A[h(x)] to estimate f (x).\nClaim 1: We always have A[h(x)] \u2265f (x). Why?\nf (x): frequency of x in the stream (i.e., number of items equal to x). h: random hash\nfunction. m: size of Count-min sketch array.\n2\n\ncount-min sketch accuracy\nUse A[h(x)] to estimate f (x).\nClaim 1: We always have A[h(x)] \u2265f (x). Why?\n\u2022 A[h(x)] counts the number of occurrences of any y with h(y) = h(x),\nincluding x itself.\nf (x): frequency of x in the stream (i.e., number of items equal to x). h: random hash\nfunction. m: size of Count-min sketch array.\n2\n\ncount-min sketch accuracy\nUse A[h(x)] to estimate f (x).\nClaim 1: We always have A[h(x)] \u2265f (x). Why?\n\u2022 A[h(x)] counts the number of occurrences of any y with h(y) = h(x),\nincluding x itself.\n\u2022 A[h(x)] = f (x) + P\ny6=x:h(y)=h(x) f (y).\nf (x): frequency of x in the stream (i.e., number of items equal to x). h: random hash\nfunction. m: size of Count-min sketch array.\n2\n\u2467\n\ncount-min sketch accuracy\nA[h(x)] = f (x) +\nX\ny6=x:h(y)=h(x)\nf (y)\n|\n{z\n}\nerror in frequency estimate\n.\nExpected Error:\nE\n2\n4\nX\ny6=x:h(y)=h(x)\nf (y)\n3\n5 =\nX\ny6=x\nPr(h(y) = h(x)) \u00b7 f (y)\n\uf8ff\nX\ny6=x\n1\nm \u00b7 f (y) = 1\nm \u00b7 (n \u2212f (x)) \uf8ffn\nm\nWhat is a bound on probability that the error is \u22652n\nm ?\nMarkov\u2019s inequality: Pr\nhP\ny6=x:h(y)=h(x) f (y) \u22652n\nm\ni\n\uf8ff1\n2.\nWhat property of h is required to show this bound? a) fully random\nb)\npairwise independent\nc) 2-universal\nd) locality sensitive\nf (x): frequency of x in the stream (i.e., number of items equal to x). h: random hash\nfunction. m: size of Count-min sketch array.\n3\n\ncount-min sketch accuracy\nClaim: For any x, with probability at least 1/2,\nf (x) \uf8ffA[h(x)] \uf8fff (x) + 2n\nm .\nHow can we improve the success probability?Repetition.\nf (x): frequency of x in the stream (i.e., number of items equal to x). h: random hash\nfunction. m: size of Count-min sketch array.\n4\n\ncount-min sketch accuracy\n5\n\ncount-min sketch accuracy\n5\n\ncount-min sketch accuracy\n5\n\ncount-min sketch accuracy\nEstimate f (x) with \u02dcf (x) = mini2[t] Ai[hi(x)]. (count-min sketch)\n5\n\ncount-min sketch accuracy\nEstimate f (x) with \u02dcf (x) = mini2[t] Ai[hi(x)]. (count-min sketch)\nWhy min instead of mean or median? The minimum estimate is always\nthe most accurate since they are all overestimates of the true frequency!\n5\nA\n\ncount-min sketch analysis\nEstimate f (x) by \u02dcf (x) = mini2[t] Ai[hi(x)]\n\u2022 For each x and i 2 [t], with probability \u22651/2:\nf (x) \uf8ffAi[hi(x)] \uf8fff (x) + 2n\nm .\n\u2022 What is Pr[f (x) \uf8ff\u02dcf (x) \uf8fff (x) + 2n\nm ]?\n1 \u22121/2t.\n\u2022 To get a good estimate with probability \u22651 \u2212\u03b4, set t = log(1/\u03b4).\n6\n\ncount-min application\nAndrew McGregor\n0\nA\n\nrecap: count-min sketch\nEstimate the number of occurrences of x, i.e., f (x) by\n\u02dcf (x) = min\ni2[t] Ai[hi(x)]\nIf t = log(1/\u03b4) then\nPr[f (x) \uf8ff\u02dcf (x) \uf8fff (x) + 2n/m] \u22651 \u2212\u03b4\n1\n\n(\u270f, k)-frequent items problem\nGiven stream of n items x1, . . . , xn where each xi 2 U. Return a set F,\nsuch that for every x 2 U:\n1. If f (x) \u2265n/k then x 2 F\n2. If f (x) < (1 \u2212\u270f)n/k then x 62 F\nwhere f (x) is the number of times x appears in the stream.\nRelationship to Frequency Estimation. If m = 2k/\u270fthen\nf (x) \uf8ff\u02dcf (x) \uf8fff (x) + \u270fn/k\nand outputting x if \u02dcf (x) \u2265n/k ensures both requirements satis\ufb01ed.\nBut do we need to query all possible x 2 U? That\u2019s time consuming and\nincreases the chance we\u2019d make some mistakes.\n2\n\nidentifying frequent elements\nOne approach:\n\u2022 Maintain set F of elements, initially empty, while processing stream\n\u2022 At step i:\n\u2022 Add ith stream element to F if its estimated frequency is \u2265i/k and it\nisn\u2019t already in F.\n\u2022 Remove any element from F whose estimated frequency is < i/k.\n\u2022 Stores O(k) items at any one time and F includes all elements with\nestimated frequency \u2265n/k at the end.\nError Probability: We query the Count-Min sketch at t = O(nk) times\nso setting \u03b4 = 0.001/t ensures the probability that a query answer is\ninsu\ufb03ciently accurate is at most 0.001 (by the Union bound).\n3\n\ndistinct elements\nAndrew McGregor\n0\nAl5\n\u2193\n\ndistinct elements\nDistinct Elements (Count-Distinct) Problem: Given a stream\nx1, . . . , xn, estimate the number of distinct elements in the stream. E.g.,\n1, 5, 7, 5, 2, 1 ! 4 distinct elements\nApplications:\n\u2022 Distinct IP addresses clicking on an ad or visiting a site.\n\u2022 Number of distinct search engine queries.\n\u2022 Counting distinct motifs in large DNA sequences.\n\u2022 Implemented in Google Sawzall, Facebook Presto, Apache Drill,\nTwitter Algebird\n1\n\nhashing for distinct elements\nDistinct Elements Problem: Given a stream x1, . . . , xn, estimate the\nnumber of distinct elements.\nMin-Hashing for Distinct Elements (variant of Flajolet-Martin):\n\u2022 Let h : U ! [0, 1] be a random hash function (with real valued output)\n\u2022 s := 1\n\u2022 For i = 1, . . . , n\n\u2022 s := min(s, h(xi))\n\u2022 Return \u02c6d = 1\ns \u22121\n2\n\nhashing for distinct elements\nDistinct Elements Problem: Given a stream x1, . . . , xn, estimate the\nnumber of distinct elements.\nMin-Hashing for Distinct Elements (variant of Flajolet-Martin):\n\u2022 Let h : U ! [0, 1] be a random hash function (with real valued output)\n\u2022 s := 1\n\u2022 For i = 1, . . . , n\n\u2022 s := min(s, h(xi))\n\u2022 Return \u02c6d = 1\ns \u22121\n2\n\nhashing for distinct elements\nDistinct Elements Problem: Given a stream x1, . . . , xn, estimate the\nnumber of distinct elements.\nMin-Hashing for Distinct Elements (variant of Flajolet-Martin):\n\u2022 Let h : U ! [0, 1] be a random hash function (with real valued output)\n\u2022 s := 1\n\u2022 For i = 1, . . . , n\n\u2022 s := min(s, h(xi))\n\u2022 Return \u02c6d = 1\ns \u22121\n2\n\nhashing for distinct elements\nDistinct Elements Problem: Given a stream x1, . . . , xn, estimate the\nnumber of distinct elements.\nMin-Hashing for Distinct Elements (variant of Flajolet-Martin):\n\u2022 Let h : U ! [0, 1] be a random hash function (with real valued output)\n\u2022 s := 1\n\u2022 For i = 1, . . . , n\n\u2022 s := min(s, h(xi))\n\u2022 Return \u02c6d = 1\ns \u22121\n2\n\nhashing for distinct elements\nDistinct Elements Problem: Given a stream x1, . . . , xn, estimate the\nnumber of distinct elements.\nMin-Hashing for Distinct Elements (variant of Flajolet-Martin):\n\u2022 Let h : U ! [0, 1] be a random hash function (with real valued output)\n\u2022 s := 1\n\u2022 For i = 1, . . . , n\n\u2022 s := min(s, h(xi))\n\u2022 Return \u02c6d = 1\ns \u22121\n2\n\nhashing for distinct elements\nDistinct Elements Problem: Given a stream x1, . . . , xn, estimate the\nnumber of distinct elements.\nMin-Hashing for Distinct Elements (variant of Flajolet-Martin):\n\u2022 Let h : U ! [0, 1] be a random hash function (with real valued output)\n\u2022 s := 1\n\u2022 For i = 1, . . . , n\n\u2022 s := min(s, h(xi))\n\u2022 Return \u02c6d = 1\ns \u22121\n2\n\nhashing for distinct elements\nDistinct Elements Problem: Given a stream x1, . . . , xn, estimate the\nnumber of distinct elements.\nMin-Hashing for Distinct Elements (variant of Flajolet-Martin):\n\u2022 Let h : U ! [0, 1] be a random hash function (with real valued output)\n\u2022 s := 1\n\u2022 For i = 1, . . . , n\n\u2022 s := min(s, h(xi))\n\u2022 Return \u02c6d = 1\ns \u22121\n2\n\nperformance in expectation\n\u2022 s is the minimum of d points chosen uniformly at random on [0, 1].\n\u2022 Can show via calculus that E[s] =\n1\nd+1 since E(s) =\nR 1\n0\nPr(s > x)dx\nand similarly that Var[s] \uf8ff1/(d + 1)2.\n\u2022 Hence, d = 1/E[s] \u22121. Does s \u21e1E[s] imply bd = 1/s \u22121 \u21e1d?\n\u2022 Exercise: For any \u270f2 (0, 1/2), of\n|s \u2212E[s]| \uf8ff\u270f\u00b7 E[s] =) (1 \u22124\u270f)d \uf8ffbd \uf8ff(1 + 4\u270f)d\n3\n\nconcentration bound\nHow well is s concentrates around its mean?\nE[s] =\n1\nd + 1 and Var[s] \uf8ff\n1\n(d + 1)2\nNow apply the Median Trick!\n\u2022 Repeat process r = r1r2 times in parallel.\n\u2022 Partition the estimates into r1 groups of size r2. Let si be the average\nof the ith group of estimates:\nPr [|si \u2212E[s]| \u2265\u270fE[s]] \uf8ffVar[si]\n(\u270fE[s])2 \uf8ffVar[s]/r2\n(\u270fE[s])2 = 1/4\nwhere the last step follow if r2 = 4/\u270f2.\n\u2022 Using Median Trick, if r1 = 12 log(2\u03b4\u22121) then median(s1, . . . , sr1) is\nbetween 1\u2212\u270f\nd+1 and 1+\u270f\nd+1 with probability at least 1 \u2212\u03b4.\n4\n\ndistinct elements in practice\nAndrew McGregor\n0\nA16\n\u2193\n\ndistinct elements in practice\nOur algorithm uses continuous valued fully random hash functions. Can\u2019t\nbe implemented...\n\u2022 The idea of using the minimum hash value of x1, . . . , xn to estimate\nthe number of distinct elements naturally extends to when the hash\nfunctions map to discrete values.\n\u2022 Flajolet-Martin (LogLog) algorithm and HyperLogLog.\nEstimate # distinct elements based\non max number of trailing zeros m.\nThe more distinct hashes we see, the\nhigher we expect the max to be.\n1\n\nloglog counting of distinct elements\nFlajolet-Martin (LogLog) algorithm and HyperLogLog.\nEstimate # distinct elements based on\nmaximum number of trailing zeros m.\nWith d distinct elements, roughly what do we expect m to be?\nPr(h(xi) has log d trailing zeros) =\n1\n2log d = 1\nd .\nSo with d distinct hashes, expect to see 1 with log d trailing zeros. Expect\nm \u21e1log d. m takes log log d bits to store.\nTotal Space: O\n! log log d\n\u270f2\n+ log d\n\"\nfor an \u270fapproximate count.\nNote: Careful averaging of estimates from multiple hash functions.\n2\n\nloglog space guarantees\nUsing HyperLogLog to count 1 billion distinct items with 2% accuracy:\nspace used = O\n\u2713log log d\n\u270f2\n+ log d\n\u25c6\n= 1.04 \u00b7 dlog2 log2 de\n\u270f2\n+ dlog2 de bits1\n= 1.04 \u00b7 5\n.022\n+ 30 = 13030 bits \u21e11.6 kB!\nMergeable Sketch: Consider the case (essentially always in practice) that the\nitems are processed on di\u21b5erent machines.\n\u2022 Given data structures (sketches) HLL(x1, . . . , xn), HLL(y1, . . . , yn) it is easy\nto merge them to give HLL(x1, . . . , xn, y1, . . . , yn). How?\n\u2022 Set the maximum # of trailing zeros to the maximum in the two sketches.\n1. 1.04 is the constant in the HyperLogLog analysis. Not important!\n3\n\nSummary\nSlides\n112-A16 :\n\n\nlocality sensitive hashing i\nAndrew McGregor\n0\nAl7\n\u2193\n\nanother fundamental problem\nJaccard Index: A similarity measure between two sets.\nJ(A, B) = |A \\ B|\n|A [ B| = # shared elements\n# total elements .\nAlso a natural measure for similarity between bit strings: interpret an n\nbit string as a set, containing the elements corresponding the positions of\nits ones. J(x, y) = # shared ones\ntotal ones\n.\n1\n\nsearch with jaccard similarity\nJ(A, B) = |A \\ B|\n|A [ B| = # shared elements\n# total elements .\nWant Fast Implementations For:\n\u2022 Near Neighbor Search: Have a database of n sets/bit strings and\ngiven a set A, want to \ufb01nd if it has high Jaccard similarity to anything\nin the database. \u2326(n) time with a linear scan.\n\u2022 All-pairs Similarity Search: Have n di\u21b5erent sets/bit strings and\nwant to \ufb01nd all pairs with high Jaccard similarity. \u2326(n2) time if we\ncheck all pairs explicitly.\nWill speed up via randomized locality sensitive hashing.\n2\n\napplications\nDocument Similarity:\n\u2022 E.g., detecting plagiarism, copyright infringement, spam.\n\u2022 Use Shingling (aka n-grams or k-mers) + Jaccard similarity.\n3\n\napplication: collaborative filtering\nOnline recommendation systems are often based on collaborative \ufb01ltering.\nSimplest approach: \ufb01nd similar users and make recommendations based on\nthose users.\n\u2022 Twitter: represent a user as the set of accounts they follow. Match similar\nusers based on the Jaccard similarity of these sets. Recommend that you\nfollow accounts followed by similar users. Net\ufb02ix: look at sets of movies\nwatched. Amazon: look at products purchased, etc.\n4\n\napplication: entity resolution\nEntity Resolution Problem: Want to combine records from multiple data\nsources that refer to the same entities.\n\u2022 E.g., data on individuals from voting registrations, property records, and\nsocial media accounts. Names and addresses may not exactly match, due to\ntypos, nicknames, moves, etc.\n\u2022 Still want to match records that all refer to the same person using all pairs\nsimilarity search.\nSee Section 3.8.2 of Mining Massive Datasets for a discussion of a real world\nexample involving 1 million customers. Naively this would be\n!1000000\n2\n\"\n\u21e1500\nbillion pairs of customers to check!\n5\n\napplication: spam and fraud detection\nMany applications to spam/fraud detection. E.g.\n\u2022 Fake Reviews: Common on websites like Amazon. Detection often\nlooks for (near) duplicate reviews on similar products, which have been\ncopied. \u2018Near duplicate\u2019 measured with shingles + Jaccard similarity.\n\u2022 Lateral phishing: Phishing emails sent to addresses at a business\ncoming from a legitimate email address at the same business that has\nbeen compromised.\n\u2022 One method of detection looks at the recipient list of an email and checks\nif it has small Jaccard similarity with any previous recipient lists. If not,\nthe email is \ufb02agged as possible spam.\n6\n\nlocality sensitive hashing\nA hash function is a locality sensitive hash function if the collision\nprobability is higher when two inputs are more similar (can design\ndi\u21b5erent functions for di\u21b5erent similarity metrics.)\n7\n\nlsh for similarity search\nHow does locality sensitive hashing help for similarity search?\n\u2022 Near Neighbor Search: Given item x, compute h(x). Only search for\nsimilar items in the h(x) bucket of the hash table.\n\u2022 All-pairs Similarity Search: Scan through all buckets of the hash\ntable and look for similar pairs within each bucket.\n8\n\nlocality sensitive hashing 2\nAndrew McGregor\n0\nAIS\n\u2193\n\nminhashing\nGoal: Speed up Jaccard similarity search.\nStrategy: Use random hashing to map each set to a very compressed\nrepresentation. Jaccard similarity can be estimated from these.\nMinHash(A): [Andrei Broder, 1997 at Altavista]\n\u2022 Let h : U ! [0, 1] be a random hash\nfunction\n\u2022 s := 1\n\u2022 For x1, . . . , x|A| 2 A\n\u2022 s := min(s, h(xk))\n\u2022 Return s\nIdentical to our distinct elements sketch!\n1\n\nminhash\nFor two sets A and B, what is Pr(MinHash(A) = MinHash(B))?\n\u2022 Since we are hashing into the continuous range [0, 1], we will never\nhave h(x) = h(y) for x 6= y (i.e., no spurious collisions)\n\u2022 MH(A) = MH(B) i\u21b5an item in A \\ B has the minimum hash value in\nboth sets. Therefore,\nPr(MH(A) = MH(B)) =\nX\nx2A\\B\nPr(MH(A) = h(x) \\ MH(B) = h(x))\n=\nX\nx2A\\B\nPr(x = arg min\ny2A[B\nh(y))\n=\nX\nx2A\\B\n1\n|A [ B| = |A \\ B|\n|A [ B| = J(A, B)\n2\n\nlsh with minhash\nGoal: Given a document y, identify all documents x in a database with\nJaccard similarity (of their shingle sets) J(x, y) \u22651/2.\nOur Approach:\n\u2022 Create a hash table of size m, choose a random hash function\ng : [0, 1] ! [m], and insert each item x into bucket g(MH(x)). Search\nfor items similar to y in bucket g(MH(y)).\n\u2022 What is Pr [g(MH(z)) = g(MH(y))] assuming J(z, y) \uf8ff1/3 and g is\ncollision free? At most 1/3\n\u2022 For each document x in your database with J(x, y) \u22651/2 what is the\nprobability you will \ufb01nd x in bucket g(MH(y))? At least 1/2\n3\n\nreducing false negatives\nWith a simple use of MinHash, we miss a match x with J(x, y) = 1/2 with\nprobability 1/2. How can we reduce this false negative rate?\nRepetition: Run MinHash t times independently, to produce hash values\nMH1(x), . . . , MHt(x). Apply random hash function g to map all these values to\nlocations in t hash tables.\n\u2022 To search for items similar to y, look at all items in bucket g(MH1(y)) of\nthe 1st table, bucket g(MH2(y)) of the 2nd table, etc.\n\u2022 What is the probability that x with J(x, y) = 1/2 is in at least one of these\nbuckets, assuming for simplicity g has no collisions?\n1\u2212(probability in no buckets) = 1 \u2212\n! 1\n2\n\"t \u21e1.99 for t = 7.\n\u2022 What is the probability that x with J(x, y) = 1/4 is in at least one of these\nbuckets, assuming for simplicity g has no collisions?\n1\u2212(probability in no buckets) = 1 \u2212\n! 3\n4\n\"t \u21e1.87 for t = 7.\nPotential for a lot of false positives! Slows down search time.\n4\n\nbalancing hit rate and query time\nWe want to balance a small probability of false negatives (a high hit rate) with\na small probability of false positives (a small query time.)\nCreate t hash tables. Each is indexed into not with a single MinHash value, but\nwith r values, appended together. A length r signature.\n5\n\nbalancing hit rate and query time\nConsider searching for matches in t hash tables, using MinHash signatures of\nlength r. For x and y with Jaccard similarity J(x, y) = s:\n\u2022 Probability that a single hash matches.\nPr [MHi,j(x) = MHi,j(y)] = J(x, y) = s.\n\u2022 Probability that x and y having matching signatures in repetition i.\nPr [MHi,1(x), . . . , MHi,r(x) = MHi,1(y), . . . , MHi,r(y)] = sr.\n\u2022 Probability that x and y don\u2019t match in repetition i: 1 \u2212sr.\n\u2022 Probability that x and y don\u2019t match in all repetitions: (1 \u2212sr)t.\n\u2022 Probability that x and y match in at least one repetition:\nHit Probability: 1 \u2212(1 \u2212sr)t.\n6\n\nlocality sensitive hashing 3\nAndrew McGregor\n01\n\nthe s-curve\nUsing t repetitions each with a signature of r MinHash values, the probability\nthat x and y with Jaccard similarity J(x, y) = s match in at least one\nrepetition is: 1 \u2212(1 \u2212sr)t.\n0\n0.2\n0.4\n0.6\n0.8\n1\nJaccard Similarity s\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nHit Probability\nr = 5, t = 30\nr and t are tuned depending on application. One approach: set t as large as\nyou can a\u21b5ord (this will increase the gradient around the cut-o\u21b5) and then\ntweak r such that the cut-o\u21b5appears in the right place.\n1\n\ns-curve example\nFor example: Consider a database with 10, 000, 000 audio clips. You are given\na clip x and want to \ufb01nd any y in the database with J(x, y) \u2265.9.\n\u2022 There are 10 true matches in the database with J(x, y) \u2265.9.\n\u2022 There are 10, 000 near matches with J(x, y) 2 [.7, .9].\nWith signature length r = 25 and repetitions t = 50, hit probability for\nJ(x, y) = s is 1 \u2212(1 \u2212s25)50.\n\u2022 Hit probability for J(x, y) \u2265.9 is \u22651 \u2212(1 \u2212.925)50 \u21e1.98\n\u2022 Hit probability for J(x, y) 2 [.7, .9] is \uf8ff1 \u2212(1 \u2212.925)50 \u21e1.98\n\u2022 Hit probability for J(x, y) \uf8ff.7 is \uf8ff1 \u2212(1 \u2212.725)50 \u21e1.007\nExpected Number of Items Scanned: (proportional to query time)\n\uf8ff10 + .98 \u21e410, 000 + .007 \u21e49, 989, 990 \u21e180, 000 \u232710, 000, 000.\n2\n\ngeneralizing locality sensitive hashing\nRepetition and s-curve tuning can be used for fast similarity search with other\nsimilarity metrics:\n\u2022 LSH schemes exist for many similarity/distance measures: hamming\ndistance, cosine similarity, etc.\nCosine Similarity: cos(\u2713(x, y)) =\nhx,yi\nkxk2\u00b7kyk2 .\n\u2022 cos(\u2713(x, y)) = 1 when \u2713(x, y) = 0\u25e6and cos(\u2713(x, y)) = 0 when\n\u2713(x, y) = 90\u25e6, and cos(\u2713(x, y)) = \u22121 when \u2713(x, y) = 180\u25e6\n3\n\nsimhash for cosine similarity\nSimHash Algorithm: LSH for cosine similarity.\n4\n\nsimhash for cosine similarity\nSimHash Algorithm: LSH for cosine similarity.\nSimHash(x) = sign(hx, ti) for a random vector t.\nWhat is Pr [SimHash(x) = SimHash(y)]?\n4\n\nsimhash for cosine similarity\nWhat is Pr [SimHash(x) = SimHash(y)]?\nSimHash(x) 6= SimHash(y) when the plane separates x from y.\n\u2022 Pr [SimHash(x) 6= SimHash(y)] = \u2713(x,y)\n180\n\u2022 Pr [SimHash(x) = SimHash(y)] = 1 \u2212\u2713(x,y)\n180\n\u21e1cos \u2713for small \u2713.\n5\n\ndimensionality reduction\nAndrew McGregor\n0\nA 20\n\u2193\n\nhigh dimensional data\n\u201cBig Data\u201d means not just many data points, but many measurements per\ndata point. I.e., very high dimensional data.\n\u2022 Twitter has 321 million active monthly users. Records (tens of) thousands of\nmeasurements per user: who they follow, who follows them, when they last\nvisited the site, timestamps for speci\ufb01c interactions, how many tweets they\nhave sent, the text of those tweets, etc.\n\u2022 A 3 minute Youtube clip with a resolution of 500 \u21e5500 pixels at 15\nframes/second with 3 color channels is a recording of \u22652 billion pixel values.\nEven a 500 \u21e5500 pixel color image has 750, 000 pixel values.\n\u2022 The human genome contains 3 billion+ base pairs. Genetic datasets often\ncontain information on 100s of thousands+ mutations and genetic markers.\n1\n\ndata as vectors and matrices\nIn data analysis and machine learning, data points with many attributes\nare often stored, processed, and interpreted as high dimensional vectors,\nwith real valued entries.\nSimilarities/distances between vectors (e.g.,\nhx, yi, kx \u2212yk2) have meaning for\nunderlying data points.\n2\n\ndatasets as vectors and matrices\nData points are interpreted as high dimensional vectors, with real valued\nentries. Data set is interpreted as a matrix.\nData Points: ~x1, ~x2, . . . , ~xn 2 Rd.\nData Set: X 2 Rn\u21e5d with ith rows equal to ~xi.\nMany data points n =) tall. Many dimensions d =) wide.\n3\n\nlow distortion embedding\nLow Distortion Embedding: Given ~x1, . . . , ~xn 2 Rd, distance function D, and\nerror parameter \u270f\u22650, \ufb01nd \u02dcx1, . . . , \u02dcxn 2 Rm (where m \u2327d) and distance\nfunction \u02dcD such that for all i, j 2 [n]:\n(1 \u2212\u270f)D(~xi, ~xj) \uf8ff\u02dcD(\u02dcxi, \u02dcxj) \uf8ff(1 + \u270f)D(~xi, ~xj).\nWe\u2019ll focus on the case where D and \u02dcD are Euclidean distances. I.e., the\ndistance between two vectors x and y is de\ufb01ned as\nk~x \u2212~yk2 =\nsX\ni\n(~x(i) \u2212~y(i))2\nThis is related to the Euclidean norm, k~zk2 =\npPn\ni=1 ~z(i)2.\n4\n\nthe johnson-lindenstrauss lemma\nJohnson-Lindenstrauss Lemma: For any set of points ~x1, . . . , ~xn 2 Rd\nand \u270f> 0 there exists a linear map M : Rd ! Rm such that m =\nO\n% log n\n\u270f2\n&\nand letting \u02dcxi = M~xi:\nFor all i, j : (1 \u2212\u270f)k~xi \u2212~xjk2 \uf8ffk\u02dcxi \u2212\u02dcxjk2 \uf8ff(1 + \u270f)k~xi \u2212~xjk2.\nFurther, if M 2 Rm\u21e5d has each entry chosen independently from\nN(0, 1/m), it satis\ufb01es the guarantee with high probability.\nFor d = 1 trillion, \u270f= .05, and n = 100, 000, m \u21e16600.\nVery surprising! Powerful result with a simple construction: applying a random\nlinear transformation to a set of points preserves distances between all those\npoints with high probability.\n5\n\nrandom projection\nFor any ~x1, . . . , ~xn and M 2 Rm\u21e5d with each entry chosen independently from\nN(0, 1/m), with high probability, letting \u02dcxi = M~xi:\nFor all i, j : (1 \u2212\u270f)k~xi \u2212~xjk2 \uf8ffk\u02dcxi \u2212\u02dcxjk2 \uf8ff(1 + \u270f)k~xi \u2212~xjk2.\n\u2022 M is known as a random projection. It is a random linear function, mapping\nlength d vectors to length m vectors.\n\u2022 M is data oblivious. Stark contrast to methods like PCA.\n6\n\nalgorithmic considerations\n\u2022 Alternative constructions: \u00b11 entries, sparse (most entries 0), Fourier\nstructured, etc. =) e\ufb03cient computation of \u02dcxi = M~xi.\n\u2022 Data oblivious property means that once M is chosen, \u02dcx1, . . . ,\u02dcxn can\nbe computed in a stream with little memory.\n\u2022 Storage is just O(nm) rather than O(nd).\n\u2022 Compression can be performed in parallel on di\u21b5erent servers.\n\u2022 When new data points are added, can be easily compressed, without\nupdating existing points.\n7\n\njohnson lindenstrauss: part 1\nAndrew McGregor\n0\nA21\n\u2193\n\ndistributional jl\nThe Johnson-Lindenstrauss Lemma is a direct consequence of:\nDistributional JL Lemma: Let M 2 Rm\u21e5d have each entry chosen\ni.i.d. as N(0, 1/m). If we set m = O\n\u21e3\nlog(1/\u03b4)\n\u270f2\n\u2318\n, then for any ~y 2 Rd,\nwith probability \u22651 \u2212\u03b4\n(1 \u2212\u270f)k~yk2 \uf8ffkM~yk2 \uf8ff(1 + \u270f)k~yk2\nI.e., applying a random matrix M to any vector ~y preserves the norm with high\nprobability. Like a low-distortion embedding, but for the length of a compressed\nvector rather than distances between vectors.\n2\n\ndistributional jl =) jl\nDistributional JL Lemma =) JL Lemma: Distributional JL show that a\nrandom projection M preserves the norm of any y. The main JL Lemma says\nthat M preserves distances between vectors. Since M is linear these are the\nsame thing!\nProof: Given x1, . . . , xn, de\ufb01ne\n#n\n2\n$\nvectors yij where yij = xi \u2212xj.\n\u2022 If we choose M with m = O\n#\n\u270f\u22122log 1/\u03b40$\n, for each yij with probability at\nleast 1 \u2212\u03b40 we have:\n(1 \u2212\u270f)kxi \u2212xjk2 \uf8ffkMxi \u2212Mxjk2 \uf8ff(1 + \u270f)kxi \u2212xjk2\n\u2022 Union Bound: Every distance preserved with probability 1 \u2212\n#n\n2\n$\n\u00b7 \u03b40.\n\u2022 Setting \u03b40 = \u03b4/\n#n\n2\n$\nensures all distances preserved with probability 1 \u2212\u03b4 and\nm = O\n\u2713log(1/\u03b40)\n\u270f2\n\u25c6\n= O\n \nlog(\n#n\n2\n$\n/\u03b4)\n\u270f2\n!\n= O\n\u2713log(n/\u03b4)\n\u270f2\n\u25c6\n3\n\njohnson lindenstrauss: part 2\nAndrew McGregor\n0\nA22\n\u2193\n\ndistributional jl proof (part 1 of 3)\nDistributional JL Lemma: Let M 2 Rm\u21e5d have independent N(0, 1/m)\nentries. If we set m = O\n\u21e3\nlog(1/\u03b4)\n\u270f2\n\u2318\n, then for any y 2 Rd, with probability\nat least 1 \u2212\u03b4\n(1 \u2212\u270f)kyk2 \uf8ffkMyk2 \uf8ff(1 + \u270f)kyk2.\n\u2022 Let \u02dcy = My and Mj be the jth row of M\n\u2022 For any j, \u02dcyj = hMj, yi = Pd\ni=1 gi \u00b7 yi where gi \u21e0N(0, 1/m).\n\u2022 By linearity of expectation:\nE[\u02dcyj] =\nd\nX\ni=1\nE[gi] \u00b7 yi = 0 .\n\u2022 Since E[\u02dcyj] = 0 we have E[\u02dcy 2\nj ] = Var[\u02dcyj]. Then, by linearity of variance:\nE[\u02dcy 2\nj ] = Var[\u02dcyj] =\nd\nX\ni=1\nVar[gi \u00b7 yi] =\nX\ni\ny 2\ni /m = kyk2\n2/m .\n\u2022 Hence E[k\u02dcyk2\n2] = E[P\nj \u02dcy 2\nj ] = kyk2\n2. Remains to show k\u02dcyk2\n2 is concentrated.\n2\n\ndistributional jl proof (part 2 of 3)\nStability of Gaussian Random Variables. For independent a \u21e0\nN(\u00b51, \u03c32\n1) and b \u21e0N(\u00b52, \u03c32\n2) we have:\na + b \u21e0N(\u00b51 + \u00b52, \u03c32\n1 + \u03c32\n2)\nLetting \u02dcy = My, we have:\n\u02dcyj =\nd\nX\ni=1\ngi \u00b7 yi where gi \u00b7 yi \u21e0N(0, y 2\ni /m).\nThus, \u02dcyj \u21e0N(0, Pd\ni=1 y 2\ni /m) = N(0, kyk2\n2/m).\n3\n\ndistributional jl proof (part 3 of 3)\nSo Far: Each entry of our compressed vector \u02dcy is Gaussian with :\n\u02dcyj \u21e0N(0, kyk2\n2/m) and E[k\u02dcyk2\n2] = kyk2\n2\nk\u02dcyk2\n2 = Pm\ni=1 \u02dcy 2\nj a Chi-Squared random variable with m degrees of freedom (a\nsum of m squared independent Gaussians)\nLemma: (Chi-Squared Concentration) Letting Z be a Chi-Squared ran-\ndom variable with m degrees of freedom,\nPr [|Z \u2212EZ| \u2265\u270fEZ] \uf8ff2e\u2212m\u270f2/8.\nIf we set m = 8\u270f\u22122 log(2/\u03b4) and Z = k\u02dcyk2\n2, then Chi-Squared Concentration\nimplies that with probability at least 1 \u2212\u03b4:\n(1 \u2212\u270f)kyk2\n2 \uf8ffk\u02dcyk2\n2 \uf8ff(1 + \u270f)kyk2\n2.\n4\n\njohnson lindenstrauss: part 3\nAndrew McGregor\n0\n\u21b3\n\nexample application: k-means clustering\nGoal: Separate n points in d dimensional space into k groups C1, . . . , Ck.\nk-means Objective: Cost(C1, . . . , Ck) =\nk\nX\nj=1\nX\n~x2Cj\nk~x \u2212\u00b5jk2\n2 where\n\u00b5j =\n1\n|Cj|\nX\n~x2Cj\n~x\nis the average of the points in Cj.\nExercise: Can be rewritten as Cost(C1, . . . , Ck) =\nk\nX\nj=1\nX\n~x1,~x22Cj\nk~x1 \u2212~x2k2\n2\n|Cj|\n1\n\nexample application: k-means clustering\nk-means Objective: Cost(C1, . . . , Ck) = Pk\nj=1\nP\n~x1,~x22Cj\nk~x1\u2212~x2k2\n2\n|Cj |\nIf we randomly project to m = O\n#\n\u270f\u22122 log n\n$\ndimensions, for all pairs ~x1, ~x2,\n(1 \u2212\u270f)k~x1 \u2212~x2k2\n2 \uf8ffk\u02dcx1 \u2212\u02dcx2k2\n2 \uf8ff(1 + \u270f)k~x1 \u2212~x2k2\n2\nLetting Cost(C1, . . . , Ck) = Pk\nj=1\nP\n~x1,~x22Cj\nk\u02dcx1\u2212\u02dcx2k2\n2\n|Cj |\n(1 \u2212\u270f)Cost(C1, . . . , Ck) \uf8ffCost(C1, . . . , Ck) \uf8ff(1 + \u270f)Cost(C1, . . . , Ck).\nUpshot: Can cluster in m dimensional space (much more e\ufb03ciently) and\nminimize Cost(C1, . . . , Ck).\n2\n\njl lemma is almost optimal\n\northogonal vectors\n\u2022 Recall that we say two vectors x, y are orthogonal if hx, yi = 0.\n\u2022 What is the largest set of mutually orthogonal unit vectors in\nd-dimensional space? Answer: d.\n\u2022 How large can a set of unit vectors in d-dimensional space be that\nhave all pairwise dot products |hx, yi| \uf8ff\u270f? Answer: 2\u2326(\u270f2d).\nAn exponentially large set of random vectors will be nearly pairwise\northogonal with high probability!\n4\n\northogonal vectors proof\nClaim: 2O(\u270f2d) random d-dimensional unit vectors will have all pairwise\ndot products |hx, yi| \uf8ff\u270f(be nearly orthogonal).\nProof: Let x1, . . . , xt 2 Rd have independent random entries \u00b1 1\np\nd .\n\u2022 What is kxik2? Every xi is always a unit vector.\n\u2022 What is E[hxi, xji]? E[hxi, xji] = 0\n\u2022 By a Bernstein bound, Pr[|hxi, xji| \u2265\u270f] \uf8ff2e\u2212\u270f2d/6.\n\u2022 If t = 1\n2e\u270f2d/12, using a union bound over\n!t\n2\n\"\n\uf8ff1\n8e\u270f2d/6 possible pairs,\nwith probability \u22653/4 all will be nearly orthogonal.\nWe won\u2019t prove it but this is essentially optimal: In d dimensions, there\ncan be at most 2O(\u270f2d) nearly orthogonal unit vectors.\n5\n\nconnection to dimensionality reduction\nRecall: The Johnson Lindenstrauss lemma states that if M 2 Rm\u21e5d is a\nrandom matrix (linear map) with m = O\n\u21e3\nlog n\n\u270f2\n\u2318\n, for x1, . . . , xn 2 Rd with\nhigh probability, for all i, j:\n(1 \u2212\u270f)kxi \u2212xjk2\n2 \uf8ffkMxi \u2212Mxjk2\n2 \uf8ff(1 + \u270f)kxi \u2212xjk2\n2.\nImplies: If x1, . . . , xn are nearly orthogonal unit vectors in d-dimensions\n(with pairwise dot products bounded by \u270f/8), then\nMx1\nkMx1k2\n, . . . ,\nMxn\nkMxnk2\nare nearly orthogonal unit vectors in m-dimensions (with pairwise dot\nproducts bounded by \u270f). Algebra is a bit messy but a good exercise to\npartially work through. Proof uses the fact that\nkx \u2212yk2\n2 = kxk2\n2 + kyk2\n2 \u22122hx, yi .\n6\n\nconnection to dimensionality reduction\nClaim 1: n nearly orthogonal unit vectors can be projected to\nm = O\n\u21e3\nlog n\n\u270f2\n\u2318\ndimensions and still be nearly orthogonal.\nClaim 2: In m dimensions, there can be at most 2O(\u270f2m) nearly\northogonal unit vectors.\n\u2022 For both of these to hold it must be that n \uf8ff2O(\u270f2m).\n\u2022 I.e., n = 2log n \uf8ff2O(\u270f2m) and so m = \u2326\n\u21e3\nlog n\n\u270f2\n\u2318\n.\n\u2022 Tells us that the JL lemma is optimal up to constants.\n7\n\npart ii: overview\nAndrew McGregor\n0\no\n\ndatasets as vectors and matrices\nData points are interpreted as high dimensional vectors, with real valued\nentries. Data set is interpreted as a matrix.\nData Points: ~x1, ~x2, . . . , ~xn 2 Rd.\nData Set: X 2 Rn\u21e5d with ith rows equal to ~xi.\nMany data points n =) tall. Many dimensions d =) wide.\n1\n\nlow rank embedding\n\nwhen can we better than jl?\n\u2022 Goal of JL was to reduce the dimension of data points such that\nproperties of the original data set were preserved.\n\u2022 In JL, the compression is linear, i.e., by applying a matrix. The matrix\nwas chosen randomly and without regard to the data set.\n\u2022 What if we chose matrix taking into account structure of dataset? Can\ngive better compression than random projection?\n\u2022 Short Answer: There\u2019s a lot we can do if there\u2019s a structure in the data.\nThis section will be need a lot of linear algebra. Today we\u2019ll use:\n\u2022 A set of vectors B is a basis for a set of vectors A, if every vector in A\nis a linear combination of vectors in B.\n\u2022 The dimension of A is the size of its smallest basis.\n3\n\nexamples of data having structure\n\u2022 Data points might be approximately reconstructed from a basis of\nk \u2327d vectors.\n4\n\ndual view of low-rank approximation\n\u2022 The columns of X might be approximately spanned by k vectors.\n5\n\nsingular value decomposition (svd)\nAny matrix X 2 Rn\u21e5d can be written as X = U\u2303VT.\n\u2022 U has orthonormal columns ~u1, . . . , ~ur 2 Rn (left singular vectors).\n\u2022 V has orthonormal columns ~v1, . . . , ~vr 2 Rd (right singular vectors).\n\u2022 \u2303is diagonal with elements \u03c31 \u2265\u03c32 \u2265. . . \u2265\u03c3r > 0 (singular values).\nThe \u2018swiss army knife\u2019 of modern linear algebra.\n7\n\napplications of low-rank approximation\n\u2022 We\u2019ll look at applications including Low-Rank Matrix Completion and\nLatent Semantic Analysis.\n\u2022 Classic example of Matrix Completion: the Net\ufb02ix prize problem. The\nentries of the data matrix correspond to ratings that di\u21b5erent users\nhave given to di\u21b5erent movies. But the data matrix has missing entries!\n\u2022\n8\n\nspectral clustering\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nCommunity detection in naturally occurring networks.\n10\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nCommunity detection in naturally occurring networks.\n10\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nLinearly separable data.\n10\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nNon-linearly separable data k-nearest neighbor graph.\n10\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nNon-linearly separable data k-nearest neighbor graph.\n10\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nNon-linearly separable data k-nearest neighbor graph.\n10\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nNon-linearly separable data k-nearest neighbor graph.\nCan \ufb01nd this cut using eigenvectors!\n10\n\nthe laplacian matrix\nFor a graph with adjacency matrix A and degree matrix D, L = D \u2212A is the\ngraph Laplacian.\nFor any vector ~v, its \u2018smoothness\u2019 over the graph is given by:\nX\n(i,j)2E\n(~v(i) \u2212~v(j))2 = ~v TL~v.\nSpectral clustering is related to \ufb01nding a particular eigenvector of the Laplacian\nmatrix and then partitioning the nodes of based on the values in this vector.\n11\n\nstochastic block model\n\nstochastic block model\n\u2022 For many algorithms in data science, such as spectral clustering, there\nare no worst-case guarantees.\n\u2022 Common Approach:\n\u2022 Propose a natural randomized way the input could have been generated.\n\u2022 Analyze how the algorithm performs on inputs generated in this way.\nIs used to justify `2 linear regression, k-means and spectral clustering...\n13\n\nstochastic block model\nStochastic Block Model (Planted Partition Model): Split n nodes\nrandomly into two groups B and C, each with n/2 nodes.\n\u2022 Node pairs in the same group are connected with probability p.\n\u2022 Node pairs in di\u21b5erent groups are connected with probability q < p.\n\u2022 Connections are independent.\nWe\u2019ll see whether spectral clustering can recover A and B from the graph.\n14\n\nfinding eigenvectors\n\npower method\n\u2022 Power Method: The most fundamental iterative method for \ufb01nding\nthe eigenvectors that arise in many of the above applications.\n\u2022 Goal: Given symmetric A 2 Rd\u21e5d \ufb01nd ~z such that\nA~z \u21e1\u03bb~z\nfor some large \u03bb.\n\u2022 Algorithm:\n\u2022 Initialize: Pick random starting vector ~z(0).\n\u2022 Loop: For i = 1, . . . , t\n\u2022 ~z(i) := A \u00b7 ~z(i\u22121)\n\u2022 ~zi :=\n~z(i)\nk~z(i)k2\n\u2022 Return ~zt\nWill show (under mild conditions) that as t ! 1, the output converges\nto the desired output. We will also analyze the rate of convergence.\n16\n\nbasic linear algebra definitions\nAndrew McGregor\n0i\n\nvector definitions\n\u2022 The Euclidean norm of a vector v 2 Rd is de\ufb01ned as\nkvk2 =\nsX\nj\nv 2\nj .\n\u2022 The dot product (or \u201cscalar product\u201d) between u, v 2 Rd is de\ufb01ned as\nhu, vi =\nX\nj2[d]\nujvj\nand note kvk2\n2 = hv, vi.\n\u2022 A set of vectors {v1, v2, v3, . . .} is orthonormal if\nhvi, vji =\n(\n1\nif i = j\n0\nif i 6= j\n1\n\nbasis and dimension\n\u2022 u 2 Rn is in the span of a set of vectors B = {v1, v2, . . . , vk} if u can\nbe written as a linear combination of vectors in B, i.e.,\nu = \u21b51v1 + \u21b52v2 + . . . \u21b5kvk\nfor some \u21b51, . . . , \u21b5k 2 R.\n\u2022 Given two sets of vectors A and B, say B is a basis for A if every u 2 A\nis in the span of B. A \ufb01nite set B could be a basis for an in\ufb01nite set A.\n\u2022 The dimension of A is the size of its smallest basis.\n2\n\nmatrix multiplication\n\u2022 If A 2 R`\u21e5m and B 2 Rm\u21e5n are matrices then the (i, j)-th entry of the\nproduct C = AB 2 R`\u21e5n is\nCi,j =\nX\nh2[m]\nAi,hBh,j .\n\u2022 The rows of C are linear combinations of the rows of B. The columns\nof C are linear combinations of the columns of A.\n3\n\neigenvalues and eigenvectors\n\u2022 Given a square matrix A 2 Rn\u21e5n, we say v 2 Rn is an eigenvector of A\nwith eigenvalue \u03bb 2 R if\nAv = \u03bbv\n\u2022 The trace of a square matrix is the sum of the diagonal entries. It can\nalso be shown that this equals the sum of the eigenvalues of the matrix.\n\u2022 Eigendecomposition: If A is a n \u21e5n symmetric matrix, it has n\northonormal eigenvectors. It can be written as\nA = VDV T\nwhere the columns of V are the orthonormal eigenvectors and D is a\ndiagonal matrix whose ith diagonal entry is the the eigenvalue\ncorresponding to the ith eigenvector.\n4\n\nmatrix transpose and frobenius norm\n\u2022 The transpose of an n \u21e5m matrix M, is the m \u21e5n matrix whose\n(i, j)-th entry is the same as the (j, i)-th entry of M. It is denoted MT.\n\u2022 For matrices A and B,\n(AB)T = BTAT\n\u2022 If the columns of a matrix M are orthonormal then MTM is the identity\nmatrix, i.e., the diagonal matrix where every diagonal entry is 1.\n\u2022 The Frobenius norm kXkF of a matrix X is the square root of the sum\nof its squared entires, i.e.,\nkXkF =\nqX\nX 2\ni,j\n\u2022 kXk2\nF is equal to the trace of X TX which equals the sum of the\neigenvalues of X TX.\n5\n\nlow dimensional data\nAndrew McGregor\n0\n\u21b3\n\nembedding with assumptions\n\u2022 Let V be a k-dimensional subspace of Rd, i.e., the in\ufb01nite set of vectors\nformed by taking linear combinations of some set of k orthonormal vectors.\n\u2022 Assume that data points ~x1, . . . , ~xn 2 V .\nClaim: Let ~v1, . . . , ~vk be an orthonormal basis for V and V 2 Rd\u21e5k be the\nmatrix with these vectors as its columns. For all ~xi, ~xj:\nkVT~xi \u2212VT~xjk2 = k~xi \u2212~xjk2.\nThat is, VT 2 Rk\u21e5d is a linear embedding of ~x1, . . . , ~xn into k dimensions with\nno distortion.\n1\n\ndot product transformation\nWe\u2019ll prove a more general result about any ~y 2 V, not just ~y = ~xi \u2212~xj.\nClaim: Let ~v1, . . . , ~vk be an orthonormal basis for V and V 2 Rd\u21e5k be\nthe matrix with these vectors as its columns. For all ~y 2 V:\nkVT~yk2 = k~yk2.\nProof:\n\u2022 If ~y = P\ni ci~vi then ~y = V~c where ~cT = (c1, . . . , ck)\n\u2022 k~yk2\n2 = ~y T~y = (V~c)T(V~c) = ~cTVTV~c\n\u2022 kVT~yk2\n2 = (VT~y)T(VT~y) = ~y TVVT~y = ~cTVTVVTV~c\n\u2022 But VTV = I since\n[VTV]i,j = ~v T\ni ~vj =\n(\n1\ni = j\n0\ni 6= j\n\u2022 So k~yk2\n2 = ~cT~c = kVT~yk2\n2.\n2\n\nlow-rank factorization\n\u2022 Suppose every data point ~xi (row of data matrix X) can be written as\n~xi = ci,1 \u00b7 ~v1 + . . . + ci,k \u00b7 ~vk\nand let V have columns are ~v1, . . . , ~vk.\n\u2022 X can be represented by (n + d) \u00b7 k parameters vs. n \u00b7 d.\n\u2022 The rows of X are spanned by the k rows of VT and the columns of X\nare spanned by the k columns of C.\n3\n\nlow-rank factorization\nClaim: If ~x1, . . . , ~xn lie in a k-dimensional subspace with orthonormal\nbasis V 2 Rd\u21e5k, the data matrix can be written as X = XVVT.\nProof: Since the columns of V are orthonormal, VTV is the identity\nmatrix and so:\nX = CVT =) XV = CVTV =) XV = C\nand so X = XVVT\n4\n\nprojection matrix\n\u2022 VVT is a projection matrix, which projects the rows of X (the data\npoints ~x1, . . . , ~xn) onto the subspace V.\n\u2022 We just showed that X = XVVT when the rows of X are already in V.\n\u2022 Next we\u2019ll show that XVVT is the \u201cclosest\u201d matrix to X that has the\nproperty that every row is in V.\n5\n\nmapping to a fixed low dimensional space\nAndrew McGregor\n0\n\u21b3\n\nbest approximation of x in a given subspace\n\u2022 Let X be the matrix whose rows correspond to data points ~x1, . . . , ~xn.\n\u2022 Let ~v1, . . . , ~vk be orthonormal basis of k-dimensional subspace V of Rd.\n\u2022 What matrix B is \u201cclosest\u201d to X that has the property that every row\nof B is in V? Speci\ufb01cally,\n\u2022 Want to minimize kB \u2212Xk2\nF where k \u00b7 kF is the Frobenius norm, i.e., the\nsquare root of the sum of squared entries.\n\u2022 Need to every row of B to be a linear combination of ~v1, . . . , ~vk.\nTheorem:\nXVVT =\narg min\nB with rows in V\nkX \u2212Bk2\nF.\nRecall VVT is called the projection matrix.\n1\n\nthree properties of projection matrices\nProperty 1 : Show that VVT is idempotent, i.e. ,\n(VVT)(VVT) = (VVT)\nProof: (VVT)(VVT) = V(VTV)VT = VVT\n2\n\nthree properties of projection matrices\nProperty 2: The projection is orthogonal to its complement: For any\n~y 2 Rd,\nhVVT~y, ~y \u2212VVT~yi = 0\nProof:\nhVVT~y, ~y \u2212VVT~yi\n=\nhVVT~y, ~yi \u2212hVVT~y, VVT~yi\n=\n~y TVVT~y \u2212~y TVVTVVT~y\n=\n0\nsince VVT is idempotent.\n3\n\nthree properties of projection matrices\nProperty 3 (Pythagorean Theorem): For any ~y 2 Rd,\nk~yk2\n2 = k(VVT)~yk2\n2 + k~y \u2212(VVT)~yk2\n2.\nProof: Write ~y = (~y \u2212(VVT)~y) + (VVT)~y and then use\nk~a + ~bk2\n2 = k~ak2\n2 + k~bk2\n2 + 2h~a, ~bi\nand the fact that the projection is orthogonal to its complement\n4\n\nprojection vector is closest point in subspace\nLet V 2 Rn\u21e5k have orthonormal columns and let ~y 2 Rn. Then the\nPythagorean Theorem proves that VVT~y is the closest vector to ~y that\ncan be expressed as a linear combination of the columns of V\n\u2022 Apply Pythagorus to ~y \u2212~z for arbitrary ~z 2 Rn:\nk~y \u2212~zk2\n2 = kVVT(~y \u2212~z)k2\n2 + k~y \u2212~z \u2212VVT(~y \u2212~z)k2\n2\n= kVVT~y \u2212VVT~zk2\n2 + k~y \u2212VVT~y \u2212~z + VVT~zk2\n2.\n\u2022 If ~z = V~c for some ~c 2 Rk, then VVT~z = VVTV~c = V~c = ~z and the\nabove simpli\ufb01es to\nkVVT~y \u2212~zk2\n2 + k~y \u2212VVT~yk2\n2\n\u2022 To minimize this, set ~z = VVT~y.\n5\n\nproof of theorem\nTheorem:\nXVVT =\narg min\nB with rows in V\nkX \u2212Bk2\nF.\nProof:\n\u2022 kX \u2212Bk2\nF = Pn\ni=1 k~xi \u2212~bik2\n2 where ~bi is the ith row of B.\n\u2022 Minimized by setting ~bi to be the closest vector to ~xi that is in V.\n\u2022 From the previous slide, ~bi = ~xiVVT since we\u2019re dealing with row\nvectors rather than column vectors.\n\u2022 So the optimal B is XVVT.\n6\n\neigenvalues and symmetric matrices\nAndrew McGregor\n0\n4\n\neigenvectors and eigendecomposition\n\u2022 Eigenvector: ~x 2 Rd is an eigenvector of a matrix A 2 Rd\u21e5d if\nA~x = \u03bb~x\nfor some scalar \u03bb called the eigenvalue corresponding to ~x. That is, A\njust \u2018stretches\u2019 x.\n\u2022 Spectral Theorem: If A is symmetric, has d orthonormal\neigenvectors.\n\u2022 Eigendecomposition: Let ~v1, . . . , ~vd be the orthonormal eigenvectors.\nLet V 2 Rd\u21e5d be the matrix with these vectors as columns and \u21e4be\nthe diagonal matrix with corresponding eigenvalues on the diagonal.\nAV =\n2\n64\n|\n|\n|\n|\nA~v1\nA~v2\n\u00b7 \u00b7 \u00b7\nA~vd\n|\n|\n|\n|\n3\n75 =\n2\n64\n|\n|\n|\n|\n\u03bb1~v1\n\u03bb2~v2\n\u00b7 \u00b7 \u00b7\n\u03bb~vd\n|\n|\n|\n|\n3\n75 = V\u21e4\nand so AVVT = A = V\u21e4VT where the \ufb01rst inequality follows since\nrows of A are in span of the eigenvectors.\n1\n\neigenvectors and eigendecomposition\nTypically order the eigenvectors in decreasing order:\n\u03bb1 \u2265\u03bb2 \u2265. . . \u2265\u03bbd\n2\n\ncourant-fischer theorem\nCourant-Fischer Theorem: For symmetric A, the eigenvectors are given via\nthe greedy optimization:\n~v1 =\narg max\n~v with kvk2=1\n~v TA~v.\n~v2 =\narg max\n~v with kvk2=1, h~v,~v1i=0\n~v TA~v.\n. . .\n~vd =\narg max\n~v with kvk2=1, h~v,~vj i=0 8j<d\n~v TA~v.\n\u2022 ~v T\nj A~vj = \u03bbj \u00b7 ~v T\nj ~vj = \u03bbj, the jth largest eigenvalue.\n3\n\nbest fit subspace\nAndrew McGregor\n0\n65\n\u2193\n\nbest fit subspace\n\u2022 So far we know, given X whose rows are data points in Rd and\nsubspace V \u21e2Rd\nXVVT =\narg min\nB with rows in V\nkX \u2212Bk2\nF\nwhere V is the matrix whose columns are an orthonormal basis V.\n\u2022 Therefore, kX \u2212XVVTk2\nF is the smallest error achievable when\napproximating X by a matrix with rows in V.\n\u2022 Here we investigate how to choose V (or equivalently V) such that we\nminimize:\nkX \u2212XVVTk2\nF ,\nsubject to the constraint that V is k-dimensional, i.e., V has only k\ncolumns.\n1\n\nbest fit subspace\nWant orthonormal V 2 Rd\u21e5k that minimizes\nkX \u2212XVVTk2\nF\n=\nkXT \u2212VVTXTk2\nF\n=\nn\nX\ni=1\nk~xi \u2212VVT~xik2\n2\n=\nn\nX\ni=1\nk~xik2\n2 \u2212kVVT~xik2\n2\nwhere the last line follows from Pythagoras.\nSo minimizing kX \u2212XVVTk2\nF is the same as maximizing\nX\ni\nkVVT~xik2\n2 =\nX\ni\n~xT\ni VVTVVT~xi =\nX\ni\n~xT\ni VVT~xi =\nX\ni\nkVT~xik2\n2\n2\n\nsolution via eigendecomposition\nV minimizing kX \u2212XVVTk2\nF is given by:\narg max\northonormal V2Rd\u21e5k\nn\nX\ni=1\nkVT~xik2\n2 =\nk\nX\nj=1\nn\nX\ni=1\nh~vj, ~xii2 =\nk\nX\nj=1\nkX~vjk2\n2\nSurprisingly, can \ufb01nd the columns of V, ~v1, . . . , ~vk greedily.\n~v1 =\narg max\n~v with kvk2=1\n~v TXTX~v.\n~v2 =\narg max\n~v with kvk2=1, h~v,~v1i=0\n~v TXTX~v.\n. . .\n~vk =\narg max\n~v with kvk2=1, h~v,~vj i=0 8j<k\n~v TXTX~v.\nBy Courant-Fischer Theorem, these are exactly the top k eigenvectors of XTX!\n3\n\nlow-rank approx via eigendecomposition\nUpshot: Letting Vk have columns ~v1, . . . , ~vk corresponding to the top k\neigenvectors of the covariance matrix XTX, Vk is the orthogonal matrix\nminimizing\nkX \u2212XVkVT\nk k2\nF,\nThis is principal component analysis (PCA).\nNext: How accurate is this low-rank approximation? Can understand via\neigenvalues of XTX.\n4\n\nerror of best fit subspace\nAndrew McGregor\n0\u00b7\n\nsummary so far...\n\u2022 Given X whose rows data points in Rd and subspace V \u21e2Rd\nXVVT =\narg min\nB with rows in V\nkX \u2212Bk2\nF\nwhere V is the matrix whose columns are an orthonormal basis V.\n\u2022 Therefore, kX \u2212XVVTk2\nF is the smallest error achievable when\napproximating X by a matrix with rows spanned by the V.\n\u2022 The orthonormal V 2 Rd\u21e5k that minimizes:\nkX \u2212XVVTk2\nF ,\nis the matrix Vk whose columns are the top k eigenvectors of XTX.\n\u2022 How large is the error kX \u2212XVkVT\nk k2\nF?\n1\n\nmain ingredients for the analysis\n\u2022 By applying the Pythagorus Theorem on each row:\nkXk2\nF = kX \u2212XVkVT\nk k2\nF + kXVkVT\nk k2\nF\n\u2022 Because Vk is orthonormal,\nkXVkVT\nk k2\nF = kXVkk2\nF\n\u2022 For any matrix A,\nkAk2\nF =\nd\nX\ni=1\nk~aik2\n2 = tr(ATA) = sum of diagonal entries = sum eigenvalues.\n2\n\nspectrum analysis\nApproximation error is:\nkX \u2212XVkVT\nk k2\nF = tr(XTX) \u2212tr(VT\nk XTXVk)\n=\nd\nX\ni=1\n\u03bbi \u2212\nk\nX\ni=1\n~v T\ni XTX~vi\n=\nd\nX\ni=1\n\u03bbi \u2212\nk\nX\ni=1\n\u03bbi =\nd\nX\ni=k+1\n\u03bbi\n3\n\nspectrum analysis\nClaim: The error in approximating X with the best rank k approximation\n(projecting onto the top k eigenvectors of XTX) is:\nkX \u2212XVkVT\nk k2\nF =\nd\nX\ni=k+1\n\u03bbi\n4\n\nspectrum analysis\nClaim: The error in approximating X with the best rank k approximation\n(projecting onto the top k eigenvectors of XTX) is:\nkX \u2212XVkVT\nk k2\nF =\nd\nX\ni=k+1\n\u03bbi\n4\n\nspectrum analysis\nPlotting the spectrum of the covariance matrix XTX (its eigenvalues) shows\nhow compressible X is using low-rank approximation (i.e., how close ~x1, . . . , ~xn\nare to a low-dimensional subspace).\n5\n\nspectrum analysis\nPlotting the spectrum of the covariance matrix XTX (its eigenvalues) shows\nhow compressible X is using low-rank approximation (i.e., how close ~x1, . . . , ~xn\nare to a low-dimensional subspace).\n5\n\nspectrum analysis\nPlotting the spectrum of the covariance matrix XTX (its eigenvalues) shows\nhow compressible X is using low-rank approximation (i.e., how close ~x1, . . . , ~xn\nare to a low-dimensional subspace).\n5\n\nspectrum analysis\nNote the eigenvalues of XTX are always positive. Follows since\n\u03bbj = ~v T\nj XTX~vj = kX~vjk2\n2 \u22650 .\n6\n\nsingular value decomposition\nAndrew McGregor\n0\n\u21b3\n\npreviously...\n\u2022 Eigendecomposition: For any symmetric square matrix A, we can write\nA = V\u21e4VT where columns of V are orthonormal eigenvectors and \u21e4is the\ndiagonal matrix of eigenvalues.\n\u2022 The best k-dimensional approximation of a data matrix X 2 Rn\u21e5d is\nXVkVT\nk\nwhere Vk is the matrix whose columns are the top k eigenvectors of XTX.\n\u2022 The error is\nkX \u2212XVkVT\nk k2\nF = \u03bbk+1 + \u03bbk+2 + . . . + \u03bbd\nwhere \u03bbi is the i-th biggest eigenvalue of XTX.\n1\n\nsingular value decomposition\nThe Singular Value Decomposition (SVD) generalizes the eigendecomposition\nto asymmetric (even rectangular) matrices.\nAny matrix X 2 Rn\u21e5d with\nrank(X) = r can be written as X = U\u2303VT.\n\u2022 U has orthonormal columns ~u1, . . . , ~ur 2 Rn (left singular vectors).\n\u2022 V has orthonormal columns ~v1, . . . , ~vr 2 Rd (right singular vectors).\n\u2022 \u2303is diagonal with elements \u03c31 \u2265\u03c32 \u2265. . . \u2265\u03c3r > 0 (singular values).\nThe \u2018swiss army knife\u2019 of modern linear algebra.\n2\n\nconnection of the svd to eigendecomposition\nWriting X 2 Rn\u21e5d in its singular value decomposition X = U\u2303VT:\nXTX = V\u2303UTU\u2303VT = V\u23032VT (the eigendecomposition)\nSimilarly: XXT = U\u2303VTV\u2303UT = U\u23032UT.\nThe right and left singular vectors are the eigenvectors of the covariance matrix\nXTX and the gram matrix XXT respectively.\nSo, letting Vk 2 Rd\u21e5k have columns equal to ~v1, . . . , ~vk, we know that XVkVT\nk\nis the best rank-k approximation to X (given by PCA).\nWhat about UkUT\nk X where Uk 2 Rn\u21e5k has columns equal to ~u1, . . . , ~uk?\nExercise: UkUT\nk X = XVkVT\nk = Uk\u2303kVT\nk\nX 2 Rn\u21e5d: data matrix, U 2 Rn\u21e5rank(X): matrix with orthonormal columns ~u1, ~u2, . . .\n(left singular vectors), V 2 Rd\u21e5rank(X): matrix with orthonormal columns ~v1, ~v2, . . . (right\nsingular vectors), \u23032 Rrank(X)\u21e5rank(X): positive diagonal matrix containing singular values\nof X.\n3\n\nthe svd and optimal low-rank approximation\nThe best low-rank approximation to X, i.e.,\nXk =\narg min\nrank-k B2Rn\u21e5d kX \u2212BkF\nis given by Xk = XVkVT\nk = UkUT\nk X = Uk\u2303kVT\nk\nCorresponds to projecting the rows (data points) onto the span of Vk or\nthe columns (features) onto the span of Uk\n5\n\nthe svd and optimal low-rank approximation\nThe best low-rank approximation to X, i.e.,\nXk =\narg min\nrank-k B2Rn\u21e5d kX \u2212BkF\nis given by Xk = XVkVT\nk = UkUT\nk X = Uk\u2303kVT\nk\nCorresponds to projecting the rows (data points) onto the span of Vk or\nthe columns (features) onto the span of Uk\n5\n\nthe svd and optimal low-rank approximation\nThe best low-rank approximation to X, i.e.,\nXk =\narg min\nrank-k B2Rn\u21e5d kX \u2212BkF\nis given by Xk = XVkVT\nk = UkUT\nk X = Uk\u2303kVT\nk\nCorresponds to projecting the rows (data points) onto the span of Vk or\nthe columns (features) onto the span of Uk\n5\n\nthe svd and optimal low-rank approximation\nThe best low-rank approximation to X, i.e.,\nXk =\narg min\nrank-k B2Rn\u21e5d kX \u2212BkF\nis given by Xk = XVkVT\nk = UkUT\nk X = Uk\u2303kVT\nk\nCorresponds to projecting the rows (data points) onto the span of Vk or\nthe columns (features) onto the span of Uk\n5\n\nbasic idea to prove existence of svd\n\u2022 Let ~v1, ~v2, . . . 2 Rd be orthonormal eigenvectors of XTX.\n\u2022 Let \u03c3i = kX~vik2 and de\ufb01ne unit vector ~ui = X~vi\n\u03c3i .\n\u2022 Exercise: Show ~u1, ~u2, . . . are orthonormal eigenvectors of XXT.\n\u2022 This establishes that XV = U\u2303and that V and U have the required\nproperties.\n\u2022 Full proof isn\u2019t in the scope of COMPSCI 514 but if you\u2019re interested:\nhttps://math.mit.edu/classes/18.095/2016IAP/lec2/SVD_Notes.pdf\n6\n\napplications: matrix completion\nAndrew McGregor\n0\n\u21b3\n\npuzzle\nConsider a matrix X 2 Rn\u21e5d which we cannot fully observe. What are\nthe missing entries?\n2\n6664\n1\n1\n3\n6\n2\n3\n1\n9\n3\n7775\nAn impossible question! But what if we knew the rank was only 1?\n1\n\nmatrix completion\nConsider a matrix X 2 Rn\u21e5d which we cannot fully observe but believe is\nclose to rank-k (i.e., well approximated by a rank k matrix). Classic\nexample: the Net\ufb02ix prize problem.\nSolve: Y = arg min\nrank \u2212k B\nX\nobserved (j,k)\n[Xj,k \u2212Bj,k]2\n2\n\nmatrix completion\nConsider a matrix X 2 Rn\u21e5d which we cannot fully observe but believe is\nclose to rank-k (i.e., well approximated by a rank k matrix). Classic\nexample: the Net\ufb02ix prize problem.\nSolve: Y = arg min\nrank \u2212k B\nX\nobserved (j,k)\n[Xj,k \u2212Bj,k]2\nUnder certain assumptions, can show that Y well approximates X on\nboth the observed and (most importantly) unobserved entries.\n2\n\nidealized approach\n\u2022 Given include matrix X, \ufb01ll in the missing values with the row or\ncolumn average of the non-missing values.\n\u2022 Call the resulting matrix Z.\n\u2022 Let Vk be the matrix formed by the top k eigenvectors of ZTZ\n\u2022 Return ZVkVT\nk\n\u2022 It\u2019s a little more complicated in practice and the above approach\nwouldn\u2019t work well unless there were relatively few missing values.\n3\n\napplications: lsa and word embeddings\nAndrew McGregor\n0\n\u00b7\n\nentity embeddings\nDimensionality reduction embeds d-dimensional vectors into k \u2327d\ndimensions. But what about when you want to embed objects other than\nvectors?\n\u2022 Documents (for topic-based search and classi\ufb01cation)\n\u2022 Words (to identify synonyms, translations, etc.)\n\u2022 Nodes in a social network\nUsual Approach: Convert each item into a high-dimensional feature\nvector and then apply low-rank approximation.\n1\n\nexample: latent semantic analysis\n2\n\nexample: latent semantic analysis\n2\n\nexample: latent semantic analysis\n\u2022 If the error kX \u2212YZTkF is small, then on average,\nXi,a \u21e1(YZT)i,a = h~yi, ~zai.\n\u2022 I.e., h~yi, ~zai \u21e11 when doci contains worda.\n3\n\nexample: latent semantic analysis\nIf doci and docj both contain worda, h~yi, ~zai \u21e1h~yj, ~zai \u21e11 If doci and docj\nboth don\u2019t contain worda, h~yi, ~zai \u21e1h~yj, ~zai \u21e10\nSince this applies for all words, documents with that involve a similar set of\nwords tend to have high dot product with each other.\nAnother View: Column of Y represent \u2018topics\u2019. ~yi(j) indicates how much doci\nbelongs to topic j. ~za(j) indicates how much worda associates with that topic.\n4\n\nexample: latent semantic analysis\n\u2022 Just like with documents, ~za and ~zb will tend to have high dot product if\nworda and wordb appear in many of the same documents.\n\u2022 In an SVD decomposition we set ZT = \u2303kVT\nk where columns of Vk are the\ntop k eigenvectors of XTX.\n5\n\nexample: word embedding\nLSA gives a way of embedding words into k-dimensional space.\n\u2022 Embedding is via low-rank approximation of XTX: where (XTX)a,b is the\nnumber of documents that both worda and wordb appear in.\n\u2022 Think about XTX as a similarity matrix with entry (a, b) being the similarity\nbetween worda and wordb.\n\u2022 Many ways to measure similarity: number of sentences both occur in,\nnumber of times both appear in the same window of w words, in similar\npositions of documents in di\u21b5erent languages, etc.\n\u2022 Replacing XTX with these di\u21b5erent metrics (sometimes appropriately\ntransformed) leads to popular word embedding algorithms: word2vec, GloVe,\nfastText, etc.\n6\n\nexample: word embedding\nNote: word2vec is typically described as a neural-network method, but it\nis really just low-rank approximation of a speci\ufb01c similarity matrix. Neural\nword embedding as implicit matrix factorization, Levy and Goldberg.\n7\n\ngraph embeddings\nAndrew McGregor\n0\u00b7\n\nnon-linear dimensionality reduction\nIs this set of points compressible? Does it lie close to a low-dimensional\nsubspace? (A 1-dimensional subspace of Rd.)\n1\n\nnon-linear dimensionality reduction\nIs this set of points compressible? Does it lie close to a low-dimensional\nsubspace? (A 1-dimensional subspace of Rd.)\n1\n\nnon-linear dimensionality reduction\nIs this set of points compressible? Does it lie close to a low-dimensional\nsubspace? (A 1-dimensional subspace of Rd.)\nA common way of automatically identifying this non-linear structure is to\nconnect data points in a graph. E.g., a k-nearest neighbor graph.\n\u2022 Connect items to similar items, possibly with higher weight edges when they\nare more similar.\n1\n\nlinear algebraic representation of a graph\nOnce we have connected n data points x1, . . . , xn into a graph, we can\nrepresent that graph by its (weighted) adjacency matrix.\nA 2 Rn\u21e5n with Ai,j = edge weight between nodes i and j\n2\n\nadjacency matrix eigenvectors\nHow do we compute an optimal low-rank approximation of A?\n\u2022 Project onto the top k eigenvectors of ATA = A2. Note these are just\nthe eigenvectors of A.\n1. A \u21e1AVkVT\nk where Vk is the matrix with top k eigenvectors as columns.\n2. Rows of AVk are an embedding of the nodes into Rk.\n\u2022 Similar vertices (close with regards to graph proximity) should have\nsimilar embeddings since\nk(A)i \u2212(A)jk2\n\u21e1\nk(AVkVT\nk )i \u2212(AVkVT\nk )jk2\n=\nk(AiVkVT\nk \u2212AjVkVT\nk k2\n=\nkAiVk \u2212AjVkk2\n=\nk(AVk)i \u2212(AVk)jk2\nwhere the second equality follows because for any ~y,\nk~y TVT\nk k2 = ~y TVT\nk Vk~y = k~yk2 .\n3\n\nspectral embedding\nStep 1: Produce a nearest neighbor graph\nbased on your input data in Rd.\nStep 2: Apply low-rank approximation to\nthe graph adjacency matrix to produce\nembeddings in Rk.\nStep 3: Work with the data in the\nembedded space. Where distances\napproximate distances in your original\n\u2018non-linear space.\u2019\n4\n\ngraph laplacian\nAndrew McGregor\n0\nb11\n\u2193\n\nthe laplacian\nFor a graph with adjacency matrix A and degree matrix D, L = D \u2212A is the\ngraph Laplacian.\nLemma: For any vector ~v,\nX\n(i,j)2E\n(~v(i) \u2212~v(j))2 = ~v TL~v.\n1\n\nrewriting laplacian\nLemma:\nX\n(i,j)2E\n(~v(i) \u2212~v(j))2 = ~v TL~v\nProof:\n\u2022 Let Le be the Laplacian for graph just containing edge e.\n\u2022 If e = (i, j), then ~v TLe~v = (v(i) \u2212v(j))2\n\u2022 By linearity,\n~v TL~v =\nX\ne2E\n~v TLe~v\n2\n\nother useful properties\n\u2022 Property 1: All the eigenvalues of the L are non-negative.\n\u2022 Proof: If ~v is a eigenvalue of L with eigenvalue \u03bb then\n0 \uf8ff\nX\n(i,j)2E\n(~v(i) \u2212~v(j))2 = ~v TL~v = \u03bbk~vk2\n2\n\u2022 Property 2: Let ~v 2 {\u22121, 1} and A = {i : ~v(i) = 1}. Then,\n~v TL~v = 4 \u21e5\u201cnumber of edges with exactly one node in A\u201d\n\u2022 Proof:\n~v TL~v =\nX\n(i,j)2E\n(~v(i)\u2212~v(j))2 = 4\u21e5\u201cnumber of edges with exactly one node in A\u201d\n3\n\nother useful properties\n\u2022 Property 3: Suppose the eigenvalues of the adjacency matrix A are\n\u03bb1 \u2265\u03bb2 \u2265. . . \u2265\u03bbn\nand every node has degree d. Then the eigenvalues of L are\n\u00b51 \u2265\u00b52 \u2265. . . \u2265\u00b5n\nwhere \u00b51 = d \u2212\u03bbn, \u00b52 = d \u2212\u03bbn\u22121, . . . , \u00b5n = d \u2212\u03bb1.\n\u2022 Proof: If ~v is the jth eigenvector of A then\nL~v = D~v \u2212A~v = d~v \u2212\u03bbj~v = (d \u2212\u03bbj)~v\ni.e., ~v is also an eigenvector of L and has the eigenvalue d \u2212\u03bbj.\n4\n\nspectral clustering\nAndrew McGregor\n0\nb12\n\u2193\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nCommunity detection in naturally occurring networks.\n1\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nCommunity detection in naturally occurring networks.\n1\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nLinearly separable data.\n1\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nNon-linearly separable data k-nearest neighbor graph.\n1\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nNon-linearly separable data k-nearest neighbor graph.\n1\n\nspectral clustering\nA very common task is to partition or cluster vertices in a graph based on\nsimilarity/connectivity.\nNon-linearly separable data k-nearest neighbor graph.\nCan \ufb01nd this cut using eigendecomposition!\n1\n\ncut minimization\nSimple Idea: Partition clusters along minimum cut in graph.\n2\n\ncut minimization\nSimple Idea: Partition clusters along minimum cut in graph.\nSmall cuts are often not informative.\n2\n\ncut minimization\nSimple Idea: Partition clusters along minimum cut in graph.\nSmall cuts are often not informative.\nSolution: Encourage cuts that separate large sections of the graph.\n2\n\ncut minimization\nSimple Idea: Partition clusters along minimum cut in graph.\nSmall cuts are often not informative.\nSolution: Encourage cuts that separate large sections of the graph.\n\u2022 Let ~v 2 Rn be a cut indicator: ~v(i) = 1 if i 2 S. ~v(i) = \u22121 if i 2 T. Want\n~v to have roughly equal numbers of 1s and \u22121s. I.e., ~v T~1 \u21e10.\n2\n\nthe laplacian view\nFor a graph with adjacency matrix A and degree matrix D, L = D \u2212A is\nthe graph Laplacian.\nFor a cut indicator vector ~v 2 {\u22121, 1}n with ~v(i) = \u22121 for i 2 S and\n~v(i) = 1 for i 2 T:\n1. ~v TL~v = P\n(i,j)2E(~v(i) \u2212~v(j))2 = 4 \u00b7 cut(S, T).\n2. ~v T~1 = |T| \u2212|S|.\nWant to minimize both ~v TL~v (cut size) and ~v T~1 (imbalance).\nThis can be solved by eigendecomposition!\n3\n\nsecond smallest laplacian eigenvector\nBy Courant-Fischer, the second smallest eigenvector is given by:\n~vn\u22121 =\narg min\nv2Rn with k~vk=1, ~v T\nn ~v=0\n~v TL~v\nIf ~vn\u22121 were in\nn\n\u22121\npn,\n1\npn\non\nit would have:\n\u2022 ~v T\nn\u22121L~vn\u22121 = 4\nn \u00b7 cut(S, T) as small as possible given that\n~v T\nn\u22121~vn =\n1\npn~v T\nn\u22121~1 = |T| \u2212|S|\npn\n= 0 .\n\u2022 I.e., ~vn\u22121 would indicate the smallest perfectly balanced cut.\n\u2022 The eigenvector ~vn\u22121 2 Rn is not generally binary, but still satis\ufb01es a\n\u2018relaxed\u2019 version of this property.\n5\n\ncutting with the second laplacian eigenvector\nFind a good partition of the graph by computing\n~vn\u22121 =\narg min\nv2Rdwith k~vk=1, ~v T\nn\u22121~1=0\n~v TL~v\nSet S to be all nodes with ~vn\u22121(i) < 0, T to be all with ~vn\u22121(i) \u22650.\n6\n\nspectral clustering with guarantees\n\u2022 Summary: To partition a graph, \ufb01nd the eigenvector of the Laplacian\nwith the second smallest eigenvalue. Partition nodes based on whether\ncorresponding value in eigenvector is positive/negative.\n~vn\u22121 =\narg min\n~v2Rn,k~vk=1,~v T~1=0\n~v TL~v\n\u2022 We argued this \u201cshould\u201d partition graph along a small cut that\nseparates the graph into large pieces.\n\u2022 Haven\u2019t given formal guarantees. . . but later we\u2019ll consider the\nstochastic block model as a way to justify this approach.\n7\n\nstochastic block model: part 1\nAndrew McGregor\n0\nb13\n\nspectral clustering\n\u2022 To partition a graph, \ufb01nd the eigenvector of the Laplacian with the\nsecond smallest eigenvalue. Partition nodes based on whether\ncorresponding value in eigenvector is positive/negative.\n~vn\u22121 =\narg min\n~v2Rn,k~vk=1,~v T~1=0\n~v TL~v\n\u2022 We argued this \u201cshould\u201d partition graph along a small cut that\nseparates the graph into large pieces.\n\u2022 Haven\u2019t given formal guarantees; it\u2019s di\ufb03cult for general input graphs.\nBut can consider randoms \u201cnatural\u201d graphs. . .\n\u2022 Common Approach: Give a natural generative model for random\ninputs and analyze how the algorithm performs on such inputs. Can be\nused to justify `2 linear regression, k-means clustering, etc.\n1\n\nstochastic block model\nStochastic Block Model (Planted Partition Model): Let Gn(p, q) be a\ndistribution over graphs on n nodes, split randomly into two groups B and C,\neach with n/2 nodes.\n\u2022 Any two nodes in the same group are connected with probability p (including\nself-loops).\n\u2022 Any two nodes in di\u21b5erent groups are connected with prob. q < p.\n\u2022 Connections are independent.\n2\n\nlinear algebraic view\nLet G be a stochastic block model graph drawn from Gn(p, q).\n\u2022 Let A 2 Rn\u21e5n be the adjacency matrix of G.\n\u2022 For sake of analysis assume rows/columns of A are ordered such that\nnodes in B come \ufb01rst. In reality, we the ordering could be arbitrary.\n3\n\nexpected adjacency spectrum\nLetting G be a stochastic block model graph drawn from Gn(p, q) and\nA 2 Rn\u21e5n be its adjacency matrix. (E[A])i,j = p for i, j in same group,\n(E[A])i,j = q otherwise.\nWhat is rank(E[A])? What\nare the eigenvectors and\neigenvalues of E[A]?\n4\n\nexpected adjacency spectrum\nIf we compute ~v2 then we recover the communities B and C!\n\u2022 Can show that for G \u21e0Gn(p, q), A is \u201cclose\u201d to E[A] in some\nappropriate sense (matrix concentration inequality).\n\u2022 Second eigenvector of A is close to [1, 1, 1, . . . , \u22121, \u22121, \u22121] and gives a\ngood estimate of the communities.\nWhen rows/columns aren\u2019t sorted by ID, second eigenvector is e.g.,\n[1, \u22121, 1, \u22121, . . . , 1, 1, \u22121] and entries give community ids.\n5\n\nexpected laplacian spectrum\nLetting G be a stochastic block model graph drawn from Gn(p, q),\nA 2 Rn\u21e5n be its adjacency matrix and L be its Laplacian, what are the\neigenvectors and eigenvalues of E[L]?\nE[L] = E[D] \u2212E[A] =\n\u2713n(p + q)\n2\n\u25c6\nI \u2212E[A]\nand so if E[A]~x = \u03bb~x then\nE[L]~x = (n(p + q)/2 \u2212\u03bb)~x\nTherefore, second eigenvalue of E[A] is second last eigenvector of E[L].\n6\n\nexpected laplacian spectrum\nUpshot: The second smallest eigenvector of E[L] encodes the cut\nbetween the communities.\n\u2022 If the matrices A and L were exactly equal to their expectation,\npartitioning using this eigenvector (i.e., spectral clustering) would\nexactly recover the two communities B and C.\nHow do we show that a matrix is close to its expectation? Matrix\nconcentration inequalities.\n\u2022 Analogous to scalar concentration inequalities like Cherno\u21b5etc.\n\u2022 Random matrix theory is a very recent and cutting edge sub\ufb01eld of\nmathematics that is being actively applied in computer science,\nstatistics, and ML.\n7\n\nstochastic block model: part 2\nAndrew McGregor\n0\nb14 !\n\nrecap\n\u2022 Suppose G is a random graph generated according to the stochastic\nblock model. Let B and C be the groups in the graph. Let A be the\nadjacency matrix.\n\u2022 If ~v be the second largest eigenvector of E[A] then\n~v(i) =\n(\n1\nif i 2 B\n\u22121\nif i 2 C\nThat is, we can work out B and C based on ~v. Note if we normalize\nthe vector, the values are \u00b11/pn.\n1\n\nrecap\n\u2022 Suppose G is a random graph generated according to the stochastic\nblock model. Let B and C be the groups in the graph. Let A be the\nadjacency matrix.\n\u2022 If ~v be the second largest eigenvector of E[A] then\n~v(i) =\n(\n1\nif i 2 B\n\u22121\nif i 2 C\nThat is, we can work out B and C based on ~v. Note if we normalize\nthe vector, the values are \u00b11/pn.\n\u2022 Next we argue that the second largest eigenvector of A is su\ufb03ciently\nsimilar to the second largest eigenvector of E[A] such that we can work\nout B and C approximately the second largest eigenvector of A.\n1\n\neigenvector perturbation\nDavis-Kahan Eigenvector Perturbation Theorem:\nSuppose\nA, A 2 Rd\u21e5d are symmetric with kA \u2212Ak2 \uf8ff\u270fand eigenvec-\ntors v1, v2, . . . , vd and \u00afv1, \u00afv2, . . . , \u00afvd. Letting \u2713(vi, \u00afvi) denote the\nangle between vi and \u00afvi, for all i:\nsin[\u2713(vi, \u00afvi)] \uf8ff\n\u270f\nminj6=i |\u03bbi \u2212\u03bbj|\nwhere \u03bb1, . . . , \u03bbd are the eigenvalues of A.\nThe errors get large if there are eigenvalues with similar magnitudes.\n3\n\napplication to stochastic block model\nClaim 1 (Matrix Concentration): For p \u2265O\n\u21e3\nlog4 n\nn\n\u2318\n,\nkA \u2212E[A]k2 \uf8ffO(ppn).\nClaim 2 (Davis-Kahan): For p \u2265O\n\u21e3\nlog4 n\nn\n\u2318\n,\nsin \u2713(v2, \u00afv2) \uf8ff\nO(ppn)\nminj6=2 |\u03bb2 \u2212\u03bbj| \uf8ff\nO(ppn)\n(p \u2212q)n/2 = O\n\u2713\npp\n(p \u2212q)pn\n\u25c6\nRecall: E[A] has eigenvalues \u03bb1 = (p+q)n\n2\n, \u03bb2 = (p\u2212q)n\n2\n, \u03bbi = 0 for i \u22653.\nmin\nj6=2 |\u03bb2 \u2212\u03bbj| = min\n\u2713\nqn, (p \u2212q)n\n2\n\u25c6\n.\nTypically, (p\u2212q)n\n2\nwill be the minimum of these two gaps.\nA adjacency matrix of random stochastic block model graph. p: connection probability\nwithin clusters. q < p: connection probability between clusters. n: number of nodes.\nv2, \u00afv2: second eigenvectors of A and E[A] respectively.\n4\n\napplication to stochastic block model\nSo Far: sin \u2713(v2, \u00afv2) \uf8ffO\n\u21e3\npp\n(p\u2212q)pn\n\u2318\n. What does this give us?\n\u2022 Can show that this implies kv2 \u2212\u00afv2k2\n2 \uf8ffO\n\u21e3\np\n(p\u2212q)2n\n\u2318\n(exercise).\n\u2022 \u00afv2 is\n1\npn\u03c7B,C: the community indicator vector.\n\u2022 Every i where v2(i), \u00afv2(i) di\u21b5er in sign contributes \u22651\nn to kv2 \u2212\u00afv2k2\n2.\n\u2022 So they di\u21b5er in sign in at most O\n\u21e3\np\n(p\u2212q)2\n\u2318\npositions.\nA adjacency matrix of random stochastic block model graph. p: connection probability\nwithin clusters. q < p: connection probability between clusters. n: number of nodes.\nv2, \u00afv2: second eigenvectors of A and E[A] respectively.\n5\n\napplication to stochastic block model\nUpshot: If G is a stochastic block model graph with adjacency matrix A,\nif we compute its second large eigenvector v2 and assign nodes to\ncommunities according to the sign pattern of this vector, we will correctly\nassign all but O\n\u21e3\np\n(p\u2212q)2\n\u2318\nnodes.\n6\n\npower method for finding eigenvectors\nAndrew McGregor\n0\nb15]\n\npower method\nPower Method: The most fundamental iterative method for\napproximate SVD/eigendecomposition. Applies to computing k = 1\neigenvectors, but can be generalized to larger k.\nGoal: Given symmetric A 2 Rd\u21e5d, an approximation to the top\neigenvector ~v1 of A.\n\u2022 Initialize ~z(0) randomly, e.g. ~z(0)(i) \u21e0N(0, 1).\n\u2022 For i = 1, . . . , t\n\u2022 ~z(i) := A \u00b7 ~z(i\u22121)\n\u2022 ~zi :=\n~z(i)\nk~z(i)k2\n\u2022 Return ~zt\nNote that mathematically it doesn\u2019t matter if we normalize in each\niteration of only normalize at the end.\n3\n\npower method analysis\n\u2022 Write ~z(0) in A\u2019s eigenvector basis:\n~z(0) = c1~v1 + c2~v2 + . . . + cd~vd.\n\u2022 Applying A gives:\nA~z(0) = c1A~v1 + c2A~v2 + . . . + cdA~vd\n= c1\u03bb1~v1 + c2\u03bb2~v2 + . . . + cd\u03bbd~vd\n\u2022 Applying A again gives:\nAA~z(0) = c1\u03bb1A~v1 + c2\u03bb2A~v2 + . . . + cd\u03bbdA~vd\n= c1\u03bb2\n1~v1 + c2\u03bb2\n2~v2 + . . . + cd\u03bb2\nd~vd\n\u2022 Applying A t times gives:\nAt~z(0) = c1\u03bbt\n1~v1 + c2\u03bbt\n2~v2 + . . . + cd\u03bbt\nd~vd\n4\n\npower method convergence\nAfter t iterations, we have \u2018powered\u2019 up the eigenvalues, making the\ncomponent in the direction of v1 much larger, relative to the other components.\n~z(0) = c1~v1 + c2~v2 + . . . + cd~vd =) At~z(0) = c1\u03bbt\n1~v1 + c2\u03bbt\n2~v2 + . . . + cd\u03bbt\nd~vd\nIteration 0\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nc8\nc9\nc10\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n5\n\npower method convergence\nAfter t iterations, we have \u2018powered\u2019 up the eigenvalues, making the\ncomponent in the direction of v1 much larger, relative to the other components.\n~z(0) = c1~v1 + c2~v2 + . . . + cd~vd =) At~z(0) = c1\u03bbt\n1~v1 + c2\u03bbt\n2~v2 + . . . + cd\u03bbt\nd~vd\nIteration 1\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nc8\nc9\nc10\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n5\n\npower method convergence\nAfter t iterations, we have \u2018powered\u2019 up the eigenvalues, making the\ncomponent in the direction of v1 much larger, relative to the other components.\n~z(0) = c1~v1 + c2~v2 + . . . + cd~vd =) At~z(0) = c1\u03bbt\n1~v1 + c2\u03bbt\n2~v2 + . . . + cd\u03bbt\nd~vd\nIteration 13\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nc8\nc9\nc10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nWhen will convergence be slow?\n5\n\npower method slow convergence\nSlow Case: A has eigenvalues: \u03bb1 = 1, \u03bb2 = .99, \u03bb3 = .9, \u03bb4 = .8, . . .\n~z(0) = c1~v1 + c2~v2 + . . . + cd~vd =) ~z(t) = c1\u03bbt\n1~v1 + c2\u03bbt\n2~v2 + . . . + cd\u03bbt\nd~vd\nIteration 2\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nc8\nc9\nc10\n-0.2\n-0.1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n6\n\npower method slow convergence\nSlow Case: A has eigenvalues: \u03bb1 = 1, \u03bb2 = .99, \u03bb3 = .9, \u03bb4 = .8, . . .\n~z(0) = c1~v1 + c2~v2 + . . . + cd~vd =) ~z(t) = c1\u03bbt\n1~v1 + c2\u03bbt\n2~v2 + . . . + cd\u03bbt\nd~vd\nIteration 13\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nc8\nc9\nc10\n-0.1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n6\n\npower method convergence rate\n~z(0) = c1~v1 + c2~v2 + . . . + cd~vd =) ~z(t) = c1\u03bbt\n1~v1 + c2\u03bbt\n2~v2 + . . . + cd\u03bbt\n2~vd\nWrite |\u03bb2| = (1 \u2212\u03b3)|\u03bb1| for \u2018gap\u2019 \u03b3 = |\u03bb1|\u2212|\u03bb2|\n|\u03bb1|\n.\nHow many iterations t does it take to have |\u03bb2|t \uf8ff\u03b4 \u00b7 |\u03bb1|t?\nln(1/\u03b4)\n\u03b3\n.\nWill have for all i > 1, |\u03bbi|t \uf8ff|\u03bb2|t \uf8ff\u03b4 \u00b7 |\u03bb1|t.\nHow small must we set \u03b4 to ensure that c1\u03bbt\n1 dominates all other components\nand so ~z(t) is very close to ~v1?\nA 2 Rd\u21e5d: input matrix with eigendecomposition A = V\u21e4VT . ~v1: top eigenvector, being\ncomputed, ~z(i): iterate at step i, converging to ~v1.\n7\n\nrandom initialization\nClaim: When z(0) is chosen with random Gaussian entries, writing\nz(0) = c1~v1 + c2~v2 + . . . + cd~vd, with high probability, for all i:\nO(1/d2) \uf8ff|ci| \uf8ffO(log d)\nCorollary:\nmax\nj\n!!!!\ncj\nc1\n!!!! \uf8ffO(d2 log d).\nA 2 Rd\u21e5d: input matrix with eigendecomposition A = V\u21e4VT . ~v1: top eigenvector, being\ncomputed, ~z(i): iterate at step i, converging to ~v1.\n8\n\ntechnical preliminaries\n\u2022 Claim: For 0 < c < kxk2:\n!!!!\nx\nkxk2\n\u2212\ny\nkyk2\n!!!!\n2\n\uf8ff\n!!!!\nx\nc \u2212\ny\nkyk2\n!!!!\n2\n\u2022 Proof by geometry: Try drawing a picture.\n\u2022 Claim: For any vector z 2 Rd,\nkzk2 \uf8ffkzk1 := |z(1)| + |z(2)| + . . . + |z(d)|\n\u2022 Proof follows from kzk2\n1 = (|z(1)| + . . . + |z(d)|)2 \u2265kzk2\n2\n9\n\nrandom initialization\nClaim 1: If z(0) is chosen with random Gaussian entries, writing\nz(0) = c1~v1 + . . . + cd~vd, with high probability, maxj\n!!!\ncj\nc1\n!!! \uf8ffO(d2 log d).\nClaim 2: For gap \u03b3 = |\u03bb1|\u2212|\u03bb2|\n|\u03bb1|\n, and t = ln(1/\u03b4)\n\u03b3\n,\n!!!\n\u03bbt\ni\n\u03bbt\n1\n!!! \uf8ff\u03b4 for all i.\n~z(t) :=\nc1\u03bbt\n1~v1 + . . . + cd\u03bbt\nd~vd\nkc1\u03bbt\n1~v1 + . . . + cd\u03bbt\nd~vdk2 =)\nk~z(t) \u2212~v1k2 \uf8ff\n\"\"\"\"\nc1\u03bbt\n1~v1 + . . . + cd\u03bbt\nd~vd\nkc1\u03bbt\n1~v1k2\n\u2212~v1\n\"\"\"\"\n2\n=\n\"\"\"\"\nc2\u03bbt\n2\nc1\u03bbt\n1\n~v2 + . . . + cd\u03bbt\nd\nc1\u03bbt\n1\n~vd\n\"\"\"\"\n2\n\uf8ff\n!!!!\nc2\u03bbt\n2\nc1\u03bbt\n1\n!!!! + . . . +\n!!!!\ncd\u03bbt\nd\nc1\u03bbt\n1\n!!!! \uf8ff\u03b4 \u00b7 O(d2 log d) \u00b7 d\nSetting \u03b4 = O\n\u21e3\n\u270f\nd3 log d\n\u2318\ngives k~z(t) \u2212~v1k2 \uf8ff\u270f.\nA 2 Rd\u21e5d: input matrix with eigendecomposition A = V\u21e4VT . ~v1: top eigenvector, being\ncomputed, ~z(i): iterate at step i, converging to ~v1.\n10\n\npower method theorem\nTheorem (Basic Power Method Convergence)\nLet \u03b3 = |\u03bb1|\u2212|\u03bb2|\n|\u03bb1|\nbe the relative gap between the \ufb01rst and second eigenvalues.\nIf Power Method is initialized with a random Gaussian vector ~v (0) then, with\nhigh probability, after t = O\n\u21e3\nln(d/\u270f)\n\u03b3\n\u2318\nsteps:\nk~z(t) \u2212~v1k2 \uf8ff\u270f.\nTotal runtime: O(t) matrix-vector multiplications. If A = XTX:\nO\n\u2713\nnnz(X) \u00b7 ln(d/\u270f)\n\u03b3\n\u00b7\n\u25c6\n= O\n\u2713\nnd \u00b7 ln(d/\u270f)\n\u03b3\n\u25c6\n.\nwhere nnz() is the number of non-zero entries in the matrix.\n11\n\nfinding second (etc.) eigenvector\n\u2022 If A has eigenvectors v1, . . . , vn with eigenvalues \u03bb1, . . . , \u03bbn\n(|\u03bb1| \u2265. . . \u2265|\u03bbn|) then\nB = A \u2212\u03bb1v1v T\n1\nhas eigenvectors v2, . . . , vn, v1 with eigenvectors \u03bb2, . . . , \u03bbn, 0\n\u2022 Hence, to \ufb01nd the second eigenvector of A, just apply the previous\nmethod to B.\n12\n\npower method vs. random walks\nAndrew McGregor\n0\nD16\n\nconnection to random walks\nConsider a random walk on a graph G with adjacency matrix A.\n1\n\nconnection to random walks\nLet ~p(t) 2 Rn have ith entry ~p(t)\ni\n= Pr(walk at node i at step t).\n\u2022 Initialize: ~p(0) = [1, 0, 0, . . . , 0].\n\u2022 Update:\nPr(walk at i at step t) =\nX\nj2neigh(i)\nPr(walk at j at step t \u22121) \u00b7\n1\ndegree(j)\n= ~zT ~p(t\u22121)\nwhere ~z(j) =\n1\ndegree(j) for all j 2 neigh(i), ~z(j) = 0 for all j /2 neigh(i).\n\u2022 ~z is the ith row of the right normalized adjacency matrix AD\u22121.\n\u2022 ~p(t) = AD\u22121~p(t\u22121) = AD\u22121AD\u22121 . . . AD\u22121\n|\n{z\n}\nt times\n~p(0)\n2\n\nrandom walking as power method\nClaim: After t steps, the probability that a random walk is at node i is given\nby the ith entry of\n~p(t) = AD\u22121AD\u22121 . . . AD\u22121\n|\n{z\n}\nt times\n~p(0).\nD\u22121/2~p(t) = (D\u22121/2AD\u22121/2)(D\u22121/2AD\u22121/2) . . . (D\u22121/2AD\u22121/2)\n|\n{z\n}\nt times\n(D\u22121/2~p(0)).\n\u2022 D\u22121/2~p(t) is exactly what would obtained by applying t iterations of power\nmethod to D\u22121/2~p(0)!\n\u2022 Will converge to the top eigenvector of the normalized adjacency matrix\nD\u22121/2AD\u22121/2. Stationary distribution.\n\u2022 Like the power method, the time a random walk takes to converge to its\nstationary distribution (mixing time) is dependent on the gap between the\ntop two eigenvalues of D\u22121/2AD\u22121/2. The spectral gap.\n3\n\nsection 3: optimization overview\nAndrew McGregor\n0\n001\n\nsummary of part iii: optimization\n\u2022 In this last section, we will investigate general iterative algorithms for\noptimization, speci\ufb01cally gradient descent and its variants.\n\u2022 What are these methods, when are they applied, and how do you analyze\ntheir performance?\n\u2022 Small taste of what you can \ufb01nd in COMPSCI 651: Optimization in\nComputer Science.\n1\n\nmathematical setup\nGiven some function f : Rd ! R, \ufb01nd ~\u2713? with:\nf (~\u2713?) = min\n~\u27132Rd f (~\u2713) + \u270f\nup to some small additive approximation term \u270f.\nOften under some constraints, e.g.,\nk~\u2713k2 \uf8ff1 , k~\u2713k1 \uf8ff1 , A~\u2713\uf8ff~b , ~\u2713TA~\u2713\u22650 , or\nd\nX\ni=1\n~\u2713(i) \uf8ffc .\n2\n\ncontinuous optimization examples\nThe methods we consider work for the functions on the left (these are\n\u201cconvex\u201d) but not necessarily for those with multiple local minimum.\n3\n\nwhy continuous optimization?\nModern machine learning centers around continuous optimization.\nTypical Set Up: (supervised machine learning)\n\u2022 Have a model, which is a function mapping inputs to predictions (neural\nnetwork, linear function, low-degree polynomial etc).\n\u2022 The model is parameterized by a parameter vector (weights in a neural\nnetwork, coe\ufb03cients in a linear function or polynomial)\n\u2022 Want to train this model on input data, by picking a parameter vector such\nthat the model does a good job mapping inputs to predictions on your\ntraining data.\nThis training step is typically formulated as a continuous optimization problem.\n5\n\noptimization in ml\nExample 1: Linear Regression, e.g., predicting house prices based on d\nfeatures (sq. footage, average price of houses in neighborhood. . . )\nModel: M~\u2713: Rd ! R with M~\u2713(~x)\ndef\n= ~\u2713(1) \u00b7 ~x(1) + . . . + ~\u2713(d) \u00b7 ~x(d).\nParameter Vector: ~\u27132 Rd (the regression coe\ufb03cients)\nOptimization Problem: Given data points (training points) ~x1, . . . , ~xn (the\nrows of data matrix X 2 Rn\u21e5d) and labels y1, . . . , yn 2 R, \ufb01nd ~\u2713\u21e4minimizing\nthe loss function:\nLX,y(~\u2713) =\nn\nX\ni=1\n`(M~\u2713(~xi), yi)\nwhere ` is some measurement of how far M~\u2713(~xi) is from yi.\n\u2022 Least Squares Regression: `(M~\u2713(~xi), yi) =\n\"\nM~\u2713(~xi) \u2212yi\n#2\n\u2022 Logistic Regression: yi 2 {\u22121, 1} and\n`(M~\u2713(~xi), yi) = ln\n\"\n1 + exp(\u2212yiM~\u2713(~xi))\n#\n6\n\noptimization in ml\nExample 2: Neural Networks\nModel: M~\u2713: Rd ! R. M~\u2713(~x) = h~wout, \u03c3(W2\u03c3(W1~x))i.\nParameter Vector: ~\u27132 R(# edges) (the weights on every edge)\nOptimization Problem: Given data points ~x1, . . . , ~xn and labels z1, . . . , zn 2 R,\n\ufb01nd ~\u2713\u21e4minimizing the loss function:\nLX,~y(~\u2713) =\nn\nX\ni=1\n`(M~\u2713(~xi), zi)\n7\n\noptimization in ml\nLX,~y(~\u2713) =\nn\nX\ni=1\n`(M~\u2713(~xi), yi)\n\u2022 Supervised means we have labels y1, . . . , yn for the training points.\n\u2022 Solving the \ufb01nal optimization problem has many di\u21b5erent names: likelihood\nmaximization, empirical risk minimization, minimizing training loss, etc.\n\u2022 Continuous optimization is also very common in unsupervised learning.\n(PCA, spectral clustering, etc.)\n\u2022 Generalization tries to explain why minimizing the loss LX,~y(~\u2713) on the\ntraining points minimizes the loss on future test points. I.e., makes us have\ngood predictions on future inputs.\n8\n\noptimization algorithms\nChoice of optimization algorithm for minimizing f (~\u2713) will depend on:\n\u2022 The form of f (in ML, depends on the model & loss function).\n\u2022 Any constraints on ~\u2713(e.g., k~\u2713k < c).\n\u2022 Computational constraints, such as memory constraints.\n9\n\noptimization algorithms\nChoice of optimization algorithm for minimizing f (~\u2713) will depend on:\n\u2022 The form of f (in ML, depends on the model & loss function).\n\u2022 Any constraints on ~\u2713(e.g., k~\u2713k < c).\n\u2022 Computational constraints, such as memory constraints.\nIn this section, we will introduce the gradient descent algorithm (with and\nwithout constraints), the stochastic gradient descent algorithm, and the online\ngradient descent algorithm.\n9\n\ngradient descent in one dimension\nAndrew McGregor\n0\n27\n!\n\ngradient descent\nGradient descent (and some important variants)\n\u2022 An extremely simple greedy iterative method, that can be applied to almost\nany continuous function we care about optimizing.\n\u2022 Often not the \u2018best\u2019 choice for a given function, but it is the approach of\nchoice in ML since it is simple, general, and often works very well.\n\u2022 At each step, tries to move towards the lowest nearby point in the function\nthat is can \u2013 in the opposite direction of the gradient.\n1\n\nconvexity in 1d\nDe\ufb01nition: A function f : R ! R is convex i\u21b5, for any \u27131, \u27132 2 R:\nf (\u27132) \u2265f (\u27131) + f 0(\u27131)(\u27132 \u2212\u27131)\n\u2022 Let \u2713\u21e4= arg min\u2713f (\u2713). Then f 0(\u2713) < 0 when \u2713< \u2713\u21e4and f 0(\u2713) \u22650 when\n\u2713> \u2713\u21e4. Furthermore, |f 0(\u2713)| gets smaller as \u2713gets closer to \u2713\u21e4.\n\u2022 Exercise: If f is convex, the for all \u27131, \u27132 2 R\nf (\u27132) \u2212f (\u27131) \uf8fff 0(\u27132)(\u27132 \u2212\u27131)\nand\nf (\u27131/2 + \u27132/2) \uf8fff (\u27131)/2 + f (\u2713)2/2 .\n2\n\nbasic idea of gradient descent\nGradient Descent Update in 1D:\n\u2022 Set \u27131 arbitrarily.\n\u2022 For i = 1 to t \u22121:\n\u2713i+1 = \u2713i \u2212\u2318f 0(\u2713i)\ni.e., increase \u2713if negative derivative and decrease \u2713if positive\nderivative. \u2318is small \ufb01xed value.\n\u2022 Return \u2713= arg min\u27131,...\u2713t f (\u2713i).\nExample: f (x) = (x \u22121)2, \u27131 = 2, and \u2318= 0.2\n\u2022 Compute derivative f 0(x) = 2(x \u22121)\n\u2022 \u27132 = \u27131 \u2212\u2318f 0(\u27131) = 2 \u22120.2 \u21e5f 0(2) = 2 \u22120.2 \u21e52 = 1.6.\n\u2022 \u27133 = \u27132 \u2212\u2318f 0(\u27132) = 1.6 \u22120.2 \u21e5f 0(1.6) = 1.6 \u22120.2 \u21e51.2 = 1.36.\n3\n\ngd analysis proof for d = 1\nTheorem: For convex function f : R ! R where |f 0(\u2713)| \uf8ffG for all \u2713,\nGD run with t \u2265R2G2\n\u270f2\niterations, \u2318=\nR\nGpt , and starting point within R\nof \u2713\u21e4, outputs \u02c6\u2713satisfying f (\u02c6\u2713) \uf8fff (\u2713\u21e4) + \u270f.\n\u2022 Substituting \u2713i+1 = \u2713i \u2212\u2318f 0(\u2713i) and letting ai = \u2713i \u2212\u2713\u21e4gives:\na2\ni+1 = (\u2713i+1 \u2212\u2713\u21e4)2 = (ai \u2212\u2318f 0(\u2713i))2 = a2\ni \u22122\u2318f 0(\u2713i)ai + (\u2318f 0(\u2713i))2\n\u2022 Rearrange and use convexity to show:\nf (\u2713i) \u2212f (\u2713\u21e4) \uf8fff 0(\u2713i)ai =\n1\n2\u2318\n!\na2\ni \u2212a2\ni+1\n\"\n+ \u2318(f 0(\u2713i))2/2\n\u2022 Summing over i and using the fact |f 0(\u2713i)| \uf8ffG,\n1\nt\nPt\ni=1 (f (\u2713i) \u2212f (\u2713\u21e4)) \uf8ff\n\u21e3\n1\n2t\u2318\nPt\ni=1(a2\ni \u2212a2\ni+1)\n\u2318\n+ \u2318G2\n2\n\uf8ff\na2\n1\n2t\u2318+ \u2318G2\n2\n\u2022 Using a2\n1 \uf8ffR2 and f (\u02c6\u2713) \u2212f (\u2713\u21e4) \uf8ff1\nt\nPt\ni=1 (f (\u2713i) \u2212f (\u2713\u21e4))\nf (\u02c6\u2713) \uf8fff (\u2713\u21e4) + R2\n2t\u2318+ \u2318G2\n2\n\uf8fff (\u2713\u21e4) + \u270f\n4\n\nmultivariate convexity and calculus\nAndrew McGregor\n0\nI\n\nmultivariate calculus review\nLet ~ei 2 Rd denote the ith standard basis vector,\n~ei = [0, 0, 1, 0, 0, . . . , 0]\n|\n{z\n}\n1 at position i\n.\nPartial Derivative: For f : Rd ! R,\n@f\n@~\u2713(i)\n= lim\n\u270f!0\nf (~\u2713+ \u270f\u00b7 ~ei) \u2212f (~\u2713)\n\u270f\n.\nGradient: A vector of the partial derivatives.\n~rf (~\u2713) =\n2\n666664\n@f\n@~\u2713(1)\n@f\n@~\u2713(2)\n...\n@f\n@~\u2713(d)\n3\n777775\n1\n\nmultivariate calculus review\nDirectional Derivative: For vector ~v,\nD~v f (~\u2713) = lim\n\u270f!0\nf (~\u2713+ \u270f~v) \u2212f (~\u2713)\n\u270f\n.\nDirectional Derivative in Terms of the Gradient:\nD~v f (~\u2713) = h~v, ~rf (~\u2713)i.\n2\n\nexample of gradients\n\u2022 Suppose f : R3 ! R where f (~\u2713) = \u27133\n1 + \u27132\u27133 + \u27132\n3 then\nrf (~\u2713) =\n0\nB\n@\n3\u27132\n1\n\u27133\n\u27132 + 2\u27133\n1\nC\nA\nand\nkrf (~\u2713)k2 =\nq\n(3\u27132\n1)2 + (\u27133)2 + (\u27132 + 2\u27133)2\n\u2022 Suppose f : R3 ! R where f (~\u2713) = 3\u27131 + \u27132 + 5\u27133 then\nrf (~\u2713) =\n0\nB\n@\n3\n1\n5\n1\nC\nA\nand krf (~\u2713)k2 =\np\n32 + 12 + 52.\n3\n\nconvexity\nConvex Function: A function f : Rd ! R is convex i\u21b5, for any ~\u27131, ~\u27132 2\nRd and \u03bb 2 [0, 1]:\n(1 \u2212\u03bb) \u00b7 f (~\u27131) + \u03bb \u00b7 f (~\u27132) \u2265f\n\u21e3\n(1 \u2212\u03bb) \u00b7 ~\u27131 + \u03bb \u00b7 ~\u27132\n\u2318\n4\n\nconvexity\nCorollary: A function f : R ! R is convex i\u21b5, for any \u27131, \u27132 2 R:\nf (\u27132) \u2265f (\u27131) + f 0(\u27131)(\u27132 \u2212\u27131)\nMore generally, a function f : Rd ! R is convex if and only if, for any\n~\u27131, ~\u27132 2 Rd: f (~\u27132) \u2212f (~\u27131) \u2265~rf (~\u27131)T \u21e3\n~\u27132 \u2212~\u27131\n\u2318\n5\n\nequivalent definitions for convexity\nWe de\ufb01ned convexity of f : Rd ! R in two ways:\n(1) For all x, y 2 Rd, \u03bb 2 [0, 1], \u03bbf (x) + (1 \u2212\u03bb)f (y) \u2265f (\u03bbx + (1 \u2212\u03bb)y).\n(2) For all x, y 2 Rd, f (x) \uf8fff (y) + hrf (x), x \u2212yi\nTo see (1) implies (2)\nhrf (x), y \u2212xi = Dy\u2212xf (x) = lim\n\u270f!0\nf (x + \u270f(y \u2212x)) \u2212f (x)\n\u270f\n= lim\n\u270f!0\nf ((1 \u2212\u270f)x + \u270fy) \u2212f (x)\n\u270f\n\uf8fflim\n\u270f!0\n(1 \u2212\u270f)f (x) + \u270ff (y) \u2212f (x)\n\u270f\n=f (y) \u2212f (x)\n6\n\nequivalent definitions for convexity\nWe de\ufb01ned convexity of f : Rd ! R in two ways:\n(1) For all x, y 2 Rd, \u03bb 2 [0, 1], \u03bbf (x) + (1 \u2212\u03bb)f (y) \u2265f (\u03bbx + (1 \u2212\u03bb)y).\n(2) For all x, y 2 Rd, f (x) \uf8fff (y) + hrf (x), x \u2212yi\nTo see (2) implies (1)\nf (\u03bbx + (1 \u2212\u03bb)y) \uf8fff (x) + hrf (\u03bbx + (1 \u2212\u03bb)y), \u03bbx + (1 \u2212\u03bb)y \u2212xi\nf (\u03bbx + (1 \u2212\u03bb)y) \uf8fff (y) + hrf (\u03bbx + (1 \u2212\u03bb)y), \u03bbx + (1 \u2212\u03bb)y \u2212yi\n\u03bb times the \ufb01rst equation plus (1 \u2212\u03bb) times the second equation gives\nf (\u03bbx + (1 \u2212\u03bb)y) \uf8ff\u03bbf (x) + (1 \u2212\u03bb)f (y)\n7\n\nhigher dimensional gradient descent\nAndrew McGregor\n0\n23/\n\nrecap: gradient descent in 1d\nLet f : R ! R be a convex function with \u2713\u21e4= arg min\u2713f (\u2713).\nGradient Descent Update in 1D:\n\u2022 Set \u27131 arbitrarily.\n\u2022 For i = 1 to t \u22121:\n\u2713i+1 = \u2713i \u2212\u2318f 0(\u2713i)\ni.e., increase \u2713if negative derivative and decrease \u2713if positive\nderivative. \u2318is small \ufb01xed value.\n\u2022 Return \u2713= arg min\u27131,...\u2713t f (\u2713i).\nWe proved that if |f 0(\u2713)| \uf8ffG for all \u2713, t \u2265R2G 2\n\u270f2 , \u2318=\nR\nGpt , and\n|\u27131 \u2212\u2713\u21e4| \uf8ffR then the output satis\ufb01es \u02c6\u2713satisfying f (\u02c6\u2713) \uf8fff (\u2713\u21e4) + \u270f.\n1\n\ngradient descent greedy approach\nGradient descent is a greedy iterative optimization algorithm: Starting at ~\u2713(1),\nin each iteration let\n~\u2713(i) = ~\u2713(i\u22121) + \u2318~v\nwhere \u2318is the \u2018step size\u2019 and ~v is a direction chosen to make f (~\u2713(i\u22121) + \u2318~v)\nsmall. How should we choose ~v?\nD~v f (~\u2713(i\u22121)) = lim\n\u270f!0\nf (~\u2713(i\u22121) + \u270f~v) \u2212f (~\u2713(i\u22121))\n\u270f\n.\nSo for small \u2318:\nf (~\u2713(i)) \u2212f (~\u2713(i\u22121)) = f (~\u2713(i\u22121) + \u2318~v) \u2212f (~\u2713(i\u22121)) \u21e1\u2318\u00b7 D~vf (~\u2713(i\u22121))\n= \u2318\u00b7 h~v, ~rf (~\u2713(i\u22121))i.\nWe want to minimize h~v, ~rf (~\u2713(i\u22121))i, so choose ~v pointing in the opposite\ndirection of ~rf (~\u2713(i\u22121)), i.e., \u2212rf (~\u2713(i\u22121)).\n2\n\nGradient Descent analysis for convex, Lipschitz functions.\n4\n\nlipschitz functions\nGradient Descent Update:\n~\u2713i+1 = ~\u2713i \u2212\u2318rf (~\u2713i)\nFor fast convergence, need to assume that the function is Lipschitz, i.e.,\nsize of gradient k~rf (~\u2713)k2 is bounded. We\u2019ll assume\n8~\u27131, ~\u27132 :\n|f (~\u27131) \u2212f (~\u27132)| \uf8ffG \u00b7 k~\u27131 \u2212~\u27132k2\n5\n\ngd analysis \u2013 convex functions\nAssume that:\n\u2022 f is convex.\n\u2022 f is G Lipschitz, i.e., k~rf (~\u2713)k2 \uf8ffG for all ~\u2713.\n\u2022 k~\u27131 \u2212~\u2713\u21e4k2 \uf8ffR where ~\u27131 is the initialization point.\nGradient Descent\n\u2022 Choose some initialization ~\u27131 and set \u2318=\nR\nGpt .\n\u2022 For i = 1, . . . , t \u22121\n\u2022 ~\u2713i+1 = ~\u2713i \u2212\u2318rf (~\u2713i)\n\u2022 Return \u02c6\u2713= arg min~\u27131,...~\u2713t f (~\u2713i).\n6\n\ngd analysis proof\nTheorem: For convex G-Lipschitz function f : Rd ! R, GD run with\nt \u2265R2G2\n\u270f2\niterations, \u2318=\nR\nGpt , and starting point within radius R of ~\u2713\u21e4,\noutputs \u02c6\u2713satisfying f (\u02c6\u2713) \uf8fff (~\u2713\u21e4) + \u270f.\n\u2022 Step 1: ~rf (~\u2713i)T~ai \uf8ff\nk~ai k2\n2\u2212k~ai+1k2\n2\n2\u2318\n+ \u2318G2\n2\nwhere ~ai = ~\u2713i \u2212~\u2713\u21e4.\nProof:\nk~ai+1k2\n2\n=\nk~ai \u2212\u2318~rf (~\u2713i)k2\n2\n=\nk~aik2\n2 \u22122\u2318~rf (~\u2713i)T~ai + k\u2318~rf (\u2713i)k2\n2\n\uf8ff\nk~aik2\n2 \u22122\u2318~rf (~\u2713i)T~ai + \u23182G 2\nusingka \u2212bk2\n2 = kak2\n2 \u22122aTb + kbk2\n2. Then rearrange.\n\u2022 Step 2: By convexity, for all i,\nf (~\u2713i) \u2212f (~\u2713\u21e4) \uf8ff~rf (~\u2713i)T~ai \uf8ffk~aik2\n2 \u2212k~ai+1k2\n2\n2\u2318\n+ \u2318G 2\n2\n.\n7\n\ngd analysis proof\nTheorem: For convex G-Lipschitz function f : Rd ! R, GD run with\nt \u2265R2G2\n\u270f2\niterations, \u2318=\nR\nGpt , and starting point within radius R of ~\u2713\u21e4,\noutputs \u02c6\u2713satisfying f (\u02c6\u2713) \uf8fff (~\u2713\u21e4) + \u270f.\n\u2022 So far: For all i, f (~\u2713i) \u2212f (~\u2713\u21e4) \uf8ff\nk~ai k2\n2\u2212k~ai+1k2\n2\n2\u2318\n+ \u2318G2\n2\n\u2022 Sum over i:\nt\nX\ni=1\n\u21e3\nf (~\u2713i) \u2212f (~\u2713\u21e4)\n\u2318\n\uf8ff\nt\u2318G 2\n2\n+ 1\n2\u2318\nt\u22121\nX\ni=0\n\u21e3\nk~aik2\n2 \u2212k~ai+1k2\n2\n\u2318\n\uf8ff\nt\u2318G 2\n2\n+ 1\n2\u2318k~\u27130 \u2212~\u2713\u21e4k2\n2 \uf8fft\u2318G 2\n2\n+ R2\n2\u2318\n\u2022 Substitute \u2318and t: And using 1\nt\nPt\ni=1 f (~\u2713i) \u2265f (\u02c6\u2713) implies\nf (\u02c6\u2713) \u2212f (~\u2713\u21e4) \uf8ff1\nt\nt\nX\ni=1\nf (~\u2713i) \u2212f (~\u2713\u21e4) \uf8ff\nR2\n2\u2318\u00b7 t + \u2318G 2\n2\n\uf8ff\u270f.\n8\n\nfunction access\nOften the functions we are trying to optimize are very complex (e.g., a\nneural network). We will assume access to:\nFunction Evaluation: Can compute f (~\u2713) for any ~\u2713.\nGradient Evaluation: Can compute ~rf (~\u2713) for any ~\u2713.\nIn neural networks:\n\u2022 Function evaluation is called a forward pass (propogate an input\nthrough the network).\n\u2022 Gradient evaluation is called a backward pass (compute the gradient\nvia chain rule, using backpropagation).\n9\n\nprojected gradient descent\nAndrew McGregor\n0\n-1\n\nconstrained convex optimization\nOften want to perform convex optimization with convex constraints.\n~\u2713\u21e4= arg min\n~\u27132S\nf (~\u2713),\nwhere S is a convex set.\nDe\ufb01nition (Convex Set): A set S \u2713Rd is convex if and only if, for any\n~\u27131, ~\u27132 2 S and \u03bb 2 [0, 1]: (1 \u2212\u03bb)~\u27131 + \u03bb \u00b7 ~\u27132 2 S\nFor any convex set let PS(\u00b7) denote the projection function onto S:\nPS(~y) = arg min\n~\u27132S\nk~\u2713\u2212~yk2\n\u2022 For S = {~\u27132 Rd : k~\u2713k2 \uf8ff1} what is PS(~y)?\n\u2022 For S being a k dimensional subspace of Rd, what is PS(~y)?\n1\n\nprojected gradient descent\nProjected Gradient Descent\n\u2022 Choose some initialization ~\u27131 and set \u2318=\nR\nGpt .\n\u2022 For i = 1, . . . , t \u22121\n\u2022 ~\u2713(out)\ni+1\n= ~\u2713i \u2212\u2318\u00b7 ~rf (~\u2713i)\n\u2022 ~\u2713i+1 = PS(~\u2713(out)\ni+1 ).\n\u2022 Return \u02c6\u2713= arg min~\u2713i f (~\u2713i).\n2\n\nconvex projections\nAnalysis of projected gradient descent is almost identifcal to gradient descent\nanalysis! Just need to appeal to following geometric result:\nTheorem (Projection to a convex set): For any convex set S \u2713Rd,\n~y 2 Rd, and ~\u27132 S,\nkPS(~y) \u2212~\u2713k2 \uf8ffk~y \u2212~\u2713k2.\n3\n\nprojected gradient descent analysis\nTheorem (Projected GD): For convex G-Lipschitz function f , and con-\nvex set S, Projected GD run with t \u2265\nR2G2\n\u270f2\niterations, \u2318=\nR\nGpt , and\nstarting point within radius R of ~\u2713\u21e4= min~\u27132S f (~\u2713), outputs \u02c6\u2713satisfying\nf (\u02c6\u2713) \uf8fff (~\u2713\u21e4) + \u270f\nRecall: ~\u2713(out)\ni+1\n= ~\u2713i \u2212\u2318\u00b7 ~rf (~\u2713i) and ~\u2713i+1 = PS(~\u2713(out)\ni+1 ).\nProof from earlier establishes that for all i,\nf (~\u2713i) \u2212f (~\u2713\u21e4) \uf8ffk~\u2713i \u2212\u2713\u21e4k2\n2 \u2212k~\u2713(out)\ni+1 \u2212~\u2713\u21e4k2\n2\n2\u2318\n+ \u2318G 2\n2\n.\nBut Projection Lemma then ensures that for all i,\nf (~\u2713i) \u2212f (~\u2713\u21e4) \uf8ffk~\u2713i \u2212~\u2713\u21e4k2\n2 \u2212k~\u2713i+1 \u2212~\u2713\u21e4k2\n2\n2\u2318\n+ \u2318G 2\n2\nRest of proof unchanged: f (\u02c6\u2713) \u2212f (~\u2713\u21e4) \uf8ff1\nt\nPt\ni=1 f (~\u2713i) \u2212f (~\u2713\u21e4) \uf8ff\nR2\n2\u2318\u00b7t + \u2318G2\n2 .\n4\n\nonline gradient descent\nAndrew McGregor\n0I\n\nonline gradient descent\nIn reality many learning problems are online.\n\u2022 Websites optimize ads or recommendations to show users, given\ncontinuous feedback from these users.\n\u2022 Spam \ufb01lters are incrementally updated and adapt as they see more\nexamples of spam over time.\n\u2022 Face recognition systems, other classi\ufb01cation systems, learn from\nmistakes over time.\nWant to minimize some global loss L(~\u2713, X) = Pn\ni=1 `(~\u2713, ~xi), when data\npoints are presented in an online fashion ~x1, ~x2, . . . , ~xn (similar to\nstreaming algorithms)\n1\n\nonline optimization formal setup\nOnline Optimization: In place of a single function f , we see a di\u21b5erent\nobjective function at each step:\nf1, f2, . . . , ft : Rd ! R\n\u2022 At each step, \ufb01rst pick (play) a parameter vector ~\u2713(i).\n\u2022 Then are told fi and incur cost fi(~\u2713(i)).\n\u2022 Goal: Minimize total cost Pt\ni=1 fi(~\u2713(i)).\nOur analysis will make no assumptions on how f1, . . . , ft are related to\neach other!\n2\n\nonline optimization example\nHome pricing tools.\n\u2022 Parameter vector ~\u2713(i): coe\ufb03cients of linear model at step i.\n\u2022 Functions f1, . . . , ft: fi(~\u2713(i)) = (h~xi, ~\u2713(i)i \u2212pricei)2 revealed when\nhomei is listed or sold.\n\u2022 Want to minimize total squared error Pt\ni=1 fi(~\u2713(i)) (same as classic\nleast squares regression).\n3\n\nonline optimization example\nUI design via online optimization.\n\u2022 Parameter vector ~\u2713(i): some encoding of the layout at step i.\n\u2022 Functions f1, . . . , ft: fi(~\u2713(i)) = 1 if user does not click \u2018add to cart\u2019 and\nfi(~\u2713(i)) = 0 if they do click.\n\u2022 Want to maximize number of purchases, i.e., minimize Pt\ni=1 fi(~\u2713(i)).\n4\n\nregret\nIn normal optimization, we seek \u02c6\u2713satisfying:\nf (\u02c6\u2713) \uf8ffmin\n~\u2713\nf (~\u2713) + \u270f.\nIn online optimization we want:\nt\nX\ni=1\nfi(~\u2713(i)) \uf8ff\nt\nX\ni=1\nfi(~\u2713o\u21b5) + \u270f\nwhere ~\u2713o\u21b5= arg min~\u2713\nPt\ni=1 fi(~\u2713) and \u270fis called the regret and \u270f/t is the\naverage regret.\n\u2022 This error metric is a bit unusual: Comparing online solution to best\n\ufb01xed \u201conline\u201d solution in hindsight. \u270fcan be negative!\n5\n\nintuition check\nWhat if for i = 1, . . . , t, fi(\u2713) = |\u2713\u22121000| or fi(\u2713) = |\u2713+ 1000| in an\nalternating pattern?\nHow small can the regret \u270fbe? Pt\ni=1 fi(~\u2713(i)) \uf8ffPt\ni=1 fi(~\u2713o\u21b5) + \u270f.\nWhat if for i = 1, . . . , t, fi(\u2713) = |\u2713\u22121000| or fi(\u2713) = |\u2713+ 1000| in no\nparticular pattern? How can any online learning algorithm hope to\nachieve small regret?\n6\n\nonline gradient descent\nAssume that:\n\u2022 f1, . . . , ft are all convex.\n\u2022 Each fi is G-Lipschitz, i.e., k~rfi(~\u2713)k2 \uf8ffG for all ~\u2713.\n\u2022 k~\u2713(1) \u2212~\u2713o\u21b5k2 \uf8ffR where \u2713(1) is the \ufb01rst vector chosen.\nOnline Gradient Descent\n\u2022 Pick some initial ~\u2713(1).\n\u2022 Set step size \u2318=\nR\nGpt .\n\u2022 For i = 1, . . . , t\n\u2022 Play ~\u2713(i) and incur cost fi(~\u2713(i)).\n\u2022 ~\u2713(i+1) = ~\u2713(i) \u2212\u2318\u00b7 ~rfi(~\u2713(i))\n7\n\nonline gradient descent analysis\nTheorem: For convex G-Lipschitz f1, . . . , ft, OGD initialized with start-\ning point \u2713(1) within radius R of \u2713o\u21b5, using step size \u2318=\nR\nGpt , has regret\nbounded by:\n\"\nt\nX\ni=1\nfi(\u2713(i)) \u2212\nt\nX\ni=1\nfi(\u2713o\u21b5)\n#\n\uf8ffRG\np\nt\nAverage regret goes to 0 and t ! 1.\nNo assumptions on f1, . . . , ft!\nStep 1: For all i,\nrfi(\u2713(i))T(\u2713(i) \u2212\u2713o\u21b5) \uf8ffk\u2713(i) \u2212\u2713o\u21b5k2\n2 \u2212k\u2713(i+1) \u2212\u2713o\u21b5k2\n2\n2\u2318\n+ \u2318G 2\n2\nStep 2: Convexity implies that for all i,\nfi(\u2713(i)) \u2212fi(\u2713o\u21b5) \uf8ffk\u2713(i) \u2212\u2713o\u21b5k2\n2 \u2212k\u2713(i+1) \u2212\u2713o\u21b5k2\n2\n2\u2318\n+ \u2318G 2\n2 .\n8\n\nonline gradient descent analysis\nTheorem: For convex G-Lipschitz f1, . . . , ft, OGD initialized with start-\ning point \u2713(1) within radius R of \u2713o\u21b5, using step size \u2318=\nR\nGpt , has regret\nbounded by:\n\"\nt\nX\ni=1\nfi(\u2713(i)) \u2212\nt\nX\ni=1\nfi(\u2713o\u21b5)\n#\n\uf8ffRG\np\nt\nStep 2: For all i,\nfi(\u2713(i)) \u2212fi(\u2713o\u21b5) \uf8ffk\u2713(i) \u2212\u2713o\u21b5k2\n2 \u2212k\u2713(i+1) \u2212\u2713o\u21b5k2\n2\n2\u2318\n+ \u2318G 2\n2\nStep 3:\n\"\nt\nX\ni=1\nfi(\u2713(i)) \u2212\nt\nX\ni=1\nfi(\u2713o\u21b5)\n#\n\uf8ff\nt\nX\ni=1\nk\u2713(i) \u2212\u2713o\u21b5k2\n2 \u2212k\u2713(i+1) \u2212\u2713o\u21b5k2\n2\n2\u2318\n+ t \u00b7 \u2318G 2\n2\n=\nk\u2713(1) \u2212\u2713o\u21b5k2\n2 \u2212k\u2713(t+1) \u2212\u2713o\u21b5k2\n2\n2\u2318\n+ t \u00b7 \u2318G 2\n2\n\uf8ff\nR2/(2\u2318) + t\u2318G 2/2 = RG\np\nt\n9\n\nstochastic gradient descent\nAndrew McGregor\n0\u00b7\n\nstochastic gradient descent\nStochastic gradient descent is an e\ufb03cient o\u270fine optimization method,\nseeking \u02c6\u2713with\nf (\u02c6\u2713) \uf8ffmin\n~\u2713\nf (~\u2713) + \u270f\n\u2022 The most popular optimization method in modern machine learning.\nEasily analyzed as a special case of online gradient descent!\n\u2022 Basic Idea: In gradient descent, we set ~\u2713i+1 = ~\u2713i \u2212\u2318\u00b7 ~rf (~\u2713i). In\nstochastic gradient descent we don\u2019t compute ~rf (~\u2713i) exactly but\ninstead do something random that is correct in expectation. This saves\ntime per step but might increase the number of steps.\n1\n\nstochastic gradient descent\nAssume that:\n\u2022 f is convex and decomposable as f (~\u2713) = Pn\nj=1 fj(~\u2713).\n\u2022 For example, trying to minimize a loss function over a data set X,\nL(~\u2713, X) = Pn\nj=1 `(~\u2713, ~xj) that is a sum of losses of element in data set.\n\u2022 Each fj is G\nn -Lipschitz:\nkrf (~\u2713)k2 \uf8ffk Pn\nj=1 rfj(~\u2713)k2 \uf8ffPn\nj=1 krfj(~\u2713)k2 \uf8ffG .\n\u2022 Initialize with \u2713(1) satisfying k~\u2713(1) \u2212~\u2713\u21e4k2 \uf8ffR.\nStochastic Gradient Descent\n\u2022 Pick some initial ~\u2713(1).\n\u2022 Set step size \u2318=\nR\nGpt .\n\u2022 For i = 1, . . . , t\n\u2022 Pick random ji 2 1, . . . , n.\n\u2022 ~\u2713(i+1) = ~\u2713(i) \u2212\u2318\u00b7 ~rfji (~\u2713(i))\n\u2022 Return \u02c6\u2713= 1\nt\nPt\ni=1 ~\u2713(i).\n2\n\nexample\nIf f (x, y) = (x2 + 3xy) + (x + y) then gradient descent updates\n\u2713i+1 = \u2713i \u2212\u2318\n \n2\u2713i\n1 + 3\u2713i\n2 + 1\n3\u2713i\n1 + 1\n!\nWith probability 1/2, stochastic gradient descent updates\n\u2713i+1 = \u2713i \u2212\u2318\n \n2\u2713i\n1 + 3\u2713i\n2\n3\u2713i\n1\n!\nand with probability 1/2 the update is:\n\u2713i+1 = \u2713i \u2212\u2318\n \n1\n1\n!\n3\n\nstochastic gradient descent\n~\u2713(i+1) = ~\u2713(i) \u2212\u2318\u00b7 ~rfji(~\u2713(i))\nvs.\n~\u2713(i+1) = ~\u2713(i) \u2212\u2318\u00b7 ~rf (~\u2713(i))\nNote that: E[~rfji(~\u2713(i))] = 1\nn ~rf (~\u2713(i)).\nAnalysis extends to any algorithm that takes the gradient step in\nexpectation (minibatch SGD, randomly quantized, measurement noise,\ndi\u21b5erentially private, etc.)\n4\n\nstochastic gradient descent analysis\nTheorem \u2013 SGD on Convex Lipschitz Functions: SGD run with t \u2265\nR2G2\n\u270f2\niterations, \u2318=\nR\nGpt , and starting point within radius R of \u2713\u21e4,\noutputs \u02c6\u2713satisfying: E[f (\u02c6\u2713)] \uf8fff (\u2713\u21e4) + \u270f.\nStep 1: f (\u02c6\u2713) \u2212f (\u2713\u21e4) \uf8ff1\nt\nPt\ni=1[f (\u2713(i)) \u2212f (\u2713\u21e4)] since\nf (\u02c6\u2713) = f (\nt\nX\ni=1\n\u2713(i)/t) \uf8ff1\nt\nt\nX\ni=1\nf (\u2713(i)) by convexity\nStep 2: E[f (\u02c6\u2713) \u2212f (\u2713\u21e4)] \uf8ffn\nt \u00b7 E\nhPt\ni=1[fji (\u2713(i)) \u2212fji (\u2713\u21e4)]\ni\nsince\nE[fji (~\u2713)] = 1\nn f (~\u2713) since f (~\u2713) = Pn\nj=1 fj(~\u2713)\nStep 3: E[f (\u02c6\u2713) \u2212f (\u2713\u21e4)] \uf8ffn\nt \u00b7 R \u00b7 G\nn \u00b7\np\nt\n|\n{z\n}\nOGD bound\n= RG\npt .\n5\n\nsgd vs. gd: time per iteration\nStochastic gradient descent generally makes more iterations than\ngradient descent.\nEach iteration is much cheaper (by a factor of n).\n~r\nn\nX\nj=1\nfj(~\u2713) vs. ~rfj(~\u2713)\n6\n",
        "added_at": "2025-11-30T01:36:43.415103"
      }
    ]
  },
  "5ac84717-ab67-420b-89ab-78cbff92047b": {
    "created_at": "2025-12-01T15:59:04.798356",
    "files": [
      {
        "name": "Archaeology Notes.pdf",
        "type": "document",
        "text": "9/4/25 Introduction to Archaeology \n-\u200b\nDefinition: The study of the human past through the material remains of human activity.  \n-\u200b\nEverything from the earliest human ancestors 2.5 million years ago, to the 21st century  \n-\u200b\nHow do cultures and societies develop \u2192 not just the historical aspect, but social as well \n-\u200b\nNot just what happens, but all of social life (cultural beliefs, values, subsistence practice, \nart, political/social organization, institutions, technology) throughout all human existence. \n-\u200b\nThis is what makes archaeology a subdiscipline of anthropology \n-\u200b\nArchaeology studies the past VIA objects and materials that made up this social life; \nmaterial culture \u2192 the traces of all the stuff that people throw away or leave behind \n-\u200b\nWe will only have access to things that don't compose, things made of certain materials \n-\u200b\nMaterial culture:  \n-\u200b\nThe objects humans produce, consume, and live their lives with. ie. Stuff \n-\u200b\nTechnology: The stuff people use to adapt to and transform their environments (Houses, \nclothes, weapons, etc.) \n-\u200b\nClothes, food, tools, etc. are also markers of status and identity  \n-\u200b\nNot only \u2018utilitarian\u2019 \u2192 reflects the norms and values of a culture  \n-\u200b\nHow Archaeologists think in steps:  \n1.\u200b The material traces of the past that preserve \n2.\u200b The stuff that produced these traces \n3.\u200b The practices that produced this stuff \n4.\u200b The broader social and cultural norms that gave these practices purpose and meaning    \n-\u200b\nDefinitions + terminology: \n-\u200b\nSurvey: The process of locating clues that might indicate the presence of archaeological \nmaterial culture  \n-\u200b\nArchaeological site: any locality that contains some density of archaeological \nmaterial  \n-\u200b\nExcavation:  Archaeologists dig in a controlled manner into a sire to uncover subsurface \narchaeological material   \n-\u200b\nStratigraphy:  Discrete soil layers that represent distinct occupations of the sight  \n-\u200b\nEach layer is a stratum \n-\u200b\nArtifacts:  Any kind of objects worked by humans  \n-\u200b\nEcofacts: natural objects associated with human behavior \u2192 may have been used \nor effected by humans (ex. Shells, seeds, and bones) \n-\u200b\nContext: Particular location of an artifact, feature, or sight \u2192 the meaningful \nassociations of this object in this location  \n-\u200b\nAbsolute dating:  \n-\u200b\nProvides an absolute calendar date for when an object was produced or deposited  \n-\u200b\n The most useful and widespread mode of absolute dating is C-14 dating, or radiocarbon \ndating  \n-\u200b\nC-14 dating can only date organic material and can only measure their dates accurately up \nto 50,000 years \n-\u200b\nHuman Remains \n-\u200b\nSex, age, health, diet of an individual \n-\u200b\nInequality, especially in terms of health and diet \n\n-\u200b\nNon-Archaeological Sources \n-\u200b\nTextual sources \n-\u200b\nNone in the North American arch. Prior to European contact  \n-\u200b\nEthnographic sources \n-\u200b\nRecords of indigenous communities from the past 100 years \n-\u200b\nOral Histories \n-\u200b\nLiving historic traditions  \n \n9/9/25 North America  \n-\u200b\nGeography of NA:  \n-\u200b\nAtlantic Coastal Plain \n-\u200b\nFlat, coastal plain region running from cape cod to southern texas \n-\u200b\nLong chains and barrier island \n-\u200b\nRivers, bays, and  \n-\u200b\nAppalachian Mountains \n-\u200b\nRelatively low elevation mountain rage, market by deep ravines and large valleys \n-\u200b\nVery well waters, rivers, lakes, gorges, waterfalls \n-\u200b\nCanadian Shield \n-\u200b\nRegion of exposed rock due to glaciers \n-\u200b\nVery rocky, little to no soil \n-\u200b\nInterior plain/Central Lowlands \n-\u200b\nGenerally flat region with deep alluvial soil  \n-\u200b\nBroad river valleys, rolling hills, high bluffs, forested, and many lakes \n-\u200b\nGreat Plains/High Plains \n-\u200b\nHigh elevation, semi-arid region \n-\u200b\nFlat areas broken up by some rocky outcrops and canyons \n-\u200b\nRocky mountains \n-\u200b\nVery high elevation mountain chain \n-\u200b\nMarked by cold weather, glaciers, steep valleys \n-\u200b\nIntermountain Plateau \n-\u200b\nThe raised plateau area between the two western mountain ranges \n-\u200b\nDry, mountainous area marked with deep basins \n-\u200b\nCoastal Mountains \n-\u200b\nHigh mountains along the west coast  \n-\u200b\nFjord like inlets and basins in the north  \n-\u200b\nLarge island archipelago: Haida Gwaii, Vancouver Island, Aleuntian islands  \n-\u200b\nSteep river valleys, fast flowing rivers: Frasier, Columbia, colorado  \n-\u200b\nArctic Lowlands \n-\u200b\nFlat coastal plain along the arctic ocean \n-\u200b\nLarge rivers, including the Mackenzie  \n-\u200b\nLarge island archipelago  \n-\u200b\nBiomes of NA \n-\u200b\nEastern Deciduous Forests \n-\u200b\nOak, Hickory, Maple and Chestnut \n\n-\u200b\nHot humid summers, cold winters \n-\u200b\nLarge animals like deer, moose, wolf, and bear, along with smaller animals like \nsquirrels, rabbits, and chipmunks  \n-\u200b\nSoutheast Evergreen \n-\u200b\nPine forests, deciduous trees, wetlands and marshes \n-\u200b\nMild winters, very hot and humid summers \n-\u200b\nVery rich biodiversity \n-\u200b\nGrasslands \n-\u200b\nTreeless prairie dominated by grasses, herbs and shrubs \n-\u200b\nHot summers, cold winters, extreme weather changes \n-\u200b\nMassive numbers of bison across the Plains, especially in the Western Plains \n-\u200b\nTaiga/Boreal Forest \n-\u200b\nConiferous forests, dominated by source, fir, and pine  \n-\u200b\nShort warm summer with long cold winters \n-\u200b\nSparse biodiversity, dominated by large mammals: bears, wolf, lynx, deer, elk, \nmoose, and caribou \n-\u200b\nTundra \n-\u200b\nTreeless environment dominated by lichens, mosses and grass \n-\u200b\nGround is permanently frozen, year round \n-\u200b\nShort cool summer, extremely cold long winters. Ice covers the sea  \n-\u200b\nMountain Forest \n-\u200b\nCarniferous forest \n-\u200b\nWarm summers and cold snowy winters \n-\u200b\nShorter summers further north \n-\u200b\nShrub Desert \n-\u200b\nVery arid landscape dominated by small trees (mesquite) and small shrubs like \ncreosote  \n-\u200b\nHot summers and warm winters \n-\u200b\nSeasonal rivers and few lakes or other standing water \n-\u200b\nChaparral  \n-\u200b\nMediterranean climate, mild wet winters and warm dry summers \n-\u200b\nDry and hilly landscape dominated by oak forests and short scrub plants \n-\u200b\nTemperate Rainforest \n-\u200b\nLong mild and wet winters, short and dryish summers. Significant rainfall \n-\u200b\nMassive coniferous trees \n-\u200b\nVery well watered area marked with large lakes, rivers, rivers and trees  \n-\u200b\nMississippi River system: \n-\u200b\nThe longest river and one of the largest drainage systems  \n-\u200b\nDrains many of the largest and most important rivers in the continent  \n-\u200b\nMissouri, Ohio, Tennessee, Wabash, Arkansas, and Illinois rivers all serve as tributaries \nto the Mississippi   \n \nEND OF QUIZ #1 MATERIAL  \n \n\n9/11/25 Class \n-\u200b\nArrowheads and Bones:  \n-\u200b\nAs settlers farm and clear their land, they encounter pre-contact Indigenous artifacts and \nburial mounts  \n-\u200b\nWealthy and educated Americans begin collecting these objects   \n-\u200b\nFor the most part, the material culture that was recovered appeared similar to that of the eastern \nIndigenous communities that these eEuro-American settlers had direct contact with  \n-\u200b\nMoving West  \n-\u200b\nMassive mounds and earthquakes in the Mid-West  \n-\u200b\nSeemingly unconnected to any existing Indigenous practices   \n-\u200b\nSquier and Davis:  \n-\u200b\nFirst big archaeological project in North America  \n-\u200b\nIn 1845, Ephraim Squier and Edwin Davis begin a three year project to survey, excavate \nand map the massive prehistoric earthworks along the Ohio and the Mississippi river \n-\u200b\nIn 1848, they published Ancient Monuments of the Mississippi Valley, the first major \nwork in North American archaeology \n-\u200b\nMound Builders Controversy:  \n-\u200b\nSquier and Davis argued that the earthworks were not built by Indigenous people \nbut by an unknown ancient white civilization  \n-\u200b\n\u2018Too advanced\u2019 to have been built by Indigenous peoples  \n-\u200b\nSquier and Davis's work sets off a firestorm of interest, for 30 years this is the most \npressing question in the study of pre-contact America  \n-\u200b\nWho built the mounds: Aztecs, Israelites, Vikings, Mormons \n-\u200b\nIn 1888, two academics prove it was Indigenous peoples  \n-\u200b\nSystematic Looting:  \n-\u200b\nTo answer questions of \u201cwho built these mounds\u201d, most archaeological research focused \non finding and looting Indigenous graves  \n-\u200b\nAs interest in Indigenous history grew, scientists collected\u2026[couldn\u2019t finish] \n-\u200b\nScientific racism:  \n-\u200b\nEven while this research proved Indigenous people build the mounds  \n-\u200b\nThis research on these skeletons was also used to scientifically \u2018prove\u2019 the biological \ninferiority of Indigenous people along with other non-whiye races  \n-\u200b\nNatural History Museums \n-\u200b\nMassive new museums were built to house these and other scientific collections  \n-\u200b\nNew money to fund massive new archaeological studies across North america  \n-\u200b\n\u201cVanishing Indian  \n-\u200b\nBelief in the biological and social inferiority of Indigenous people \n-\u200b\nAnthropologists and archaeologists believed Indigenous peoples\u2026..[couldn\u2019t finish] \n-\u200b\nSalvage Anthropology  \n-\u200b\nLate 19th and early 20th century, anthropologists and archaeologists flooded Indigenous  \ncommunities, collecting information, objects, artifacts, skeletons, and sometimes even \nlive people \n-\u200b\nConvinced Indigenous communities could not be trusted with their own material culture. \nThese scholars often lied and stole  \n\n-\u200b\nThis salvage took millions of objects from Indigenous tribes, along with hundreds of \nthousands of skeletons and grave goods from thousands of archaeological sites  \n-\u200b\nIn the Southwest, three Smithsonians financed expeditions led by archaeologist James \nStevenson purchased and collected over 5,000 pieces of traditional\u2026 \n-\u200b\nBy the 1920s, so much traditional pottery had been removed that many pottery styles and \ntechniques were completely lost \n-\u200b\nBy the 1960s, literally millions of artifacts, and hundreds of thousands of indigenous \nskeletons were in natural history museums and private collectives \n-\u200b\n\u201cTo disturb a grave is not only an insult to the Spirits of the dead, it is a blatant insult to \nthe creator\u201d and \u201cto dig up the grave and strip the departed one of\u2026treasures that were \nplaced by relatives would shock and sicken any moral human being\u201d(Chief Seattle speech \n1854)   \n-\u200b\n1960s/1970s \n-\u200b\nRise of Pan-Indigenous Activism \n-\u200b\nAmerican Indian Movement  (AIM) \n-\u200b\nNovember 1969-June 1971 \u2192 occupation of Alcatraz \n-\u200b\n1971 AIM protest at an archaeological site in (an island off of San Francisco?) \n-\u200b\nVine Deloria \n-\u200b\nCuster Died for your sins, published 1969 \n-\u200b\n\u201cBehind each policy and program with which Indians are plagued, if traced completely \nback to its origin, stands the anthropologist\u201d \n-\u200b\nArchaeologists believed that \u201cthe only real Indians were the dead ones\u201d \n-\u200b\n\u201cBefore the white men can relate to others he must forgo the pleasure of defining them\u201d \n-\u200b\nMaria Pearsons \n-\u200b\n\u201cYou can give me back my people\u2019s bones and you can quit digging them up\u201d \n-\u200b\nEqual treatment of Indigenous and non-Indigenous remains in Iowa 1976 \n-\u200b\nGot Iowa to pass the first legal protections for indigenous bodies  \n-\u200b\nNational Museum of the American Indian Act of 1989 \n-\u200b\nLocating and repatriating all indigenous human remains and funerary objects in the \nSmithsonian collections \n-\u200b\nAt the time, the museum had the remains of 18000 individuals along with millions of \nitems. \n-\u200b\nNAGPRA 1990 \n-\u200b\nNative American Graves Protection and Repatriation Act  \n-\u200b\nAll the agencies and institutions that received federal funding must be repatriate \nall \u201ccultural items\u201d to lineal descendants and culturally affiliated American \nIndian tribes, Alaskan Natives, and Native Hawaiian organizations: human \nremains, funerary objects, sacred objects, and objects of cultural patrimony  \n-\u200b\nAll future excavations on federal land that might come into contact with burials \nand funerary contexts, archaeologists must consult with relevant indigenous \ndescendent groups \n-\u200b\nRepatriation \n-\u200b\nOver the past 30 years, major repositories have gone through their inventories to identify \nhuman and artifactual remains to be repatriated  \n\n-\u200b\nKennewick Man/The Ancient one  \n-\u200b\nHuman remains found\u2026 \n-\u200b\nFour local indigenous tribes asked for the skeletons back according to NAGPRA \n-\u200b\nForensic archaeologists argued the skeleton was non-indigenous (polynesian ot East \nAsian) and therefore not covered by NAGPRA \n-\u200b\n20 years or highly contentious fights over who has the rights to the remains \n-\u200b\nDNA studies by a Danish geneticist conclusively showed that the remains were related to \ncontemporary indigenous groups, specifically those of the columbian river region \n-\u200b\nThe remains were reburied on February 18, 2017 \n-\u200b\nCollaboration and COnsultation \n-\u200b\nGoing beyond mandatory consultations  \n-\u200b\nIncorporating indigenous concerts and wishes, alongside the demands of traditional \narchaeology \n-\u200b\nTaking seriously indigenous knowledge and interest, including oral histories, traditional \necological knowledge as vital resources from the past  \n-\u200b\nIn theory, archaeologists no longer work on \u2018\u201cresearch \u2018about indigenous peoples\u2019\u201d... \n-\u200b\nCommunity Engaged Archaeology \n-\u200b\nRequires that scholars and community members develop equitable partnership \n-\u200b\nFor many archaeologists the struggle with be to let go of their sole authority, and allow \nothers to make decisions about research goals and interpretations  \n \n9/16/25 Peopling North America \n-\u200b\n19th century theories  \n-\u200b\nBased on skeletal data. No later that 3-4,000 years ago \n-\u200b\nArchaeological data seemed to support this data, no dating technique to prove otherwise \n-\u200b\nFolsom Site, 1908 - Folsom New Mexico \n-\u200b\nGeorge McJunkin, amateur archaeologist \n-\u200b\nIdentifies some bones eroding out of a bank \n-\u200b\nBison antiques - one of the most common animals in North America during the \npleistocene (ice age) \n-\u200b\nHad been extinct since 11,000 yBP \n-\u200b\nEventually in 1926 a team investigates \n-\u200b\nIn the same strata (soil level) as the bones they find a scatter of small lithics  \n-\u200b\nThis was \u201cinsane\u201d because archaeologists initially believed that humans were \n3-4k years old \n-\u200b\nNow, they realized that humans have been here before 11k years ago  \n-\u200b\nBecause of this, no one believed their finds \n-\u200b\n\u201cPeople could not have arrived in North AMerica Prior to the end of the \nIce Age\u201d \n-\u200b\n Head physical anthropologist at the Smithsonian, Ales Hrdlicka shot down any theories \n-\u200b\nFinds were published with disclaimers \n-\u200b\nFound intact articulated bison skeletons, lots of lithics, a projectile point between two \nbison ribs \n-\u200b\nVery finely made projectile points \n\n-\u200b\nUnlike any stone tools they had ever seen \n-\u200b\nDefines by central fluting \u201cfolsom points\u201d \n-\u200b\nBlack Water Draw Site \n-\u200b\n1932, a site near Clovis, New Mexico \n-\u200b\nMore support for the theory of human occupation older that 10,000 years old \n-\u200b\nMultiple deep layers of occupation \n-\u200b\nThe deepest layer: human made lithics combined with animal bones: mammoth, camel, \nhorse, bison antiques, dire wolf, sabre tooth cat \n-\u200b\nAll animals had gone extinct at the end of the Pleistocene  \n-\u200b\nThe same distinctive style of projectile points that was found at  Folsom was found at \nBlackwater Draw in its second deepest layer \n-\u200b\nBelow this, in the lowest layer, a different style of projectile point was noted  \n-\u200b\nClovis Points \n-\u200b\nLarge bifacial stone blade, very wide and thin, the result of very skillful lithic reduction \n-\u200b\nCharacteristic shape and fluting along the base of the point \n-\u200b\nUsed for both hunting and processing meat \n-\u200b\nClovis Tradition \n-\u200b\nClovis points have been found with a wide range of megafaunal bones: mastodon, \nmammoth, and bison. Often in large megafaunal kill and processing sites  \n-\u200b\nBone tools, antler tools \n-\u200b\nRoughly 13,300 - 12,700 yBP \n-\u200b\nClovis First Theory \n-\u200b\nThe clovis tradition is the first major cultural tradition of the Americas and that it \ndeveloped by the very first immigrants to North America \n-\u200b\nThat humans first arrived in North America around 13,000, and when they arrived \ndeveloped a highly specialized tool kit for hunting the big game (megafauna) they found \n-\u200b\nFolsom Points \n-\u200b\nAppear to be the adaptation of the clovis point \n-\u200b\nEven thinner, more reduction, even more technical proficiency requires  \n-\u200b\nDefined by a long fluting scar that runs the whole body of the point  \n-\u200b\nUsed for hunting and processing  \n-\u200b\nFolsom Tradition \n-\u200b\nAlongside Folsom points, archaeologists found associated bone needles, and sandstone \nabrades \n-\u200b\nLess variety of megafauna being hunted, focus on bison \n-\u200b\nRoughly 12,700 - 12,000 years ago \n-\u200b\nThe Late Pleistocene Environment \n-\u200b\nLasts between 126,000 - 11,700 yBP \u2192 Last Glacial Period \n-\u200b\nMarked by a significantly lower average global temperature \n-\u200b\nExpansion of glaciers in the N and S hemisphere \n-\u200b\nLast Glacial Maximum (LGM) \n-\u200b\nRoughly 26,500 yBP glaciers reached their glacial maximum \n-\u200b\nCompletely covered Canada, most of Alaska, and much of the US Midwest \n-\u200b\nCut off any land routes between Asia and the Americas \n\n-\u200b\nBeringia \n-\u200b\nGlaciation caused sea levels to drop as much as 100m. Exposing a massive subcontinent \nbetween Siberia and Alaska \n-\u200b\nThis land route was open between 30,000 - 11,000 yBP \n-\u200b\nHowever, the massive glaciers cut of the unglaciated parts of Alaska from the rest of \nNorth america  \n-\u200b\nIce Free Corridor  \n-\u200b\nBy 19,000 YBP, temperatures began warming, causing the glaciers to retreat \n-\u200b\n This retreat caused an opening up an ice-free corridor between\u2026 \n-\u200b\nIn opening up, the ice free corridor provides the first land route between Asia and the \nAmericas since 30,000 yBP \n \n \n9/18/25 \n-\u200b\nIce free corridor \n-\u200b\nBy 19,000 yBP, temperatures began warming, causing the glaciers to retreat \n-\u200b\nThis retread caused an opening up of the ice free corridor between the Laurentide and\u2026 \n-\u200b\nClovis first cont. \n-\u200b\nIce free corridor serves as the key to the first clovis argument  \n-\u200b\nAs the courier opened up, big game hunters living in Beringia began following game \nthrough the ice free corridor into the central US  \n-\u200b\nOnce in NA they developed a unique toolkit to exploit the resources  \n-\u200b\nStrengths of Clovis-first \n-\u200b\nTies together archaeological and climatic data \n-\u200b\nConnects two very important events \n-\u200b\nThe sudden spread of a clovis material culture across the Americas around \n13,000 \n-\u200b\nThe opening up of a land route between Asia and NA around 13,000 that then \ncloses around 11,000 \n-\u200b\nMegafauna Extinction \n-\u200b\nA third chronological event seems to further support this theory \n-\u200b\nBy 10,000 ya Over 90 genus of mammals weighing over 45kg went extinct  \n-\u200b\nOverkill Hypothesis \n-\u200b\nArchaeologists have linked these extinctions to the arrival of humans and their clovis \ntoolkits \n-\u200b\nThis has been widely adopted by Clovis first theorists, but the evidence is circumstantial \n-\u200b\nOther lines of evidence \n-\u200b\nIt broadly fits within genetic evidence, which shows people originating in Serbia\u2026 \n-\u200b\nIt broadly fits within linguistic evidence, which suggest three distinct migration events \nout of North-East Asia into NA \n-\u200b\nFlaws in Clovis First \n-\u200b\nRecent Genetic testing suggests that Indigenous North Americans split from North-East \nAsian populations around 25,000 years ago \n\n-\u200b\nThey entered into North America and began to diversify roughly between 17,000 - 14,000 \nyears ago \n-\u200b\nEnvironmental studies of the ice-free corridor suggest that it was barren, and potentially \ncovered in water and boulders up until around 12,000 yBP \n-\u200b\nThe Power of Clovis First \n-\u200b\n13,000 years ago \n-\u200b\nCame in by land, across Beringia and through the Ice-Free corridor  \n-\u200b\nBig Game Hunters developed a very specific fluted projectile point  \n-\u200b\nSpread across the lower 48, caused megafaunal extinctions, left an enormous \narchaeological record   \n-\u200b\nThe theory dominated archaeology for fifty years \n-\u200b\nMonte Verde \n-\u200b\nIn the 1970s, waterlogged site in peaty soil along a river in Chile \n-\u200b\nSizeable habitation likely occupied by 20-30 people \n-\u200b\nDates came back 14,500 yBP \n-\u200b\nLithics include a stemmed bifacial point, along with a number of basic scrapers and \ncutting flakes \n-\u200b\nEating shellfish, llama, horse and wild potato \n-\u200b\nHighly varied hunter-foragers, minimal hunting big game \n-\u200b\nSubstantial reliance on aquatic and marine resources. Moved back and forth to coast \n-\u200b\nMore Support \n-\u200b\nTopper site: IMportant rock quarry used throughout the clovis period \n-\u200b\nExcavations found lithics below clovis levels \n-\u200b\nFlakes and burins, no bifaces \n-\u200b\nPre-Clovis Site \n-\u200b\nDates back to as far back at 15,000 yBP have been accepted by many archaeologists \n-\u200b\nSome dates suggest human occupation roughly 20,000-50,000 yBP, although these dates \nare highly controversial  \n-\u200b\nNot a single tradition like Clovis. But a collection of very different sites  \n-\u200b\nCoastal Migration Hypothesis \n-\u200b\nUsed boat technology to come along the Pacific coastline \n-\u200b\nBoat technology is clearly being used by other humans as early as 50,000 years ago \n-\u200b\nThis theory doesn\u2019t have that much evidence  \n-\u200b\nSea-levels have risen almost 100m, inundating most of the areas one would assume there \nshould be sites  \n-\u200b\nPaisley Cave \n-\u200b\nCave in Oregon, human coprolites dated to 14,500 \n-\u200b\nHumans eating small mammals, birds, insects, and fish  \n-\u200b\nWestern Stemmed Tradition  \n-\u200b\nStemmed points common in early (13ky-12kya) sites in Western US \n-\u200b\nSites like Paisley cave sidle suggest it predates clovis stone technology  \n-\u200b\nThese stemmed points are similar to lithics found in Siberia 13kya \n-\u200b\nTriquet Island \n-\u200b\nVillage found 14,000 yBP \n\n-\u200b\nArtifacts found include: Fish hooks, a fire drill, an atl atl, a cache of stone tools \n-\u200b\nA coastal toolkit focused on fishing, small game hunting, and foraging \n-\u200b\nWhy go through coastal migration? (even with an ocean on one side and glaciers on the other) \n-\u200b\nKelp Highway Theory:  \n-\u200b\nA more in depth version of the Coastal Migration Theory  \n-\u200b\nThe specific ecology of much of the northern Pacific is very similar, defined by \nrich kelp forest \n-\u200b\nBy 30kya, coastal populations in Japan develop coastal toolkits \n-\u200b\nBoats, shell fishhooks for deep water fishing, and a stemmed point and \nmicroblade core technology \n-\u200b\nAround 19-15kya, they spread into the pacific northwestern through the recently ice free \nisland route \n-\u200b\nSimilar Toolkit! \n-\u200b\n13,000 yBP, contemporary with Clovis \n-\u200b\nDistinctive barbed points found on channel islands in california \n-\u200b\nSimilar to tanged points found in Japan, 14,5000 yBP \n-\u200b\nButtermilk Creek \n-\u200b\nTexas, 15,500 yBP  \n-\u200b\nNearby a number of very rich Clovis sites  \n-\u200b\nOver 15,000 pre-Clovis lithics, providing the most complete singles site record of what \nClovis lithic technology might have emerged out of \n-\u200b\n12 bifacial points \n-\u200b\nEvidence of stemmed bifacial points from around 15,000 yBP \n-\u200b\nLarge Non-fluted points, might be ancestor to Clovis point 14,000 yBP \n-\u200b\nAlaskan Lithic Record \n-\u200b\nTwo large lithic points have been dated to 13,000 yBP \n-\u200b\nMix of stemmed point technology from the coast and big game lithic point from Alaska  \n-\u200b\n20,000 Year old foot prints found in New mexico \n-\u200b\n Suggests humans have been in the americas than previously thought \n-\u200b\n23,000 - 19,000 yBP \n-\u200b\nLast Glacial Maximum (LGM) \n-\u200b\nRoughly 26,500 yBP glaciers reached their glacial maximum \n-\u200b\nCompletely covered Canada, most of Alaska, and much of the US Midwest \n-\u200b\nWarming began 19,000 yBP \n-\u200b\nAquatic Seeds - Problems Dating \n-\u200b\n \n-\u200b\n \n-\u200b\nReservoir Effect \n-\u200b\nCertain objects in water environments can absorb carbon even once the creature is dead \n-\u200b\nIn most cases this makes an object date younger, but id the surrounding water is filled \nwith older carbon, this absorption can make an object date older  \n-\u200b\n50 year old rupiah seeds have been dated as 8,000 year old according to radiocarbon \ndating \n\n-\u200b\nResearches dated two other sources of data, pollen and the quartz in the sand, using \nluminescence dating \n-\u200b\nBoth came back with the original dates 21000-23000 ya \n-\u200b\nThese dates has come under attack  \n \n9/23/25 Changing Environments: From the Pleistocene to the Holocene \n-\u200b\n??? \n-\u200b\nThe paleo-indian period (13,000 - 10,000 yBP) \n-\u200b\nEnvironment of the End of the Pleistocene \n-\u200b\n20,000 - 13,000 yBP significant warming period \n-\u200b\nRetreat of the glaciers, opening up of new areas of North America  \n-\u200b\nSeattle - similar to a desert arctic land dry tundra (?) \n-\u200b\nThe appalachian mountains looking like Northern Canada \n-\u200b\nAlabama and Southern Texas looking like the forests in New England \n-\u200b\nNew England covers in Ice sheet 20,000 years ago \n-\u200b\nLate Pleistocene Fauna \n-\u200b\nThe land was filled with a range of very large creatures (megafauna) \n-\u200b\nGiant sloths, bear, tapirs, peccaries, American lion, sabre-tooth cat, saiga, camelids, stage \nmoose, shrub-ox, 14 species of pronghorn, horse, mammoths, mastodons, beautiful \narmadillo (twice the size as today\u2019s armadillo\u2019s) , giant beavers, giant condors. \n-\u200b\nCool temperate forests are no longer down in Texas, they have begun moving up to \nGeorgia    \n-\u200b\nTundra starts to dry up 15,000 yBP, no longer covered by glaciers, glaciers have opened \nup great lakes (i.e lake erie)  \n-\u200b\n13,000 yBP - period where we start finding significant evidence of people  \n-\u200b\nClovis Tradition \n-\u200b\n13,300 - 12,700 ya \n-\u200b\nBig game hunters with large fluted projectile points spread across North America  \n-\u200b\nYounger Dryas \n-\u200b\n12,700 - 11,500 yBP, sudden cooling period \n-\u200b\nA drop of 2 to 6 degrees calculus global temperatures  \n-\u200b\nVery hard winters \n-\u200b\n10-15 degrees C colder across much of North America \n-\u200b\nRetreat of forests and the expansion of tundra and Taiga   \n-\u200b\nBroady a return back to the landscape of 15,000 yBP or so \n-\u200b\nBy the end of the younger Dryans, 32 genera of Pleistocene Magnafauna have gone \nextinct \n-\u200b\n70% of all megafaunal species on the continent \n-\u200b\n100% of all species weighing more than 1000kg \n-\u200b\nWhat Caused this Mass Extinction? \n-\u200b\nEnvironment: Rapid change in climate - goes from warm to cold  \n-\u200b\nHumans: Overkill, hunting  \n-\u200b\n \n-\u200b\nHumans: The OverKill Hypothesis \n\n-\u200b\nBig game hunting humans into North America caused sudden extinction of the large \nmammals  \n-\u200b\n25-30 Clovis era kill sites that show evidence of hunting and butchering megafauna \n-\u200b\nOr was it the Younger Dryas? \n-\u200b\nMany megafauna that humans hunted did not go extinct \n-\u200b\nMany megafauna that humans did not hunt went extinct  \n-\u200b\nHuman response to Younger Dryas \n-\u200b\nThe end of a single tool tradition across the continent (clovis) \n-\u200b\nDevelopment of regional tool kits, and likely regional cultures (Folsom, PaleoCoastal \ntradition, Plano, etc) \n-\u200b\nHighly mobile societies moved great distances \n-\u200b\nNortheast, tundra,taiga caribou hunters \n-\u200b\nSite in Turners Falls, 12000 yBP \n-\u200b\nSituated near a caribou river crossing  \n-\u200b\nNear lithic source  \n-\u200b\nBull\u2019s Brook Site \n-\u200b\nOne of the largest sites in NA during this period \n-\u200b\nAdjacent to caribou crossing \n-\u200b\nThousands of lithics and caribou remains found  \n-\u200b\nLikely drive site \n-\u200b\nPlains \n-\u200b\nBison hunters using Folsom points and other large points \n-\u200b\nPaleocoastal \n-\u200b\nHighly mobile society focused on coastal tool kits and boats \n-\u200b\nIntensive exploitation of waterflow, shellfish, fish, and small seals \n-\u200b\nSerrated and stemmed lithic points, lithic crescents used to hunt birds \n-\u200b\nAfter the Younger dryas \n-\u200b\nWarming 11,500 yBP ends the Younger Dryas and brings us to the Holocene, a nice \nstable climate period \n-\u200b\nTemperate deciduous forests spread across the Northeast, replacing the taiga and pushing \nback the grasslands \n-\u200b\nAdaptation \n-\u200b\nGeneral switch from reliance on extinct megafaunsa, as well as non extinct migratory \ngame like caribou and bison to smaller migratory game like deer \n-\u200b\nRockshelters \n-\u200b\nProtected sites and communities could return to as part of their local hunting cycle \n-\u200b\nLarge rockshelter sites, occupied by significant numbers of people returning every year \nfor generations \n-\u200b\nNine Major traditions emerging out of the nine major ecological biomes \n-\u200b\n \n-\u200b\n \n9/25/25 California  \n-\u200b\nCalifornia: Baskets and Acorns \n-\u200b\n5 major regional parts to California: \n\n-\u200b\nSouth Coast/Channel Islands \n-\u200b\nCentral Coast \n-\u200b\nNorth Coast \n-\u200b\nSierra Nevadas \n-\u200b\nCentral Valley \n-\u200b\nCalifornia \n-\u200b\nMild climate on coast and valley \n-\u200b\nVery diverse landscapes, deserts, deserts, mountains, and beaches \n-\u200b\nCoasts and Interiors \n-\u200b\nRich coats filled with shellfish, fish and sea mammals \n-\u200b\nDry and mountainous interiors but with rich foraging opportunities; seeds, nuts and edible \nplants \n-\u200b\nOak Tree Forests \n-\u200b\nCovering much of the coast and the interior of California  \n-\u200b\nCalifornia at Contact \n-\u200b\nFirst contacts (1542) \n-\u200b\nThe densest population of any region of North America of central Mexico \n-\u200b\nEstimated \u2153 of entire Indigenous population of lower 48 states \n-\u200b\nRoughly a hundred different indigenous cultural groups, each speaking their own \nlanguages, maintaining their own cultural practice \n-\u200b\nIn the interior they has thousands of large permanent villages \n-\u200b\nAlong the coast they had permanent villages of up to 1000 people ruled by powerful \nchiefs who had incredible wealth and political control over vast resources and labor \n-\u200b\nThey used an early form of money, and a trade network that extended across the Western \ncontinent  \n-\u200b\nDifferent California Periods \n-\u200b\nPaleo-Coastal Period (13,000 - 9,000 yBP) \n-\u200b\nAs early as 13,000 years ago  \n-\u200b\nHighly mobile societies focused on\u2026 \n-\u200b\nEarly Holocene (9,000-7,000 yBP) \n-\u200b\nWarming environment  \n-\u200b\nOccupation of central and south coast \n-\u200b\nEarly but relatively sparse occupation of the interior  \n-\u200b\nIntensive coastal exploitation along southern and central coast \n-\u200b\nShell Middens: significant shellfish consumption, and sea mammals (seals and \nsea lions) \n-\u200b\nMiddle Holocene (7,000-3,000 yBP) \n-\u200b\nIncreased population clustered on south and central coast, but also throughout the \nrest of California \n-\u200b\nWarming climate, interior becomes more arid \n-\u200b\nIncreased population spread across all regions of California  \n-\u200b\nRegional toolkits and regional cultures  \n-\u200b\nSedentism along coasts \n\n-\u200b\nHeavy occupation of San Francisco Bay, large permanent villages, focused on \nclams  \n-\u200b\nFirst substantial settlement along the northern coast \n-\u200b\nCentral valley and Sierra Nevada: acorn and salmon \n-\u200b\nLarge cemeteries built in the Central Valley. Burials show rich grave goods, some \nevidence of societal stratification, extensive evidence of warfare  \n-\u200b\nLate Holocene (3,000 - 150 yBP) \n-\u200b\nCooling CLimare (except for warming blip 1,000-800 yBP) \n-\u200b\nIncredible population growth across california \n-\u200b\nThe general distribution og indigenous cultures at contact begins to form \n-\u200b\nSouth Coast and Channel Islands: \n-\u200b\nCooling climate leads to increasingly rich coastal environments \n-\u200b\nTechnological adaptations to exploit coastal resources, population \ngrowth, and increased socio-political complexity  \n-\u200b\nMilling stones \n-\u200b\nOr also known as manos and metates, used for the crushing of hard seeds and other food \nprocessing \n-\u200b\nToolkit more adapted to foraging  \n-\u200b\nSmall groups, 2-4 families in size \n-\u200b\nDiverse diet focused on seed foraging, shellfish collection and waterfowl \n-\u200b\nDecline in projectile points  \n-\u200b\nBaskets/Fishing in CA \n-\u200b\nUsed for storing food, cooking, transporting, collecting food, art \n-\u200b\nOn the coast, boats and nets were used to hunt sea mammals on the open sea (seals, \ndolphins and sea lions) \n-\u200b\nFish hook used to catch larger fish (mackerel and lingcod) \n-\u200b\nMortar and Pestle \n-\u200b\nFound in most regions of California \n-\u200b\nCommonly viewed as a technology primarily used for processing acorn  \n-\u200b\nAcorn Processing \n-\u200b\nAcorns must be ground into a flour and leached before eating \n-\u200b\nMortar and Pestle unlock the rich oak forests of California  \n-\u200b\nSierra Nevada and Central Valley \n-\u200b\nPopulations increased dramatically \n-\u200b\nNumerous different Indigenous groups establish permanent villages among the acorn rich \nhills \n-\u200b\nBedrock Mortars \n-\u200b\nMortarts are cut into the bedrock  \n-\u200b\nDrastically increases scale of acorn processing \n-\u200b\nAcorn processing becomes sedentary \n-\u200b\nMiwok \n-\u200b\nOne of the largest cultural groups living in the Sierra Nevadas \n-\u200b\n100s of permanent villages throughout the hills and central valley  \n-\u200b\nVillages  \n\n-\u200b\nSources of water, and exposed bedrock  \n-\u200b\nVillages were situated close to the best acorn trees \n-\u200b\nSmall wooden hits called u\u2019machas built around a main roundhouse (hangi) for \nmeetings and ceremonies  \n-\u200b\n10-30 Umachas in a village  \n-\u200b\nBaskets:  \n-\u200b\nProduced roughly 15 different types of baskets  \n-\u200b\nMainly made from sedge, dogbane, and willow bark  \n-\u200b\nFire Gardens: \n-\u200b\nThe Miwok would light fires in the oak forests surrounding their village  \n-\u200b\nPeriodic ground fires would clear brush and support plentiful acorn crops \n-\u200b\nWIld tobacco also managed in this way  \n-\u200b\nStaple: Acorns \n-\u200b\nMiwok social like organized around acorn production \n-\u200b\nIncredible surplus of food from acorns facilitated larger populations, craft \nspecialization, and community wealth \n-\u200b\nThe staple of the Miwok diet was acorn stew (nupa) \n-\u200b\nCha\u2019ka \n-\u200b\nAcorns could be stored for long periods \n-\u200b\nStored in large granaries, or Cha\u2019kas, in Miwok \n-\u200b\nThese could be 8 or more feet high and were made of poles interwoven with \nslender brush stems  \n-\u200b\nThe source of community wealth \n-\u200b\nNon-hierarchical society, no central authority \n-\u200b\nCentral figures, like the shamans, held significant influence over spiritual \nmanners \n-\u200b\nThe roundhouses was the site of elaborate dancing festivals and community \ndecision making \n-\u200b\nAcorn Economy and Gender \n-\u200b\nAll aspects of acorn procurements, storage, and processing were performed and \ncontrolled by women \n-\u200b\nHunting and trade were the domain of men  \n-\u200b\nAs acorns became the essential staple for communities like the Mowok, women also \ngained an incredible amount of influence  \n-\u200b\nIn determining the best place to process acorns, women determined where villages were \nlocated \n-\u200b\nThrough their control of acorn storage, women controlled the wealth of the community  \n-\u200b\nArchaeologists often assume foraging, processing food, and domestic work (practices \nassociated with women in many societies) are less prestigious than hunting and trading \n(associated with men) \n-\u200b\nThe Miwok shows how the opposite is often true  \n-\u200b\n \n-\u200b\nSociopolitical Complexity \n-\u200b\nSociopolitical- Relating to the social and political organization of a society: \n\n-\u200b\n How society organizes itself, how it distributes power and influence  \n-\u200b\nComplexity - How internally differentiated is a society? How many different classes of \npeople, specialists, institutions, etc?  \n-\u200b\nTo what extent does the society have an institutionalized hierarchy of power and \ninfluence that govern central aspects of sicla life? Chiefs, elites commoners  \n-\u200b\nComplexity \u2260 advanced \n-\u200b\nIt is not a moral judgment  \n-\u200b\nChumash \n-\u200b\nCultrual group that emerges around Santa Barbara \n-\u200b\nBetween 3,000 and 1,000 yPB, there is an incredible increase in the population  \n-\u200b\nBy 750 yBP most of this population has coalesced into large permanent villages of up to \n1,000 people \n-\u200b\nWhen Spanish arrived in the 17th century, they identified hundreds of villages and the \npopulation of the chumas in the tens of thousands \n-\u200b\nRuled by chiefs and wealthy elites \n-\u200b\nTechnological Changes \n-\u200b\nA stronger more efficient fish hook made from abalone shell \n-\u200b\nAllowed them to catch large tuna, shark and swordfish \n-\u200b\nHarpoon technology allows open water hunting of sea-mammals \n-\u200b\nTomol \n-\u200b\nBetween 3000-2000 yBP the Chumash developed an incredibly sturdy and fast \nocean-going vessel, the Tomol \n-\u200b\nPlank canoe, made from red-wood cedar \n-\u200b\nVery sophisticated construction method, weaving together planks with natural tar  \n-\u200b\nVery stable ocean-going vessel, facilitated trade and open ocean fishing  \n-\u200b\nShell beads \n-\u200b\nAround 2000 yBP staggering numbers of shell beads were produced along the coast in \nthe Channel Islands and Santa Barbara \n-\u200b\nThese beads were standardized and strung into long strands \n-\u200b\nWere traded along the coast for food, baskets, other goods \n-\u200b\nTrade \n-\u200b\nThese bead strands highly valued across California and beyond, and the Chumash would \nsail up and down the coast trading them with other groups  \n-\u200b\nThe development of these beads coincided with a rapis explosion of regional trade \n-\u200b\nSouthwest, the Great Basin, and the Northwest Coasts \n-\u200b\nShell-Money \n-\u200b\nAs a universally in demand good, shell-beads facilitated the trade of other goods. They \nfunctioned as a currency \n-\u200b\nThe standardization of shell-beads is seen as the first evidence of money in North \nAmerica \n-\u200b\nCraft Specialization \n-\u200b\nThe repsoduction of this varied tool kit relied upon full time specialists \n-\u200b\nCertain people specialized in particular activities fill time \n\n-\u200b\nBoat building, shell-bead production, astronomy, shamanism, undertaking, \ndancing and singing  \n-\u200b\nThe building of the Tomol boats was tightly controlled by the guild known as the \nBrotherhood of the Tomol  \n-\u200b\nThis guild closely guarded the techniques they used to build the essential vessel  \n-\u200b\nShamans and Rock-art \n-\u200b\nThe development of an incredible elaborate rock art tradition in the Santa Barbara \nRegions \n-\u200b\nPainted by religious specialists, or chamans, often in an altered state, the paintings \ndepicted interactions with the spirit worl  \n-\u200b\nLocated in caves, these paintings were an essential part of the ceremonial life and in \nparticular coming of age ceremonies like spirit quests  \n-\u200b\nChiefs \n-\u200b\nBetween 3,000 yBP and 1,000 yBP, the development of a leader that weilded paramount \nauthority within a village  \n-\u200b\nWhile they may have been elected originally, by contact these chiefs were hereditary, and \nmostly men, although they could be women  \n-\u200b\nThese chefs helped organize shell-bead production and trade, managed resources and \nlabor, developed intervillage alliances \n-\u200b\nChief power was closely linked to the Tomol boat technology, as well as shell-bead \nproduction \n-\u200b\nElites \n-\u200b\nA smaller number of elite families who controlled substantial wealth, resources,and labor \n-\u200b\nTried to control of shell-bead beds, trade routes and boat building  \n10/7/25 California 2: Boats and Chiefs \n-\u200b\nMain theories of China's political development \n-\u200b\nPopulation growth starting around 3,000 yBP combined with the rise of shell-bead \nproduction and the need to organize labor \n-\u200b\nSudden environmental pressures such as drought at around 1,000 yBP, or water warming \naround 800 yBP required political leadership to maintain food production and storage \n-\u200b\nControl of boat production technology and trade networks starting around 1,500 yBP \n-\u200b\nSocial Evolution\u2019s explanation \n-\u200b\nFor many years, anthropologists had a basic answer to the question of evolution \n-\u200b\nOver time societies tend to go from simple to complex  \n-\u200b\nAll societies go through the same basic stages of complexity \n-\u200b\nChanges in the complexity of a society were driven by changes in methods of subsistence \n-\u200b\nAll aspects of society (population, settlement pattern, economy, political, etc) \nshifted as subsistence practices shifted \n-\u200b\nThe level of sociopolitical complexity (its political organization) was directly tied \nto its mode of subsistence \n-\u200b\nie: the development of social stratification (inequity) was a natural outcome of a \nchanging mode of subsistence \n-\u200b\nDomestication/Agricultural: the end of Egalitarianism \n-\u200b\nA band settles down and begins to domesticate the animals it once hunted \n\n-\u200b\nDomestication leads to larger populations, increases sedentism, and food surplus \n-\u200b\nLeads to social differentiation: the rise of a class of people to organize labor during the \nharvest, to store the surplus of food, and to protect the land and the surplus  \n-\u200b\n\u2018Complex\u201d Hunter Gatherers?\u200b  \n-\u200b\nHunter and foragers who develop high levels of social differentiation, inequality, political \nauthority, and other non-egalitarian features \n-\u200b\nReveals a fake in social evolutionary logic \n-\u200b\nThere is no single way of explaining the wide variety of human societies \n-\u200b\nThere is no one way of explaining why political authority and inequality develops in all \nplaces \n-\u200b\nThere is no immutable law of how the different parts of a society (subsistence, \npopulation, political authority, stratification) will necessarily relate or develope \n-\u200b\nThere is no single evolutionary line to rank them on \n-\u200b\nWe have to understand the particularities of local histories, local regions, and local \ncultures \n-\u200b\nCalifornia to the Northwest Coast \n-\u200b\nThe NW coast regions of North America has many more examples of \u2018Complex\u2019 Hunter \nGatherers; hereditary chiefs, elites, and slavery, all within communities that subsist by \nhunting and gathering  \n-\u200b\nLonger histories than those of the Chumas \n-\u200b\nNW Coast \n-\u200b\nA mild climate with rich resources  \n-\u200b\nMild winters and cool summers  \n-\u200b\nVery rocky landscape  \n-\u200b\nLarge islanfs \n-\u200b\nAt contact in the late 18th century, densely populated area, highly varied cultural groups \n-\u200b\nLarge populations that relied on highly sophisticated hunting and gathering subsistence \ntechnologies \n-\u200b\nStorage and surplus of food \n-\u200b\nLarge permanent villages of longhouses occupied by extended families \n-\u200b\nEach longhouse might be\u2026 \n-\u200b\nHighly hierarchical societies ruled by hereditary chiefs and stratifies by class: alites, \ncommoners, and slaves \n-\u200b\nEach village controlled by paramount chief, who might rule over multiple villages  \n-\u200b\nChiefs possessed incredible levels of material wealth, prestige and authority  \n-\u200b\nLed war parties to capture wealth and enslave prisoners of war \n-\u200b\nElite families and chiefs build totem poles and other forms of monumental art to reflect \ntheir status and family lineage \n10/9/25:  \n-\u200b\nThings archaeologists look for \n-\u200b\nMaterial evidence that might be used to identify the emergence of social inequality: \n-\u200b\nSymbols of rank, pieces of clothing \n-\u200b\nRich graves goods, especially among children  \n-\u200b\nHouse size variation  \n\n-\u200b\nResource intensification, warfare, and feasting \n-\u200b\nNorthWest coast subregions \n-\u200b\nNorth coast \n-\u200b\nColder, fewer resources  \n-\u200b\nLess dense population \n-\u200b\nLonger evidence of social status, warfare \n-\u200b\nCentral coast \n-\u200b\nSouth coast \n-\u200b\nMore resources  \n-\u200b\nDense population \n-\u200b\nLonger history of wealth  \n-\u200b\nOther geographic features:  \n-\u200b\nFraser river \n-\u200b\nSkeena river \n-\u200b\nVancouver island \n-\u200b\nHaida Gwai  \n-\u200b\n \n-\u200b\nChronology \n-\u200b\nPaleo-Coastal Period (15000-9500 yBP) \n-\u200b\nHighly adapted coastal foraging \n-\u200b\nArchaic (9500-5000 yBP) \n-\u200b\nChanging sea-levels and environment \n-\u200b\nVery little archaeological data \n-\u200b\nStart of microblade technology  \n-\u200b\nMix of marine and terrestrial resources  \n-\u200b\nShellfish, salmon, halibut, cod, some sea mammal \n-\u200b\nDeer and caribou \n-\u200b\nThe Pacific Period:  \n-\u200b\nEarly (5000-3500 yBP) \n-\u200b\nShell middens  \n-\u200b\nThin midden build-up, small populations \n-\u200b\nChanging toolkit \n-\u200b\nChipped stone tools were mostly replaced with ground stone \ntools \n-\u200b\nAdzes and other woodworking tools \n-\u200b\nMiddle \n-\u200b\nLate \n-\u200b\nPacific era:  \n-\u200b\nStill very mobile societies, but groups concentrated in the same area \n-\u200b\nCommunities become tied to places \n-\u200b\nMicroblade technology \n-\u200b\nSmall sharp stone blades taken off cores and hafted onto larger tools \n-\u200b\nVery efficient and adaptable toolkit perfect for highly mobile groups across different \nlandscapes \n\n-\u200b\nCemeteries \n-\u200b\nFound on both the North and South Coast  \n-\u200b\nDeep Oral Histories \n-\u200b\nOral histories from multiple different groups recount periods of great migration that end \naround this time period (pacific era) \n-\u200b\nAdawx: \u200b\n \n-\u200b\nThe Tsimshian Oral tradition \n-\u200b\nEach clan in charge of keeping track of their history  \n-\u200b\nHistory of the Tsimshian recounts both myths and historical events \n-\u200b\nRoughly 5000 yBP the Tsimthian settled on the coast  \n-\u200b\nAccounts of a Great Flood seem to match up to a climatic event 7000 yBP \n-\u200b\nSocial Transformations \n-\u200b\nNorth Coast \u2192 extensive evidence of warfare \n-\u200b\n20% of all skeletons show evidence of\u2026 \n-\u200b\nHidden Falls Site, Alaska \n-\u200b\n4600-3200 yBP \n-\u200b\nEvidence of markers of status: labrets and ground stone pendants \n-\u200b\nLabrets:  \n-\u200b\nStone piercing of lower lip  \n-\u200b\nStatus/Rank \n-\u200b\nElite social status emerging in small North Coast communities tied to a rise of violence \n-\u200b\nBead Burials:  \n-\u200b\nIn the south coast 4000-38000 yBP, denizens of cemeteries \n-\u200b\nRich grave goods, bead burials  \n-\u200b\nVERY rich  \n-\u200b\nGreen point burial:  \n-\u200b\nFour burials found \n-\u200b\n35 y/o male associated with thousands of beads  \n-\u200b\nTsawwassen Burial:  \n-\u200b\nA man in his 40\u2019s buried with 11,000 stone beads \n-\u200b\nA boy 11-14 y/o with 53,000 stone beads \n-\u200b\nSechelt Inlet Burial Site  \n-\u200b\nRichest grave goods ever found in NWC \n-\u200b\n5 bodies recovered with hundreds of thousands of beads  \n-\u200b\nAll members of a single family  \n-\u200b\nAn older adult male, 50+ y/o \n-\u200b\nBuried with 300k stone beads \n-\u200b\nOne young woman, 19-23 y/o \n-\u200b\nBuried with 6k stone beads and over 3k shell beads \n-\u200b\nFour projectile points at her feet that had their tips broken \nritually  \n-\u200b\nTwo young men buried together \n-\u200b\nA three month old child \n\n-\u200b\nHad no beads, covered in red ochre, a highly colored red pigment \ncommonly used in NWC ritual \n-\u200b\nNearby the burials, three never used projectile points had been \nburied covered in red ochre. Two ground stones and one chipped. \n-\u200b\n3800 yBP \n-\u200b\nThe level of wealth, and the suggestion of heredity also points to the possibility \nthat the older man was a powerful chief \n-\u200b\nCemetery found near a number of small midden  \n-\u200b\nRelatively small population, still mobile  \n-\u200b\nFish, shellfish \n-\u200b\nElite families, hereditary wealth, and the ability of these elites to control large \namount of labor \n-\u200b\nNot based on large scale intensive food production and storage \n-\u200b\nWealth based social stratification and inequality along the south coast between \n4,000-3,800 yBP \n-\u200b\nThis inequality ends shortly after 3,800 yBP \n-\u200b\nMiddle Pacific \n-\u200b\n3,500-1,800 yBP \n-\u200b\nIntensification of resource exploitation and storage \n-\u200b\nSignificant population increase \n-\u200b\nMassive shall middens all along the coast \n-\u200b\nGrowing and sedentary population \n-\u200b\nFishing Weirs \n-\u200b\nMassive increase in the harvesting of fish through the development of fish weirs and traps  \n-\u200b\nTidal weirs and river weirs  \n-\u200b\nMainly focused on salmon and herring  \n-\u200b\nThese two fish are caught in incredible quantities, specifically when they are spawning  \n \n10/14 North West coast continued \n-\u200b\nMiddle Pacific Period  \n-\u200b\n3,500 -1,800 yBP \n-\u200b\nIntensification of resource exploitation and storage \n-\u200b\nSignificant population increase \n-\u200b\nFishing Weirs \n-\u200b\nMassive increase in the harvesting of fish through the development of fish weirs  \n-\u200b\nMainly focused on salmon and herring  \n-\u200b\nThese two fish are caught in incredible quantities, specifically when they\u2026 \n-\u200b\nClam Gardens  \n-\u200b\n3,500 yBP, adapting beaches to create better clam habitat \n-\u200b\nBuilding rock terraces in the tidal zone, they produce flat sandy areas that were ideal for \nclams \n-\u200b\nFood storage \n-\u200b\nDevelopment of food storage technology (sophisticated watertight bentwood boxes) \n-\u200b\nUsed to store oil and food, cooking, boiling water, burying dead \n\n-\u200b\nPaul Mason Site \n-\u200b\n3,000 yBP, first village site in the NWC \n-\u200b\nNorth Coast, along the Skeena River, near the Prince Rupert Harbor \n-\u200b\nAlong a rich salmon run river  \n-\u200b\nTwo rows of rectangular plank houses \n-\u200b\nHouses were laid out row on row, larger houses in the middle, smaller houses radiating \noutwards \n-\u200b\nNorthwest Coast Plant-House \n-\u200b\nLarge wooden buildings, often 40-50 feet long, 30 feet wide and 12 feet tall \n-\u200b\nMade of thick cedar plants, bound together with cedar cordage \n-\u200b\nLabor Requirements \n-\u200b\nFishing weirs, seasonal salmon runs, clam gardens, food processing all required \nsignificant labor at specific times of the year \n-\u200b\nThese houses likely represented ways to organise labor, with the heads of the households \nable to organize the labor of their household \n-\u200b\nEach house was filled with an extended family, often dozens of people, ruled by \nthe family head \n-\u200b\nHighly stratified inside, head of family lived at the back, more common people \nlived near the door \n-\u200b\nThe most important family lived in the largest house in the center of the village \n-\u200b\nLargest house = largest family \n-\u200b\nVillage Chief  \n-\u200b\nThe head of the family with the most amount of prestige held influence over the rest of \nthe community as a kind of paramount Chief \n-\u200b\nIt is disputed how much authority this figure would have had \n-\u200b\nAlong with the Paul Mason site, a few other plant house village sites near the North Coast \nand a couple in the South Coast \n-\u200b\nLayouts of the villages largely reflect the stratification of society (large houses in middle, \nsmaller houses outwards) \n-\u200b\nGraves \n-\u200b\nThese villages are associated with cemeteries that show clear distinct in grave \ngoods \n-\u200b\nDifferent ranks of people from elite, to commoner, to potentially slave \n-\u200b\n10% of all bodies buried with labrets, all men \n-\u200b\nElites and chiefs buried with beautiful carved objects \n-\u200b\nArtistic explosion, early development of the NWC artistic style  \n-\u200b\nDeath-worthy Canoes \n-\u200b\nIndirect evidence of large oceangoing canoes being used by the North Coast groups, for \nfishing, hunting and raiding other communities \n-\u200b\nWarfare \n-\u200b\nGroups from the north would raid down to the south, attacking villages to acquire food, \nprecious goods and prisoners of war \n-\u200b\nSlavery \n-\u200b\nSlavery developed along the North Coast at least by 2,500 yBP \n\n-\u200b\nUnclear evidence on the date: burials as well as demographics \n-\u200b\nSlaves were prisoners of war who captured on raids, or children on slaves \n-\u200b\nMostly women  \n-\u200b\nHereditary position \u2192 if parents are slaves, then children are too \n-\u200b\nLowest status member of society, were seen as wealth to be owned and traded and were \nused as a source of labor  \n-\u200b\nUp to \u00bc of the population in communities along the North Coast were slaves  \n-\u200b\nDogs \n-\u200b\nStarting 4,000 yBP, Coast Salish communities bred a longhaired dog for its wool (dog is \nextinct now) \n-\u200b\nDogs were kept separate from the hunting dogs, in some cases living out on smaller \nislands in packs of 20-30 \n-\u200b\nDogs were fed special diet and bred fo that they would produce a pure white wool that \nwas used for weaving blankets \n-\u200b\nLate Pacific Period \n-\u200b\n1,800 - 200 yBP \n-\u200b\nSubsistence practices stay similar to those of the Middle Pacific, but larger scale \n-\u200b\nEven more warfare, raids  \n-\u200b\nMonumental art, ceremonies  \n-\u200b\nForts \n-\u200b\nNear permanent villages, elaborate stone and wood forts were built to defend against \nraids \n-\u200b\nForts are well used in the face of increased attacks, evidence of long sieges \n-\u200b\nForts concentrated on the South Coast, as Northern Groups would raid the South \n-\u200b\nHaida \n-\u200b\nCalled the Viking of the Northwest coast \n-\u200b\nLiving on the Haida Gwaii and the islands of Southern Alaska  \n-\u200b\nWould raid down the coast as far as california  \n-\u200b\nWar canoes 60 feet in length, hold over 40 warriors \n-\u200b\nRaids of up to 40 war canoes recorded \n-\u200b\nIncreased Warfare led to increased wealth and authority of the individual chief, who \ngained influence over larger areas \n-\u200b\nBy contact the haida had two paramount chiefs  \n-\u200b\nCedar \n-\u200b\nProliferation of Wood working tools \n-\u200b\nBark scrapers, pile drivers, hammer stones, chisel, adzes  \n-\u200b\nLonghouses, monuments, war canoes, fishing weirs, ceremonial art \n-\u200b\nCedar bark was even used for clothes \n-\u200b\nCedar was so important to Indigenous communities, many called it \u201ctree of life\u201d \n-\u200b\nEven used for clothes \n-\u200b\nMost important side of wood for woodmaking  \n-\u200b\nTotem Poles \n-\u200b\nClaimed to have been developed by the Haida, used across the coast \n-\u200b\nElaborate carved single cedar tree, depicting the different clan relations of a single family \n\n-\u200b\nOften placed in front of the longhouse as a public display of family relations, and status \n10/16 Continuation of NWC \n-\u200b\nCeremonial Masks \n-\u200b\nCarved wooden masks \n-\u200b\nOwned by elites, worn as a part of important ritual dances and ceremonies \n-\u200b\nOnly certain masks could be used for certain dances \n-\u200b\nTransformation Mask \n-\u200b\nAs part of the ritual dance they were worn for, some masks were designed to transform \n(ex. Eagle into a person) \n-\u200b\nPotlatch \n-\u200b\nAn important feast/gathering that was practiced across the Northwest Coast \n-\u200b\nHeld during the wintertime, to celebrate major events within an elite family (weddings, \ndeath, births, coming-of-age) \n-\u200b\nA wealthy chief/elite would invite rivals and other relations to bear witness \n-\u200b\nGuests would be treated to an elaborate feast  \n-\u200b\nProvided rich gifts \n-\u200b\nCeremonies \n-\u200b\nThe potlatch was the venue for all the most important ceremonies \n-\u200b\nSongs, stories, and dances \n-\u200b\nJust like the masks, these dances, etc. were owned by particular families, only \nthey could perform  \n-\u200b\nPolitics:  \n-\u200b\nOnly elite families could put on potlatches \n-\u200b\nAllowed families to show their wealth and transform it into social \nstature/influence \n-\u200b\nThe better the food and the richer the gifts, the more prestige the elite family \ngained \n-\u200b\nCompetitive gift giving  \n-\u200b\nOzette Site (Washington State) \n-\u200b\n500 yBP \n-\u200b\nMakah Culture \n-\u200b\nVillage was covered by a mudslide, perfectly preserving it as an archeological site \n-\u200b\nVillage Architecture:  \n-\u200b\nLarge permanent village  \n-\u200b\n6 large multifamily plank houses: \n-\u200b\n35x70 ft long \n-\u200b\n10ft high ceilings \n-\u200b\nFound Whalebone War clubs \n-\u200b\nHunted seals  \n-\u200b\nWould make basketry hats \u2192 woven cedar \n-\u200b\nSocial Inequality in the Northwest Coast \n-\u200b\nNo one clear line of evolving social inequality on the NWC \n-\u200b\nSocial inequality develops at different times, due to different conditions, increases during \nsome period, but decrease at other periods  \n\n-\u200b\nThe long history of labrets (5,000 years on the North Coast) suggests that societies of the \nNW Coast had social hierarchy along the\u2026 \n-\u200b\n The first evidence of hereditary political authority and family wealth (bead burials \naround 4000 yBP) emerges prior to the development of intensive food production\u2026 \n-\u200b\nThis form of political authority eventually dies off \n-\u200b\nIt is replaced with a form of societal stratification that was deeply tied to \nhousehold/labor organization, food storage/wealth, ritual and warfare \nArctic Lowlands: Life on the Ice \n-\u200b\nA massive region that stretches across the coasts of Alaska, Northern Canada, and Greenland \n-\u200b\nGeography \n-\u200b\nCoastal Plain along the Arctic ocean  \n-\u200b\nMountainous in West \n-\u200b\nFlatter to the East  \n-\u200b\nVast archipelago of large islands, stretching from the Yukon all the way to Greenland \n-\u200b\nEnvironment \n-\u200b\nThe coldest environment in North America \n-\u200b\nIncredibly harsh winters, no sunlight for weeks, air temperatures regularly going below \n-40 degrees C \n-\u200b\nWarm very short summers with continuous light \n-\u200b\nSea ice covers the ocean for half the year \n-\u200b\nDuring the summer this ice recedes  \n-\u200b\nFlora and Fauna \n-\u200b\nFew plants grow in this area, a few grasses, mosses and lichens \n-\u200b\nTreeless landscape \n-\u200b\nLack of wood \n-\u200b\nHeat and light source, building material, tool materials \n-\u200b\nSome access to driftwood \n-\u200b\nSummer:  \n-\u200b\nLandscape is rich in wildlife in the summer  \n-\u200b\nNineteen different species of whale can be found in the Arctic ocean, eight of seal, two of \nwalrus \n-\u200b\nMigratory birds: Ducks, ptarmigans, arctic terns, snow geese, and a hundred other birds \nuse the arctic as a summer breeding ground \n-\u200b\nCaribou migrate into the coastal areas during the summer and autumn \n-\u200b\nRich runs of char, salmon, pike, herring, halibut and cod \n-\u200b\nMost of these creatures are seasonal. Sea-mammals, caribou, birds, are fish are migratory \n-\u200b\nNonmigratory animals: polar bears, Musk-ox, Ringed Seal \n-\u200b\nRinged Seals have long sharp claws that allow them to maintain breathing holes in the ice \n\u2192 because they live under ice packs all winter long  \n-\u200b\nWho lived here \n-\u200b\nAlmost all the peoples of the arctic shared a similar toolkit, culture, and language \n-\u200b\nInuit - a group of related cultures that extend almost completely across the Arctic \n-\u200b\nA summer toolkit, boats, and harpoons \n-\u200b\nHunting large whales and walruses in the open water \n\n-\u200b\nLiving out on the sea ice \u2192 Igloos \n-\u200b\nHunting seals at their breathing holes \n-\u200b\nDogs and sleds \n-\u200b\nRich ceremonial culture and artistic tradition: myths, songs, dances centered around \nshamans \n-\u200b\nArcheology in the Arctic \n-\u200b\nExcellent preservation \n-\u200b\nLittle soil buildup \n-\u200b\nLife on the ice is invisible \n-\u200b\nNo soil buildup over time (~1cm) \n-\u200b\nArcheological sites remain untouched and uncovered over time \n10/21 \n-\u200b\nSub-Regions \n-\u200b\nWestern Arctic:  \n-\u200b\nAlaska \n-\u200b\nAleutian Islands \n-\u200b\nSt. Lawrence Island \n-\u200b\nEastern Arctic: \n-\u200b\nMackenzie Delta \n-\u200b\nArctic Archipelago \n-\u200b\nEastern Canada, Labrador and Quebec \n-\u200b\nGreenland \n-\u200b\nHigh Arctic \n-\u200b\nPeriods \n-\u200b\nWestern Arctic: \n-\u200b\nPaleoarctic: 10,000-5,000 yBP \n-\u200b\nTerrestrial hunting  \n-\u200b\nHighly mobile \n-\u200b\nAtl-atl \n-\u200b\nArctic Small Tool Tradition (ASTt): 5,000-2,700 yBP \n-\u200b\nA mixed coastal/terrestrial toolkit that develops/spreads to the Western \nArctic \n-\u200b\nVery small, highly mobile toolkit  \n-\u200b\nMicroblade technology, cores, small projectile points, and burins \n-\u200b\nUsed for hunting \u2192 arrow \n-\u200b\nBow and Arrow:  \n-\u200b\nFirst extensive use of bow and arrow hunting in North \nAmerica \n-\u200b\nMostly used for caribou hunting \n-\u200b\nHarpoons:  \n-\u200b\nSea-Mammal hunting took place with boats and harpoons \n-\u200b\nSpears with detachable heads that were connected to a line \n-\u200b\nCoast in the summer and inland in the winter \n\n-\u200b\nHunted fish and sea mammals on the coast (summer) and caribou in land \n(winter) \n-\u200b\nNorton Tradition: 2,700-1500 yBP \n-\u200b\nThule?inuit: 1,500 yBO-Contact  \n-\u200b\nEastern  \n-\u200b\nASTt in the East (Pre-Dorset): 4,500-2,800 yBP \n-\u200b\nThe glaciers had recently melted  \n-\u200b\nPeople carrying this ASTt technology spread along the coastline  \n-\u200b\nHide tents, mixed coastal (summer) and terrestrial (winter) hunting \n-\u200b\nHighly mobile  \n-\u200b\nDisko Bay Sites:  \n-\u200b\nIncredible preservation of a number of pre-dorset tent villages \n-\u200b\nDriftwood destination \u2192 lots of wood  \n-\u200b\nToolkits:  \n-\u200b\nBow and arrows, harpoons, kayaks \n-\u200b\nDogs:  \n-\u200b\nCarrying packs \n-\u200b\nHunting \n-\u200b\nFood  \n-\u200b\nDorset Tradition: 2,800-800 yBP \n-\u200b\nDeveloped out of pre-dorset, larger populations \n-\u200b\nAcross eastern arctic from Greenland to Newfoundland \n-\u200b\nExclusively coastal adapted toolkit, year-round marine resources  \n-\u200b\nSeals, walrus, beluga, and narwhal. With some exploitation of birds. \n-\u200b\nNo evidence that dorset hunters used bows and arrows, dogs or boats \n-\u200b\nDespite all of these tools being used by their predecessors \n-\u200b\nHunting on the ice:  \n-\u200b\nHunting associated with hunting ringed seals  \n-\u200b\nBreathing holes \n-\u200b\nHarpoons  \n-\u200b\nSnow knives \u2192 evidence of Igloos \n-\u200b\nOne of the most important adaptations to the Arctic \n-\u200b\nWinter living on ice/winter exploitation of the ocean  \n-\u200b\nLamps:  \n-\u200b\nMade from soapstone \n-\u200b\nBurnt seal/while fat  \n-\u200b\nLight and heat on the ice  \n-\u200b\nDuring the winter, people move between large permanent coastal villages \nand small (2-3 families) hunting communities on the ice \n-\u200b\nSemi-subterranean winter houses built of whale bones, stone, and animal \nhides  \n-\u200b\nIn the spring and summer, many dorset communities would come camp \ntogether  \n\n-\u200b\nBuild large long houses where hide tents were clustered next to each \nother. Evidence of seasonal gathering for ceremonial activities.  \n-\u200b\nThule/Inuit: 800 yBP-Contact \n10/23 \n-\u200b\nThule \n-\u200b\nBy 1500 yBP, the Thule culture emerged along the coast in Alaska, maybe from Siberia? \n-\u200b\nHighly mobile population, exclusively coastal hunters focused on large sea mammals \n-\u200b\nAncestors of the Inuit \n-\u200b\nUmiaq \n-\u200b\nAlong with the kayak, the Thile built the umiaq \n-\u200b\nLarger, open, made from wood/bone and hide \n-\u200b\nUp to 10m in length, carried people and goods  \n-\u200b\nWhaling Toolkit \n-\u200b\nUmiaqs, toggling harkoons, and floats enabled hunting bowhead whales, walruses, \nbelugas, and narwhal in the open ocean \n-\u200b\nMuktuk  \n-\u200b\nMainstay of diet \n-\u200b\nWhale blubber \n-\u200b\nEaten raw or frozen  \n-\u200b\nWinter Hunting \n-\u200b\nCombining dogs, harpoons, and igloos out on the sea ice \n-\u200b\nUsing dogs to locate breathing holes of ringed seals  \n-\u200b\nDog Sledding \n-\u200b\nWooden sleds that run on the ice and can be pulled by teams of dogs \n-\u200b\nSleds allowed them to move very quickly during the winter months, and carry a \nsignificant amount of materials along with them  \n-\u200b\nUmiaks allowed the same during the summer \n-\u200b\nImpact of new tools \n-\u200b\nHuge population expansion \n-\u200b\nLikely due to the requirements of whale hunting and rowing the umiak, villages grow in \nsize. Need for organization of labor \n-\u200b\nLived in large semi-subterranean whale bone houses during the winter \n-\u200b\nDesigned to keep cold air out  \n-\u200b\n \n-\u200b\nThule Migration \n-\u200b\nThe whaling toolkit, the sledding and boat technology allowed the Thule to move great \ndistances carrying substantial amounts of gear \n-\u200b\nAround 1000 yBP the Thule begin to spread to the Eastern Arctic  \n-\u200b\nWhat about the Dorset? \n-\u200b\nNo modern Indigenous groups appear to be descended from Dorset \n-\u200b\nEvidence of Thule adoption of certain Dorset technologies and scavenging of Dorset \ntools \n-\u200b\nInuit oral histories speak about the Tuniit, a group of shy and physically imposing people \nwho lived in the area before them  \n\n-\u200b\nAround 800 yBP, Thule/Inuit encountered the Norse in Greenland  \n-\u200b\nBy 500 yBP, the Norse abandoned Greenland, due to changing environment and pressure \nfrom the Inuit \n-\u200b\nAs thule spread across the whole arctic, different populations settle into particular areas \n-\u200b\nThis creates the modern distribution of Inuit groups  \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n\n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n\n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n\n-\u200b\n \n-\u200b\n \n",
        "added_at": "2025-12-01T15:59:04.919101"
      }
    ]
  },
  "fd325d19-5594-4159-9c8e-059a5f6fa908": {
    "created_at": "2025-12-01T16:28:34.420591",
    "files": [
      {
        "name": "Archaeology Notes.pdf",
        "type": "document",
        "text": "9/4/25 Introduction to Archaeology \n-\u200b\nDefinition: The study of the human past through the material remains of human activity.  \n-\u200b\nEverything from the earliest human ancestors 2.5 million years ago, to the 21st century  \n-\u200b\nHow do cultures and societies develop \u2192 not just the historical aspect, but social as well \n-\u200b\nNot just what happens, but all of social life (cultural beliefs, values, subsistence practice, \nart, political/social organization, institutions, technology) throughout all human existence. \n-\u200b\nThis is what makes archaeology a subdiscipline of anthropology \n-\u200b\nArchaeology studies the past VIA objects and materials that made up this social life; \nmaterial culture \u2192 the traces of all the stuff that people throw away or leave behind \n-\u200b\nWe will only have access to things that don't compose, things made of certain materials \n-\u200b\nMaterial culture:  \n-\u200b\nThe objects humans produce, consume, and live their lives with. ie. Stuff \n-\u200b\nTechnology: The stuff people use to adapt to and transform their environments (Houses, \nclothes, weapons, etc.) \n-\u200b\nClothes, food, tools, etc. are also markers of status and identity  \n-\u200b\nNot only \u2018utilitarian\u2019 \u2192 reflects the norms and values of a culture  \n-\u200b\nHow Archaeologists think in steps:  \n1.\u200b The material traces of the past that preserve \n2.\u200b The stuff that produced these traces \n3.\u200b The practices that produced this stuff \n4.\u200b The broader social and cultural norms that gave these practices purpose and meaning    \n-\u200b\nDefinitions + terminology: \n-\u200b\nSurvey: The process of locating clues that might indicate the presence of archaeological \nmaterial culture  \n-\u200b\nArchaeological site: any locality that contains some density of archaeological \nmaterial  \n-\u200b\nExcavation:  Archaeologists dig in a controlled manner into a sire to uncover subsurface \narchaeological material   \n-\u200b\nStratigraphy:  Discrete soil layers that represent distinct occupations of the sight  \n-\u200b\nEach layer is a stratum \n-\u200b\nArtifacts:  Any kind of objects worked by humans  \n-\u200b\nEcofacts: natural objects associated with human behavior \u2192 may have been used \nor effected by humans (ex. Shells, seeds, and bones) \n-\u200b\nContext: Particular location of an artifact, feature, or sight \u2192 the meaningful \nassociations of this object in this location  \n-\u200b\nAbsolute dating:  \n-\u200b\nProvides an absolute calendar date for when an object was produced or deposited  \n-\u200b\n The most useful and widespread mode of absolute dating is C-14 dating, or radiocarbon \ndating  \n-\u200b\nC-14 dating can only date organic material and can only measure their dates accurately up \nto 50,000 years \n-\u200b\nHuman Remains \n-\u200b\nSex, age, health, diet of an individual \n-\u200b\nInequality, especially in terms of health and diet \n\n-\u200b\nNon-Archaeological Sources \n-\u200b\nTextual sources \n-\u200b\nNone in the North American arch. Prior to European contact  \n-\u200b\nEthnographic sources \n-\u200b\nRecords of indigenous communities from the past 100 years \n-\u200b\nOral Histories \n-\u200b\nLiving historic traditions  \n \n9/9/25 North America  \n-\u200b\nGeography of NA:  \n-\u200b\nAtlantic Coastal Plain \n-\u200b\nFlat, coastal plain region running from cape cod to southern texas \n-\u200b\nLong chains and barrier island \n-\u200b\nRivers, bays, and  \n-\u200b\nAppalachian Mountains \n-\u200b\nRelatively low elevation mountain rage, market by deep ravines and large valleys \n-\u200b\nVery well waters, rivers, lakes, gorges, waterfalls \n-\u200b\nCanadian Shield \n-\u200b\nRegion of exposed rock due to glaciers \n-\u200b\nVery rocky, little to no soil \n-\u200b\nInterior plain/Central Lowlands \n-\u200b\nGenerally flat region with deep alluvial soil  \n-\u200b\nBroad river valleys, rolling hills, high bluffs, forested, and many lakes \n-\u200b\nGreat Plains/High Plains \n-\u200b\nHigh elevation, semi-arid region \n-\u200b\nFlat areas broken up by some rocky outcrops and canyons \n-\u200b\nRocky mountains \n-\u200b\nVery high elevation mountain chain \n-\u200b\nMarked by cold weather, glaciers, steep valleys \n-\u200b\nIntermountain Plateau \n-\u200b\nThe raised plateau area between the two western mountain ranges \n-\u200b\nDry, mountainous area marked with deep basins \n-\u200b\nCoastal Mountains \n-\u200b\nHigh mountains along the west coast  \n-\u200b\nFjord like inlets and basins in the north  \n-\u200b\nLarge island archipelago: Haida Gwaii, Vancouver Island, Aleuntian islands  \n-\u200b\nSteep river valleys, fast flowing rivers: Frasier, Columbia, colorado  \n-\u200b\nArctic Lowlands \n-\u200b\nFlat coastal plain along the arctic ocean \n-\u200b\nLarge rivers, including the Mackenzie  \n-\u200b\nLarge island archipelago  \n-\u200b\nBiomes of NA \n-\u200b\nEastern Deciduous Forests \n-\u200b\nOak, Hickory, Maple and Chestnut \n\n-\u200b\nHot humid summers, cold winters \n-\u200b\nLarge animals like deer, moose, wolf, and bear, along with smaller animals like \nsquirrels, rabbits, and chipmunks  \n-\u200b\nSoutheast Evergreen \n-\u200b\nPine forests, deciduous trees, wetlands and marshes \n-\u200b\nMild winters, very hot and humid summers \n-\u200b\nVery rich biodiversity \n-\u200b\nGrasslands \n-\u200b\nTreeless prairie dominated by grasses, herbs and shrubs \n-\u200b\nHot summers, cold winters, extreme weather changes \n-\u200b\nMassive numbers of bison across the Plains, especially in the Western Plains \n-\u200b\nTaiga/Boreal Forest \n-\u200b\nConiferous forests, dominated by source, fir, and pine  \n-\u200b\nShort warm summer with long cold winters \n-\u200b\nSparse biodiversity, dominated by large mammals: bears, wolf, lynx, deer, elk, \nmoose, and caribou \n-\u200b\nTundra \n-\u200b\nTreeless environment dominated by lichens, mosses and grass \n-\u200b\nGround is permanently frozen, year round \n-\u200b\nShort cool summer, extremely cold long winters. Ice covers the sea  \n-\u200b\nMountain Forest \n-\u200b\nCarniferous forest \n-\u200b\nWarm summers and cold snowy winters \n-\u200b\nShorter summers further north \n-\u200b\nShrub Desert \n-\u200b\nVery arid landscape dominated by small trees (mesquite) and small shrubs like \ncreosote  \n-\u200b\nHot summers and warm winters \n-\u200b\nSeasonal rivers and few lakes or other standing water \n-\u200b\nChaparral  \n-\u200b\nMediterranean climate, mild wet winters and warm dry summers \n-\u200b\nDry and hilly landscape dominated by oak forests and short scrub plants \n-\u200b\nTemperate Rainforest \n-\u200b\nLong mild and wet winters, short and dryish summers. Significant rainfall \n-\u200b\nMassive coniferous trees \n-\u200b\nVery well watered area marked with large lakes, rivers, rivers and trees  \n-\u200b\nMississippi River system: \n-\u200b\nThe longest river and one of the largest drainage systems  \n-\u200b\nDrains many of the largest and most important rivers in the continent  \n-\u200b\nMissouri, Ohio, Tennessee, Wabash, Arkansas, and Illinois rivers all serve as tributaries \nto the Mississippi   \n \nEND OF QUIZ #1 MATERIAL  \n \n\n9/11/25 Class \n-\u200b\nArrowheads and Bones:  \n-\u200b\nAs settlers farm and clear their land, they encounter pre-contact Indigenous artifacts and \nburial mounts  \n-\u200b\nWealthy and educated Americans begin collecting these objects   \n-\u200b\nFor the most part, the material culture that was recovered appeared similar to that of the eastern \nIndigenous communities that these eEuro-American settlers had direct contact with  \n-\u200b\nMoving West  \n-\u200b\nMassive mounds and earthquakes in the Mid-West  \n-\u200b\nSeemingly unconnected to any existing Indigenous practices   \n-\u200b\nSquier and Davis:  \n-\u200b\nFirst big archaeological project in North America  \n-\u200b\nIn 1845, Ephraim Squier and Edwin Davis begin a three year project to survey, excavate \nand map the massive prehistoric earthworks along the Ohio and the Mississippi river \n-\u200b\nIn 1848, they published Ancient Monuments of the Mississippi Valley, the first major \nwork in North American archaeology \n-\u200b\nMound Builders Controversy:  \n-\u200b\nSquier and Davis argued that the earthworks were not built by Indigenous people \nbut by an unknown ancient white civilization  \n-\u200b\n\u2018Too advanced\u2019 to have been built by Indigenous peoples  \n-\u200b\nSquier and Davis's work sets off a firestorm of interest, for 30 years this is the most \npressing question in the study of pre-contact America  \n-\u200b\nWho built the mounds: Aztecs, Israelites, Vikings, Mormons \n-\u200b\nIn 1888, two academics prove it was Indigenous peoples  \n-\u200b\nSystematic Looting:  \n-\u200b\nTo answer questions of \u201cwho built these mounds\u201d, most archaeological research focused \non finding and looting Indigenous graves  \n-\u200b\nAs interest in Indigenous history grew, scientists collected\u2026[couldn\u2019t finish] \n-\u200b\nScientific racism:  \n-\u200b\nEven while this research proved Indigenous people build the mounds  \n-\u200b\nThis research on these skeletons was also used to scientifically \u2018prove\u2019 the biological \ninferiority of Indigenous people along with other non-whiye races  \n-\u200b\nNatural History Museums \n-\u200b\nMassive new museums were built to house these and other scientific collections  \n-\u200b\nNew money to fund massive new archaeological studies across North america  \n-\u200b\n\u201cVanishing Indian  \n-\u200b\nBelief in the biological and social inferiority of Indigenous people \n-\u200b\nAnthropologists and archaeologists believed Indigenous peoples\u2026..[couldn\u2019t finish] \n-\u200b\nSalvage Anthropology  \n-\u200b\nLate 19th and early 20th century, anthropologists and archaeologists flooded Indigenous  \ncommunities, collecting information, objects, artifacts, skeletons, and sometimes even \nlive people \n-\u200b\nConvinced Indigenous communities could not be trusted with their own material culture. \nThese scholars often lied and stole  \n\n-\u200b\nThis salvage took millions of objects from Indigenous tribes, along with hundreds of \nthousands of skeletons and grave goods from thousands of archaeological sites  \n-\u200b\nIn the Southwest, three Smithsonians financed expeditions led by archaeologist James \nStevenson purchased and collected over 5,000 pieces of traditional\u2026 \n-\u200b\nBy the 1920s, so much traditional pottery had been removed that many pottery styles and \ntechniques were completely lost \n-\u200b\nBy the 1960s, literally millions of artifacts, and hundreds of thousands of indigenous \nskeletons were in natural history museums and private collectives \n-\u200b\n\u201cTo disturb a grave is not only an insult to the Spirits of the dead, it is a blatant insult to \nthe creator\u201d and \u201cto dig up the grave and strip the departed one of\u2026treasures that were \nplaced by relatives would shock and sicken any moral human being\u201d(Chief Seattle speech \n1854)   \n-\u200b\n1960s/1970s \n-\u200b\nRise of Pan-Indigenous Activism \n-\u200b\nAmerican Indian Movement  (AIM) \n-\u200b\nNovember 1969-June 1971 \u2192 occupation of Alcatraz \n-\u200b\n1971 AIM protest at an archaeological site in (an island off of San Francisco?) \n-\u200b\nVine Deloria \n-\u200b\nCuster Died for your sins, published 1969 \n-\u200b\n\u201cBehind each policy and program with which Indians are plagued, if traced completely \nback to its origin, stands the anthropologist\u201d \n-\u200b\nArchaeologists believed that \u201cthe only real Indians were the dead ones\u201d \n-\u200b\n\u201cBefore the white men can relate to others he must forgo the pleasure of defining them\u201d \n-\u200b\nMaria Pearsons \n-\u200b\n\u201cYou can give me back my people\u2019s bones and you can quit digging them up\u201d \n-\u200b\nEqual treatment of Indigenous and non-Indigenous remains in Iowa 1976 \n-\u200b\nGot Iowa to pass the first legal protections for indigenous bodies  \n-\u200b\nNational Museum of the American Indian Act of 1989 \n-\u200b\nLocating and repatriating all indigenous human remains and funerary objects in the \nSmithsonian collections \n-\u200b\nAt the time, the museum had the remains of 18000 individuals along with millions of \nitems. \n-\u200b\nNAGPRA 1990 \n-\u200b\nNative American Graves Protection and Repatriation Act  \n-\u200b\nAll the agencies and institutions that received federal funding must be repatriate \nall \u201ccultural items\u201d to lineal descendants and culturally affiliated American \nIndian tribes, Alaskan Natives, and Native Hawaiian organizations: human \nremains, funerary objects, sacred objects, and objects of cultural patrimony  \n-\u200b\nAll future excavations on federal land that might come into contact with burials \nand funerary contexts, archaeologists must consult with relevant indigenous \ndescendent groups \n-\u200b\nRepatriation \n-\u200b\nOver the past 30 years, major repositories have gone through their inventories to identify \nhuman and artifactual remains to be repatriated  \n\n-\u200b\nKennewick Man/The Ancient one  \n-\u200b\nHuman remains found\u2026 \n-\u200b\nFour local indigenous tribes asked for the skeletons back according to NAGPRA \n-\u200b\nForensic archaeologists argued the skeleton was non-indigenous (polynesian ot East \nAsian) and therefore not covered by NAGPRA \n-\u200b\n20 years or highly contentious fights over who has the rights to the remains \n-\u200b\nDNA studies by a Danish geneticist conclusively showed that the remains were related to \ncontemporary indigenous groups, specifically those of the columbian river region \n-\u200b\nThe remains were reburied on February 18, 2017 \n-\u200b\nCollaboration and COnsultation \n-\u200b\nGoing beyond mandatory consultations  \n-\u200b\nIncorporating indigenous concerts and wishes, alongside the demands of traditional \narchaeology \n-\u200b\nTaking seriously indigenous knowledge and interest, including oral histories, traditional \necological knowledge as vital resources from the past  \n-\u200b\nIn theory, archaeologists no longer work on \u2018\u201cresearch \u2018about indigenous peoples\u2019\u201d... \n-\u200b\nCommunity Engaged Archaeology \n-\u200b\nRequires that scholars and community members develop equitable partnership \n-\u200b\nFor many archaeologists the struggle with be to let go of their sole authority, and allow \nothers to make decisions about research goals and interpretations  \n \n9/16/25 Peopling North America \n-\u200b\n19th century theories  \n-\u200b\nBased on skeletal data. No later that 3-4,000 years ago \n-\u200b\nArchaeological data seemed to support this data, no dating technique to prove otherwise \n-\u200b\nFolsom Site, 1908 - Folsom New Mexico \n-\u200b\nGeorge McJunkin, amateur archaeologist \n-\u200b\nIdentifies some bones eroding out of a bank \n-\u200b\nBison antiques - one of the most common animals in North America during the \npleistocene (ice age) \n-\u200b\nHad been extinct since 11,000 yBP \n-\u200b\nEventually in 1926 a team investigates \n-\u200b\nIn the same strata (soil level) as the bones they find a scatter of small lithics  \n-\u200b\nThis was \u201cinsane\u201d because archaeologists initially believed that humans were \n3-4k years old \n-\u200b\nNow, they realized that humans have been here before 11k years ago  \n-\u200b\nBecause of this, no one believed their finds \n-\u200b\n\u201cPeople could not have arrived in North AMerica Prior to the end of the \nIce Age\u201d \n-\u200b\n Head physical anthropologist at the Smithsonian, Ales Hrdlicka shot down any theories \n-\u200b\nFinds were published with disclaimers \n-\u200b\nFound intact articulated bison skeletons, lots of lithics, a projectile point between two \nbison ribs \n-\u200b\nVery finely made projectile points \n\n-\u200b\nUnlike any stone tools they had ever seen \n-\u200b\nDefines by central fluting \u201cfolsom points\u201d \n-\u200b\nBlack Water Draw Site \n-\u200b\n1932, a site near Clovis, New Mexico \n-\u200b\nMore support for the theory of human occupation older that 10,000 years old \n-\u200b\nMultiple deep layers of occupation \n-\u200b\nThe deepest layer: human made lithics combined with animal bones: mammoth, camel, \nhorse, bison antiques, dire wolf, sabre tooth cat \n-\u200b\nAll animals had gone extinct at the end of the Pleistocene  \n-\u200b\nThe same distinctive style of projectile points that was found at  Folsom was found at \nBlackwater Draw in its second deepest layer \n-\u200b\nBelow this, in the lowest layer, a different style of projectile point was noted  \n-\u200b\nClovis Points \n-\u200b\nLarge bifacial stone blade, very wide and thin, the result of very skillful lithic reduction \n-\u200b\nCharacteristic shape and fluting along the base of the point \n-\u200b\nUsed for both hunting and processing meat \n-\u200b\nClovis Tradition \n-\u200b\nClovis points have been found with a wide range of megafaunal bones: mastodon, \nmammoth, and bison. Often in large megafaunal kill and processing sites  \n-\u200b\nBone tools, antler tools \n-\u200b\nRoughly 13,300 - 12,700 yBP \n-\u200b\nClovis First Theory \n-\u200b\nThe clovis tradition is the first major cultural tradition of the Americas and that it \ndeveloped by the very first immigrants to North America \n-\u200b\nThat humans first arrived in North America around 13,000, and when they arrived \ndeveloped a highly specialized tool kit for hunting the big game (megafauna) they found \n-\u200b\nFolsom Points \n-\u200b\nAppear to be the adaptation of the clovis point \n-\u200b\nEven thinner, more reduction, even more technical proficiency requires  \n-\u200b\nDefined by a long fluting scar that runs the whole body of the point  \n-\u200b\nUsed for hunting and processing  \n-\u200b\nFolsom Tradition \n-\u200b\nAlongside Folsom points, archaeologists found associated bone needles, and sandstone \nabrades \n-\u200b\nLess variety of megafauna being hunted, focus on bison \n-\u200b\nRoughly 12,700 - 12,000 years ago \n-\u200b\nThe Late Pleistocene Environment \n-\u200b\nLasts between 126,000 - 11,700 yBP \u2192 Last Glacial Period \n-\u200b\nMarked by a significantly lower average global temperature \n-\u200b\nExpansion of glaciers in the N and S hemisphere \n-\u200b\nLast Glacial Maximum (LGM) \n-\u200b\nRoughly 26,500 yBP glaciers reached their glacial maximum \n-\u200b\nCompletely covered Canada, most of Alaska, and much of the US Midwest \n-\u200b\nCut off any land routes between Asia and the Americas \n\n-\u200b\nBeringia \n-\u200b\nGlaciation caused sea levels to drop as much as 100m. Exposing a massive subcontinent \nbetween Siberia and Alaska \n-\u200b\nThis land route was open between 30,000 - 11,000 yBP \n-\u200b\nHowever, the massive glaciers cut of the unglaciated parts of Alaska from the rest of \nNorth america  \n-\u200b\nIce Free Corridor  \n-\u200b\nBy 19,000 YBP, temperatures began warming, causing the glaciers to retreat \n-\u200b\n This retreat caused an opening up an ice-free corridor between\u2026 \n-\u200b\nIn opening up, the ice free corridor provides the first land route between Asia and the \nAmericas since 30,000 yBP \n \n \n9/18/25 \n-\u200b\nIce free corridor \n-\u200b\nBy 19,000 yBP, temperatures began warming, causing the glaciers to retreat \n-\u200b\nThis retread caused an opening up of the ice free corridor between the Laurentide and\u2026 \n-\u200b\nClovis first cont. \n-\u200b\nIce free corridor serves as the key to the first clovis argument  \n-\u200b\nAs the courier opened up, big game hunters living in Beringia began following game \nthrough the ice free corridor into the central US  \n-\u200b\nOnce in NA they developed a unique toolkit to exploit the resources  \n-\u200b\nStrengths of Clovis-first \n-\u200b\nTies together archaeological and climatic data \n-\u200b\nConnects two very important events \n-\u200b\nThe sudden spread of a clovis material culture across the Americas around \n13,000 \n-\u200b\nThe opening up of a land route between Asia and NA around 13,000 that then \ncloses around 11,000 \n-\u200b\nMegafauna Extinction \n-\u200b\nA third chronological event seems to further support this theory \n-\u200b\nBy 10,000 ya Over 90 genus of mammals weighing over 45kg went extinct  \n-\u200b\nOverkill Hypothesis \n-\u200b\nArchaeologists have linked these extinctions to the arrival of humans and their clovis \ntoolkits \n-\u200b\nThis has been widely adopted by Clovis first theorists, but the evidence is circumstantial \n-\u200b\nOther lines of evidence \n-\u200b\nIt broadly fits within genetic evidence, which shows people originating in Serbia\u2026 \n-\u200b\nIt broadly fits within linguistic evidence, which suggest three distinct migration events \nout of North-East Asia into NA \n-\u200b\nFlaws in Clovis First \n-\u200b\nRecent Genetic testing suggests that Indigenous North Americans split from North-East \nAsian populations around 25,000 years ago \n\n-\u200b\nThey entered into North America and began to diversify roughly between 17,000 - 14,000 \nyears ago \n-\u200b\nEnvironmental studies of the ice-free corridor suggest that it was barren, and potentially \ncovered in water and boulders up until around 12,000 yBP \n-\u200b\nThe Power of Clovis First \n-\u200b\n13,000 years ago \n-\u200b\nCame in by land, across Beringia and through the Ice-Free corridor  \n-\u200b\nBig Game Hunters developed a very specific fluted projectile point  \n-\u200b\nSpread across the lower 48, caused megafaunal extinctions, left an enormous \narchaeological record   \n-\u200b\nThe theory dominated archaeology for fifty years \n-\u200b\nMonte Verde \n-\u200b\nIn the 1970s, waterlogged site in peaty soil along a river in Chile \n-\u200b\nSizeable habitation likely occupied by 20-30 people \n-\u200b\nDates came back 14,500 yBP \n-\u200b\nLithics include a stemmed bifacial point, along with a number of basic scrapers and \ncutting flakes \n-\u200b\nEating shellfish, llama, horse and wild potato \n-\u200b\nHighly varied hunter-foragers, minimal hunting big game \n-\u200b\nSubstantial reliance on aquatic and marine resources. Moved back and forth to coast \n-\u200b\nMore Support \n-\u200b\nTopper site: IMportant rock quarry used throughout the clovis period \n-\u200b\nExcavations found lithics below clovis levels \n-\u200b\nFlakes and burins, no bifaces \n-\u200b\nPre-Clovis Site \n-\u200b\nDates back to as far back at 15,000 yBP have been accepted by many archaeologists \n-\u200b\nSome dates suggest human occupation roughly 20,000-50,000 yBP, although these dates \nare highly controversial  \n-\u200b\nNot a single tradition like Clovis. But a collection of very different sites  \n-\u200b\nCoastal Migration Hypothesis \n-\u200b\nUsed boat technology to come along the Pacific coastline \n-\u200b\nBoat technology is clearly being used by other humans as early as 50,000 years ago \n-\u200b\nThis theory doesn\u2019t have that much evidence  \n-\u200b\nSea-levels have risen almost 100m, inundating most of the areas one would assume there \nshould be sites  \n-\u200b\nPaisley Cave \n-\u200b\nCave in Oregon, human coprolites dated to 14,500 \n-\u200b\nHumans eating small mammals, birds, insects, and fish  \n-\u200b\nWestern Stemmed Tradition  \n-\u200b\nStemmed points common in early (13ky-12kya) sites in Western US \n-\u200b\nSites like Paisley cave sidle suggest it predates clovis stone technology  \n-\u200b\nThese stemmed points are similar to lithics found in Siberia 13kya \n-\u200b\nTriquet Island \n-\u200b\nVillage found 14,000 yBP \n\n-\u200b\nArtifacts found include: Fish hooks, a fire drill, an atl atl, a cache of stone tools \n-\u200b\nA coastal toolkit focused on fishing, small game hunting, and foraging \n-\u200b\nWhy go through coastal migration? (even with an ocean on one side and glaciers on the other) \n-\u200b\nKelp Highway Theory:  \n-\u200b\nA more in depth version of the Coastal Migration Theory  \n-\u200b\nThe specific ecology of much of the northern Pacific is very similar, defined by \nrich kelp forest \n-\u200b\nBy 30kya, coastal populations in Japan develop coastal toolkits \n-\u200b\nBoats, shell fishhooks for deep water fishing, and a stemmed point and \nmicroblade core technology \n-\u200b\nAround 19-15kya, they spread into the pacific northwestern through the recently ice free \nisland route \n-\u200b\nSimilar Toolkit! \n-\u200b\n13,000 yBP, contemporary with Clovis \n-\u200b\nDistinctive barbed points found on channel islands in california \n-\u200b\nSimilar to tanged points found in Japan, 14,5000 yBP \n-\u200b\nButtermilk Creek \n-\u200b\nTexas, 15,500 yBP  \n-\u200b\nNearby a number of very rich Clovis sites  \n-\u200b\nOver 15,000 pre-Clovis lithics, providing the most complete singles site record of what \nClovis lithic technology might have emerged out of \n-\u200b\n12 bifacial points \n-\u200b\nEvidence of stemmed bifacial points from around 15,000 yBP \n-\u200b\nLarge Non-fluted points, might be ancestor to Clovis point 14,000 yBP \n-\u200b\nAlaskan Lithic Record \n-\u200b\nTwo large lithic points have been dated to 13,000 yBP \n-\u200b\nMix of stemmed point technology from the coast and big game lithic point from Alaska  \n-\u200b\n20,000 Year old foot prints found in New mexico \n-\u200b\n Suggests humans have been in the americas than previously thought \n-\u200b\n23,000 - 19,000 yBP \n-\u200b\nLast Glacial Maximum (LGM) \n-\u200b\nRoughly 26,500 yBP glaciers reached their glacial maximum \n-\u200b\nCompletely covered Canada, most of Alaska, and much of the US Midwest \n-\u200b\nWarming began 19,000 yBP \n-\u200b\nAquatic Seeds - Problems Dating \n-\u200b\n \n-\u200b\n \n-\u200b\nReservoir Effect \n-\u200b\nCertain objects in water environments can absorb carbon even once the creature is dead \n-\u200b\nIn most cases this makes an object date younger, but id the surrounding water is filled \nwith older carbon, this absorption can make an object date older  \n-\u200b\n50 year old rupiah seeds have been dated as 8,000 year old according to radiocarbon \ndating \n\n-\u200b\nResearches dated two other sources of data, pollen and the quartz in the sand, using \nluminescence dating \n-\u200b\nBoth came back with the original dates 21000-23000 ya \n-\u200b\nThese dates has come under attack  \n \n9/23/25 Changing Environments: From the Pleistocene to the Holocene \n-\u200b\n??? \n-\u200b\nThe paleo-indian period (13,000 - 10,000 yBP) \n-\u200b\nEnvironment of the End of the Pleistocene \n-\u200b\n20,000 - 13,000 yBP significant warming period \n-\u200b\nRetreat of the glaciers, opening up of new areas of North America  \n-\u200b\nSeattle - similar to a desert arctic land dry tundra (?) \n-\u200b\nThe appalachian mountains looking like Northern Canada \n-\u200b\nAlabama and Southern Texas looking like the forests in New England \n-\u200b\nNew England covers in Ice sheet 20,000 years ago \n-\u200b\nLate Pleistocene Fauna \n-\u200b\nThe land was filled with a range of very large creatures (megafauna) \n-\u200b\nGiant sloths, bear, tapirs, peccaries, American lion, sabre-tooth cat, saiga, camelids, stage \nmoose, shrub-ox, 14 species of pronghorn, horse, mammoths, mastodons, beautiful \narmadillo (twice the size as today\u2019s armadillo\u2019s) , giant beavers, giant condors. \n-\u200b\nCool temperate forests are no longer down in Texas, they have begun moving up to \nGeorgia    \n-\u200b\nTundra starts to dry up 15,000 yBP, no longer covered by glaciers, glaciers have opened \nup great lakes (i.e lake erie)  \n-\u200b\n13,000 yBP - period where we start finding significant evidence of people  \n-\u200b\nClovis Tradition \n-\u200b\n13,300 - 12,700 ya \n-\u200b\nBig game hunters with large fluted projectile points spread across North America  \n-\u200b\nYounger Dryas \n-\u200b\n12,700 - 11,500 yBP, sudden cooling period \n-\u200b\nA drop of 2 to 6 degrees calculus global temperatures  \n-\u200b\nVery hard winters \n-\u200b\n10-15 degrees C colder across much of North America \n-\u200b\nRetreat of forests and the expansion of tundra and Taiga   \n-\u200b\nBroady a return back to the landscape of 15,000 yBP or so \n-\u200b\nBy the end of the younger Dryans, 32 genera of Pleistocene Magnafauna have gone \nextinct \n-\u200b\n70% of all megafaunal species on the continent \n-\u200b\n100% of all species weighing more than 1000kg \n-\u200b\nWhat Caused this Mass Extinction? \n-\u200b\nEnvironment: Rapid change in climate - goes from warm to cold  \n-\u200b\nHumans: Overkill, hunting  \n-\u200b\n \n-\u200b\nHumans: The OverKill Hypothesis \n\n-\u200b\nBig game hunting humans into North America caused sudden extinction of the large \nmammals  \n-\u200b\n25-30 Clovis era kill sites that show evidence of hunting and butchering megafauna \n-\u200b\nOr was it the Younger Dryas? \n-\u200b\nMany megafauna that humans hunted did not go extinct \n-\u200b\nMany megafauna that humans did not hunt went extinct  \n-\u200b\nHuman response to Younger Dryas \n-\u200b\nThe end of a single tool tradition across the continent (clovis) \n-\u200b\nDevelopment of regional tool kits, and likely regional cultures (Folsom, PaleoCoastal \ntradition, Plano, etc) \n-\u200b\nHighly mobile societies moved great distances \n-\u200b\nNortheast, tundra,taiga caribou hunters \n-\u200b\nSite in Turners Falls, 12000 yBP \n-\u200b\nSituated near a caribou river crossing  \n-\u200b\nNear lithic source  \n-\u200b\nBull\u2019s Brook Site \n-\u200b\nOne of the largest sites in NA during this period \n-\u200b\nAdjacent to caribou crossing \n-\u200b\nThousands of lithics and caribou remains found  \n-\u200b\nLikely drive site \n-\u200b\nPlains \n-\u200b\nBison hunters using Folsom points and other large points \n-\u200b\nPaleocoastal \n-\u200b\nHighly mobile society focused on coastal tool kits and boats \n-\u200b\nIntensive exploitation of waterflow, shellfish, fish, and small seals \n-\u200b\nSerrated and stemmed lithic points, lithic crescents used to hunt birds \n-\u200b\nAfter the Younger dryas \n-\u200b\nWarming 11,500 yBP ends the Younger Dryas and brings us to the Holocene, a nice \nstable climate period \n-\u200b\nTemperate deciduous forests spread across the Northeast, replacing the taiga and pushing \nback the grasslands \n-\u200b\nAdaptation \n-\u200b\nGeneral switch from reliance on extinct megafaunsa, as well as non extinct migratory \ngame like caribou and bison to smaller migratory game like deer \n-\u200b\nRockshelters \n-\u200b\nProtected sites and communities could return to as part of their local hunting cycle \n-\u200b\nLarge rockshelter sites, occupied by significant numbers of people returning every year \nfor generations \n-\u200b\nNine Major traditions emerging out of the nine major ecological biomes \n-\u200b\n \n-\u200b\n \n9/25/25 California  \n-\u200b\nCalifornia: Baskets and Acorns \n-\u200b\n5 major regional parts to California: \n\n-\u200b\nSouth Coast/Channel Islands \n-\u200b\nCentral Coast \n-\u200b\nNorth Coast \n-\u200b\nSierra Nevadas \n-\u200b\nCentral Valley \n-\u200b\nCalifornia \n-\u200b\nMild climate on coast and valley \n-\u200b\nVery diverse landscapes, deserts, deserts, mountains, and beaches \n-\u200b\nCoasts and Interiors \n-\u200b\nRich coats filled with shellfish, fish and sea mammals \n-\u200b\nDry and mountainous interiors but with rich foraging opportunities; seeds, nuts and edible \nplants \n-\u200b\nOak Tree Forests \n-\u200b\nCovering much of the coast and the interior of California  \n-\u200b\nCalifornia at Contact \n-\u200b\nFirst contacts (1542) \n-\u200b\nThe densest population of any region of North America of central Mexico \n-\u200b\nEstimated \u2153 of entire Indigenous population of lower 48 states \n-\u200b\nRoughly a hundred different indigenous cultural groups, each speaking their own \nlanguages, maintaining their own cultural practice \n-\u200b\nIn the interior they has thousands of large permanent villages \n-\u200b\nAlong the coast they had permanent villages of up to 1000 people ruled by powerful \nchiefs who had incredible wealth and political control over vast resources and labor \n-\u200b\nThey used an early form of money, and a trade network that extended across the Western \ncontinent  \n-\u200b\nDifferent California Periods \n-\u200b\nPaleo-Coastal Period (13,000 - 9,000 yBP) \n-\u200b\nAs early as 13,000 years ago  \n-\u200b\nHighly mobile societies focused on\u2026 \n-\u200b\nEarly Holocene (9,000-7,000 yBP) \n-\u200b\nWarming environment  \n-\u200b\nOccupation of central and south coast \n-\u200b\nEarly but relatively sparse occupation of the interior  \n-\u200b\nIntensive coastal exploitation along southern and central coast \n-\u200b\nShell Middens: significant shellfish consumption, and sea mammals (seals and \nsea lions) \n-\u200b\nMiddle Holocene (7,000-3,000 yBP) \n-\u200b\nIncreased population clustered on south and central coast, but also throughout the \nrest of California \n-\u200b\nWarming climate, interior becomes more arid \n-\u200b\nIncreased population spread across all regions of California  \n-\u200b\nRegional toolkits and regional cultures  \n-\u200b\nSedentism along coasts \n\n-\u200b\nHeavy occupation of San Francisco Bay, large permanent villages, focused on \nclams  \n-\u200b\nFirst substantial settlement along the northern coast \n-\u200b\nCentral valley and Sierra Nevada: acorn and salmon \n-\u200b\nLarge cemeteries built in the Central Valley. Burials show rich grave goods, some \nevidence of societal stratification, extensive evidence of warfare  \n-\u200b\nLate Holocene (3,000 - 150 yBP) \n-\u200b\nCooling CLimare (except for warming blip 1,000-800 yBP) \n-\u200b\nIncredible population growth across california \n-\u200b\nThe general distribution og indigenous cultures at contact begins to form \n-\u200b\nSouth Coast and Channel Islands: \n-\u200b\nCooling climate leads to increasingly rich coastal environments \n-\u200b\nTechnological adaptations to exploit coastal resources, population \ngrowth, and increased socio-political complexity  \n-\u200b\nMilling stones \n-\u200b\nOr also known as manos and metates, used for the crushing of hard seeds and other food \nprocessing \n-\u200b\nToolkit more adapted to foraging  \n-\u200b\nSmall groups, 2-4 families in size \n-\u200b\nDiverse diet focused on seed foraging, shellfish collection and waterfowl \n-\u200b\nDecline in projectile points  \n-\u200b\nBaskets/Fishing in CA \n-\u200b\nUsed for storing food, cooking, transporting, collecting food, art \n-\u200b\nOn the coast, boats and nets were used to hunt sea mammals on the open sea (seals, \ndolphins and sea lions) \n-\u200b\nFish hook used to catch larger fish (mackerel and lingcod) \n-\u200b\nMortar and Pestle \n-\u200b\nFound in most regions of California \n-\u200b\nCommonly viewed as a technology primarily used for processing acorn  \n-\u200b\nAcorn Processing \n-\u200b\nAcorns must be ground into a flour and leached before eating \n-\u200b\nMortar and Pestle unlock the rich oak forests of California  \n-\u200b\nSierra Nevada and Central Valley \n-\u200b\nPopulations increased dramatically \n-\u200b\nNumerous different Indigenous groups establish permanent villages among the acorn rich \nhills \n-\u200b\nBedrock Mortars \n-\u200b\nMortarts are cut into the bedrock  \n-\u200b\nDrastically increases scale of acorn processing \n-\u200b\nAcorn processing becomes sedentary \n-\u200b\nMiwok \n-\u200b\nOne of the largest cultural groups living in the Sierra Nevadas \n-\u200b\n100s of permanent villages throughout the hills and central valley  \n-\u200b\nVillages  \n\n-\u200b\nSources of water, and exposed bedrock  \n-\u200b\nVillages were situated close to the best acorn trees \n-\u200b\nSmall wooden hits called u\u2019machas built around a main roundhouse (hangi) for \nmeetings and ceremonies  \n-\u200b\n10-30 Umachas in a village  \n-\u200b\nBaskets:  \n-\u200b\nProduced roughly 15 different types of baskets  \n-\u200b\nMainly made from sedge, dogbane, and willow bark  \n-\u200b\nFire Gardens: \n-\u200b\nThe Miwok would light fires in the oak forests surrounding their village  \n-\u200b\nPeriodic ground fires would clear brush and support plentiful acorn crops \n-\u200b\nWIld tobacco also managed in this way  \n-\u200b\nStaple: Acorns \n-\u200b\nMiwok social like organized around acorn production \n-\u200b\nIncredible surplus of food from acorns facilitated larger populations, craft \nspecialization, and community wealth \n-\u200b\nThe staple of the Miwok diet was acorn stew (nupa) \n-\u200b\nCha\u2019ka \n-\u200b\nAcorns could be stored for long periods \n-\u200b\nStored in large granaries, or Cha\u2019kas, in Miwok \n-\u200b\nThese could be 8 or more feet high and were made of poles interwoven with \nslender brush stems  \n-\u200b\nThe source of community wealth \n-\u200b\nNon-hierarchical society, no central authority \n-\u200b\nCentral figures, like the shamans, held significant influence over spiritual \nmanners \n-\u200b\nThe roundhouses was the site of elaborate dancing festivals and community \ndecision making \n-\u200b\nAcorn Economy and Gender \n-\u200b\nAll aspects of acorn procurements, storage, and processing were performed and \ncontrolled by women \n-\u200b\nHunting and trade were the domain of men  \n-\u200b\nAs acorns became the essential staple for communities like the Mowok, women also \ngained an incredible amount of influence  \n-\u200b\nIn determining the best place to process acorns, women determined where villages were \nlocated \n-\u200b\nThrough their control of acorn storage, women controlled the wealth of the community  \n-\u200b\nArchaeologists often assume foraging, processing food, and domestic work (practices \nassociated with women in many societies) are less prestigious than hunting and trading \n(associated with men) \n-\u200b\nThe Miwok shows how the opposite is often true  \n-\u200b\n \n-\u200b\nSociopolitical Complexity \n-\u200b\nSociopolitical- Relating to the social and political organization of a society: \n\n-\u200b\n How society organizes itself, how it distributes power and influence  \n-\u200b\nComplexity - How internally differentiated is a society? How many different classes of \npeople, specialists, institutions, etc?  \n-\u200b\nTo what extent does the society have an institutionalized hierarchy of power and \ninfluence that govern central aspects of sicla life? Chiefs, elites commoners  \n-\u200b\nComplexity \u2260 advanced \n-\u200b\nIt is not a moral judgment  \n-\u200b\nChumash \n-\u200b\nCultrual group that emerges around Santa Barbara \n-\u200b\nBetween 3,000 and 1,000 yPB, there is an incredible increase in the population  \n-\u200b\nBy 750 yBP most of this population has coalesced into large permanent villages of up to \n1,000 people \n-\u200b\nWhen Spanish arrived in the 17th century, they identified hundreds of villages and the \npopulation of the chumas in the tens of thousands \n-\u200b\nRuled by chiefs and wealthy elites \n-\u200b\nTechnological Changes \n-\u200b\nA stronger more efficient fish hook made from abalone shell \n-\u200b\nAllowed them to catch large tuna, shark and swordfish \n-\u200b\nHarpoon technology allows open water hunting of sea-mammals \n-\u200b\nTomol \n-\u200b\nBetween 3000-2000 yBP the Chumash developed an incredibly sturdy and fast \nocean-going vessel, the Tomol \n-\u200b\nPlank canoe, made from red-wood cedar \n-\u200b\nVery sophisticated construction method, weaving together planks with natural tar  \n-\u200b\nVery stable ocean-going vessel, facilitated trade and open ocean fishing  \n-\u200b\nShell beads \n-\u200b\nAround 2000 yBP staggering numbers of shell beads were produced along the coast in \nthe Channel Islands and Santa Barbara \n-\u200b\nThese beads were standardized and strung into long strands \n-\u200b\nWere traded along the coast for food, baskets, other goods \n-\u200b\nTrade \n-\u200b\nThese bead strands highly valued across California and beyond, and the Chumash would \nsail up and down the coast trading them with other groups  \n-\u200b\nThe development of these beads coincided with a rapis explosion of regional trade \n-\u200b\nSouthwest, the Great Basin, and the Northwest Coasts \n-\u200b\nShell-Money \n-\u200b\nAs a universally in demand good, shell-beads facilitated the trade of other goods. They \nfunctioned as a currency \n-\u200b\nThe standardization of shell-beads is seen as the first evidence of money in North \nAmerica \n-\u200b\nCraft Specialization \n-\u200b\nThe repsoduction of this varied tool kit relied upon full time specialists \n-\u200b\nCertain people specialized in particular activities fill time \n\n-\u200b\nBoat building, shell-bead production, astronomy, shamanism, undertaking, \ndancing and singing  \n-\u200b\nThe building of the Tomol boats was tightly controlled by the guild known as the \nBrotherhood of the Tomol  \n-\u200b\nThis guild closely guarded the techniques they used to build the essential vessel  \n-\u200b\nShamans and Rock-art \n-\u200b\nThe development of an incredible elaborate rock art tradition in the Santa Barbara \nRegions \n-\u200b\nPainted by religious specialists, or chamans, often in an altered state, the paintings \ndepicted interactions with the spirit worl  \n-\u200b\nLocated in caves, these paintings were an essential part of the ceremonial life and in \nparticular coming of age ceremonies like spirit quests  \n-\u200b\nChiefs \n-\u200b\nBetween 3,000 yBP and 1,000 yBP, the development of a leader that weilded paramount \nauthority within a village  \n-\u200b\nWhile they may have been elected originally, by contact these chiefs were hereditary, and \nmostly men, although they could be women  \n-\u200b\nThese chefs helped organize shell-bead production and trade, managed resources and \nlabor, developed intervillage alliances \n-\u200b\nChief power was closely linked to the Tomol boat technology, as well as shell-bead \nproduction \n-\u200b\nElites \n-\u200b\nA smaller number of elite families who controlled substantial wealth, resources,and labor \n-\u200b\nTried to control of shell-bead beds, trade routes and boat building  \n10/7/25 California 2: Boats and Chiefs \n-\u200b\nMain theories of China's political development \n-\u200b\nPopulation growth starting around 3,000 yBP combined with the rise of shell-bead \nproduction and the need to organize labor \n-\u200b\nSudden environmental pressures such as drought at around 1,000 yBP, or water warming \naround 800 yBP required political leadership to maintain food production and storage \n-\u200b\nControl of boat production technology and trade networks starting around 1,500 yBP \n-\u200b\nSocial Evolution\u2019s explanation \n-\u200b\nFor many years, anthropologists had a basic answer to the question of evolution \n-\u200b\nOver time societies tend to go from simple to complex  \n-\u200b\nAll societies go through the same basic stages of complexity \n-\u200b\nChanges in the complexity of a society were driven by changes in methods of subsistence \n-\u200b\nAll aspects of society (population, settlement pattern, economy, political, etc) \nshifted as subsistence practices shifted \n-\u200b\nThe level of sociopolitical complexity (its political organization) was directly tied \nto its mode of subsistence \n-\u200b\nie: the development of social stratification (inequity) was a natural outcome of a \nchanging mode of subsistence \n-\u200b\nDomestication/Agricultural: the end of Egalitarianism \n-\u200b\nA band settles down and begins to domesticate the animals it once hunted \n\n-\u200b\nDomestication leads to larger populations, increases sedentism, and food surplus \n-\u200b\nLeads to social differentiation: the rise of a class of people to organize labor during the \nharvest, to store the surplus of food, and to protect the land and the surplus  \n-\u200b\n\u2018Complex\u201d Hunter Gatherers?\u200b  \n-\u200b\nHunter and foragers who develop high levels of social differentiation, inequality, political \nauthority, and other non-egalitarian features \n-\u200b\nReveals a fake in social evolutionary logic \n-\u200b\nThere is no single way of explaining the wide variety of human societies \n-\u200b\nThere is no one way of explaining why political authority and inequality develops in all \nplaces \n-\u200b\nThere is no immutable law of how the different parts of a society (subsistence, \npopulation, political authority, stratification) will necessarily relate or develope \n-\u200b\nThere is no single evolutionary line to rank them on \n-\u200b\nWe have to understand the particularities of local histories, local regions, and local \ncultures \n-\u200b\nCalifornia to the Northwest Coast \n-\u200b\nThe NW coast regions of North America has many more examples of \u2018Complex\u2019 Hunter \nGatherers; hereditary chiefs, elites, and slavery, all within communities that subsist by \nhunting and gathering  \n-\u200b\nLonger histories than those of the Chumas \n-\u200b\nNW Coast \n-\u200b\nA mild climate with rich resources  \n-\u200b\nMild winters and cool summers  \n-\u200b\nVery rocky landscape  \n-\u200b\nLarge islanfs \n-\u200b\nAt contact in the late 18th century, densely populated area, highly varied cultural groups \n-\u200b\nLarge populations that relied on highly sophisticated hunting and gathering subsistence \ntechnologies \n-\u200b\nStorage and surplus of food \n-\u200b\nLarge permanent villages of longhouses occupied by extended families \n-\u200b\nEach longhouse might be\u2026 \n-\u200b\nHighly hierarchical societies ruled by hereditary chiefs and stratifies by class: alites, \ncommoners, and slaves \n-\u200b\nEach village controlled by paramount chief, who might rule over multiple villages  \n-\u200b\nChiefs possessed incredible levels of material wealth, prestige and authority  \n-\u200b\nLed war parties to capture wealth and enslave prisoners of war \n-\u200b\nElite families and chiefs build totem poles and other forms of monumental art to reflect \ntheir status and family lineage \n10/9/25:  \n-\u200b\nThings archaeologists look for \n-\u200b\nMaterial evidence that might be used to identify the emergence of social inequality: \n-\u200b\nSymbols of rank, pieces of clothing \n-\u200b\nRich graves goods, especially among children  \n-\u200b\nHouse size variation  \n\n-\u200b\nResource intensification, warfare, and feasting \n-\u200b\nNorthWest coast subregions \n-\u200b\nNorth coast \n-\u200b\nColder, fewer resources  \n-\u200b\nLess dense population \n-\u200b\nLonger evidence of social status, warfare \n-\u200b\nCentral coast \n-\u200b\nSouth coast \n-\u200b\nMore resources  \n-\u200b\nDense population \n-\u200b\nLonger history of wealth  \n-\u200b\nOther geographic features:  \n-\u200b\nFraser river \n-\u200b\nSkeena river \n-\u200b\nVancouver island \n-\u200b\nHaida Gwai  \n-\u200b\n \n-\u200b\nChronology \n-\u200b\nPaleo-Coastal Period (15000-9500 yBP) \n-\u200b\nHighly adapted coastal foraging \n-\u200b\nArchaic (9500-5000 yBP) \n-\u200b\nChanging sea-levels and environment \n-\u200b\nVery little archaeological data \n-\u200b\nStart of microblade technology  \n-\u200b\nMix of marine and terrestrial resources  \n-\u200b\nShellfish, salmon, halibut, cod, some sea mammal \n-\u200b\nDeer and caribou \n-\u200b\nThe Pacific Period:  \n-\u200b\nEarly (5000-3500 yBP) \n-\u200b\nShell middens  \n-\u200b\nThin midden build-up, small populations \n-\u200b\nChanging toolkit \n-\u200b\nChipped stone tools were mostly replaced with ground stone \ntools \n-\u200b\nAdzes and other woodworking tools \n-\u200b\nMiddle \n-\u200b\nLate \n-\u200b\nPacific era:  \n-\u200b\nStill very mobile societies, but groups concentrated in the same area \n-\u200b\nCommunities become tied to places \n-\u200b\nMicroblade technology \n-\u200b\nSmall sharp stone blades taken off cores and hafted onto larger tools \n-\u200b\nVery efficient and adaptable toolkit perfect for highly mobile groups across different \nlandscapes \n\n-\u200b\nCemeteries \n-\u200b\nFound on both the North and South Coast  \n-\u200b\nDeep Oral Histories \n-\u200b\nOral histories from multiple different groups recount periods of great migration that end \naround this time period (pacific era) \n-\u200b\nAdawx: \u200b\n \n-\u200b\nThe Tsimshian Oral tradition \n-\u200b\nEach clan in charge of keeping track of their history  \n-\u200b\nHistory of the Tsimshian recounts both myths and historical events \n-\u200b\nRoughly 5000 yBP the Tsimthian settled on the coast  \n-\u200b\nAccounts of a Great Flood seem to match up to a climatic event 7000 yBP \n-\u200b\nSocial Transformations \n-\u200b\nNorth Coast \u2192 extensive evidence of warfare \n-\u200b\n20% of all skeletons show evidence of\u2026 \n-\u200b\nHidden Falls Site, Alaska \n-\u200b\n4600-3200 yBP \n-\u200b\nEvidence of markers of status: labrets and ground stone pendants \n-\u200b\nLabrets:  \n-\u200b\nStone piercing of lower lip  \n-\u200b\nStatus/Rank \n-\u200b\nElite social status emerging in small North Coast communities tied to a rise of violence \n-\u200b\nBead Burials:  \n-\u200b\nIn the south coast 4000-38000 yBP, denizens of cemeteries \n-\u200b\nRich grave goods, bead burials  \n-\u200b\nVERY rich  \n-\u200b\nGreen point burial:  \n-\u200b\nFour burials found \n-\u200b\n35 y/o male associated with thousands of beads  \n-\u200b\nTsawwassen Burial:  \n-\u200b\nA man in his 40\u2019s buried with 11,000 stone beads \n-\u200b\nA boy 11-14 y/o with 53,000 stone beads \n-\u200b\nSechelt Inlet Burial Site  \n-\u200b\nRichest grave goods ever found in NWC \n-\u200b\n5 bodies recovered with hundreds of thousands of beads  \n-\u200b\nAll members of a single family  \n-\u200b\nAn older adult male, 50+ y/o \n-\u200b\nBuried with 300k stone beads \n-\u200b\nOne young woman, 19-23 y/o \n-\u200b\nBuried with 6k stone beads and over 3k shell beads \n-\u200b\nFour projectile points at her feet that had their tips broken \nritually  \n-\u200b\nTwo young men buried together \n-\u200b\nA three month old child \n\n-\u200b\nHad no beads, covered in red ochre, a highly colored red pigment \ncommonly used in NWC ritual \n-\u200b\nNearby the burials, three never used projectile points had been \nburied covered in red ochre. Two ground stones and one chipped. \n-\u200b\n3800 yBP \n-\u200b\nThe level of wealth, and the suggestion of heredity also points to the possibility \nthat the older man was a powerful chief \n-\u200b\nCemetery found near a number of small midden  \n-\u200b\nRelatively small population, still mobile  \n-\u200b\nFish, shellfish \n-\u200b\nElite families, hereditary wealth, and the ability of these elites to control large \namount of labor \n-\u200b\nNot based on large scale intensive food production and storage \n-\u200b\nWealth based social stratification and inequality along the south coast between \n4,000-3,800 yBP \n-\u200b\nThis inequality ends shortly after 3,800 yBP \n-\u200b\nMiddle Pacific \n-\u200b\n3,500-1,800 yBP \n-\u200b\nIntensification of resource exploitation and storage \n-\u200b\nSignificant population increase \n-\u200b\nMassive shall middens all along the coast \n-\u200b\nGrowing and sedentary population \n-\u200b\nFishing Weirs \n-\u200b\nMassive increase in the harvesting of fish through the development of fish weirs and traps  \n-\u200b\nTidal weirs and river weirs  \n-\u200b\nMainly focused on salmon and herring  \n-\u200b\nThese two fish are caught in incredible quantities, specifically when they are spawning  \n \n10/14 North West coast continued \n-\u200b\nMiddle Pacific Period  \n-\u200b\n3,500 -1,800 yBP \n-\u200b\nIntensification of resource exploitation and storage \n-\u200b\nSignificant population increase \n-\u200b\nFishing Weirs \n-\u200b\nMassive increase in the harvesting of fish through the development of fish weirs  \n-\u200b\nMainly focused on salmon and herring  \n-\u200b\nThese two fish are caught in incredible quantities, specifically when they\u2026 \n-\u200b\nClam Gardens  \n-\u200b\n3,500 yBP, adapting beaches to create better clam habitat \n-\u200b\nBuilding rock terraces in the tidal zone, they produce flat sandy areas that were ideal for \nclams \n-\u200b\nFood storage \n-\u200b\nDevelopment of food storage technology (sophisticated watertight bentwood boxes) \n-\u200b\nUsed to store oil and food, cooking, boiling water, burying dead \n\n-\u200b\nPaul Mason Site \n-\u200b\n3,000 yBP, first village site in the NWC \n-\u200b\nNorth Coast, along the Skeena River, near the Prince Rupert Harbor \n-\u200b\nAlong a rich salmon run river  \n-\u200b\nTwo rows of rectangular plank houses \n-\u200b\nHouses were laid out row on row, larger houses in the middle, smaller houses radiating \noutwards \n-\u200b\nNorthwest Coast Plant-House \n-\u200b\nLarge wooden buildings, often 40-50 feet long, 30 feet wide and 12 feet tall \n-\u200b\nMade of thick cedar plants, bound together with cedar cordage \n-\u200b\nLabor Requirements \n-\u200b\nFishing weirs, seasonal salmon runs, clam gardens, food processing all required \nsignificant labor at specific times of the year \n-\u200b\nThese houses likely represented ways to organise labor, with the heads of the households \nable to organize the labor of their household \n-\u200b\nEach house was filled with an extended family, often dozens of people, ruled by \nthe family head \n-\u200b\nHighly stratified inside, head of family lived at the back, more common people \nlived near the door \n-\u200b\nThe most important family lived in the largest house in the center of the village \n-\u200b\nLargest house = largest family \n-\u200b\nVillage Chief  \n-\u200b\nThe head of the family with the most amount of prestige held influence over the rest of \nthe community as a kind of paramount Chief \n-\u200b\nIt is disputed how much authority this figure would have had \n-\u200b\nAlong with the Paul Mason site, a few other plant house village sites near the North Coast \nand a couple in the South Coast \n-\u200b\nLayouts of the villages largely reflect the stratification of society (large houses in middle, \nsmaller houses outwards) \n-\u200b\nGraves \n-\u200b\nThese villages are associated with cemeteries that show clear distinct in grave \ngoods \n-\u200b\nDifferent ranks of people from elite, to commoner, to potentially slave \n-\u200b\n10% of all bodies buried with labrets, all men \n-\u200b\nElites and chiefs buried with beautiful carved objects \n-\u200b\nArtistic explosion, early development of the NWC artistic style  \n-\u200b\nDeath-worthy Canoes \n-\u200b\nIndirect evidence of large oceangoing canoes being used by the North Coast groups, for \nfishing, hunting and raiding other communities \n-\u200b\nWarfare \n-\u200b\nGroups from the north would raid down to the south, attacking villages to acquire food, \nprecious goods and prisoners of war \n-\u200b\nSlavery \n-\u200b\nSlavery developed along the North Coast at least by 2,500 yBP \n\n-\u200b\nUnclear evidence on the date: burials as well as demographics \n-\u200b\nSlaves were prisoners of war who captured on raids, or children on slaves \n-\u200b\nMostly women  \n-\u200b\nHereditary position \u2192 if parents are slaves, then children are too \n-\u200b\nLowest status member of society, were seen as wealth to be owned and traded and were \nused as a source of labor  \n-\u200b\nUp to \u00bc of the population in communities along the North Coast were slaves  \n-\u200b\nDogs \n-\u200b\nStarting 4,000 yBP, Coast Salish communities bred a longhaired dog for its wool (dog is \nextinct now) \n-\u200b\nDogs were kept separate from the hunting dogs, in some cases living out on smaller \nislands in packs of 20-30 \n-\u200b\nDogs were fed special diet and bred fo that they would produce a pure white wool that \nwas used for weaving blankets \n-\u200b\nLate Pacific Period \n-\u200b\n1,800 - 200 yBP \n-\u200b\nSubsistence practices stay similar to those of the Middle Pacific, but larger scale \n-\u200b\nEven more warfare, raids  \n-\u200b\nMonumental art, ceremonies  \n-\u200b\nForts \n-\u200b\nNear permanent villages, elaborate stone and wood forts were built to defend against \nraids \n-\u200b\nForts are well used in the face of increased attacks, evidence of long sieges \n-\u200b\nForts concentrated on the South Coast, as Northern Groups would raid the South \n-\u200b\nHaida \n-\u200b\nCalled the Viking of the Northwest coast \n-\u200b\nLiving on the Haida Gwaii and the islands of Southern Alaska  \n-\u200b\nWould raid down the coast as far as california  \n-\u200b\nWar canoes 60 feet in length, hold over 40 warriors \n-\u200b\nRaids of up to 40 war canoes recorded \n-\u200b\nIncreased Warfare led to increased wealth and authority of the individual chief, who \ngained influence over larger areas \n-\u200b\nBy contact the haida had two paramount chiefs  \n-\u200b\nCedar \n-\u200b\nProliferation of Wood working tools \n-\u200b\nBark scrapers, pile drivers, hammer stones, chisel, adzes  \n-\u200b\nLonghouses, monuments, war canoes, fishing weirs, ceremonial art \n-\u200b\nCedar bark was even used for clothes \n-\u200b\nCedar was so important to Indigenous communities, many called it \u201ctree of life\u201d \n-\u200b\nEven used for clothes \n-\u200b\nMost important side of wood for woodmaking  \n-\u200b\nTotem Poles \n-\u200b\nClaimed to have been developed by the Haida, used across the coast \n-\u200b\nElaborate carved single cedar tree, depicting the different clan relations of a single family \n\n-\u200b\nOften placed in front of the longhouse as a public display of family relations, and status \n10/16 Continuation of NWC \n-\u200b\nCeremonial Masks \n-\u200b\nCarved wooden masks \n-\u200b\nOwned by elites, worn as a part of important ritual dances and ceremonies \n-\u200b\nOnly certain masks could be used for certain dances \n-\u200b\nTransformation Mask \n-\u200b\nAs part of the ritual dance they were worn for, some masks were designed to transform \n(ex. Eagle into a person) \n-\u200b\nPotlatch \n-\u200b\nAn important feast/gathering that was practiced across the Northwest Coast \n-\u200b\nHeld during the wintertime, to celebrate major events within an elite family (weddings, \ndeath, births, coming-of-age) \n-\u200b\nA wealthy chief/elite would invite rivals and other relations to bear witness \n-\u200b\nGuests would be treated to an elaborate feast  \n-\u200b\nProvided rich gifts \n-\u200b\nCeremonies \n-\u200b\nThe potlatch was the venue for all the most important ceremonies \n-\u200b\nSongs, stories, and dances \n-\u200b\nJust like the masks, these dances, etc. were owned by particular families, only \nthey could perform  \n-\u200b\nPolitics:  \n-\u200b\nOnly elite families could put on potlatches \n-\u200b\nAllowed families to show their wealth and transform it into social \nstature/influence \n-\u200b\nThe better the food and the richer the gifts, the more prestige the elite family \ngained \n-\u200b\nCompetitive gift giving  \n-\u200b\nOzette Site (Washington State) \n-\u200b\n500 yBP \n-\u200b\nMakah Culture \n-\u200b\nVillage was covered by a mudslide, perfectly preserving it as an archeological site \n-\u200b\nVillage Architecture:  \n-\u200b\nLarge permanent village  \n-\u200b\n6 large multifamily plank houses: \n-\u200b\n35x70 ft long \n-\u200b\n10ft high ceilings \n-\u200b\nFound Whalebone War clubs \n-\u200b\nHunted seals  \n-\u200b\nWould make basketry hats \u2192 woven cedar \n-\u200b\nSocial Inequality in the Northwest Coast \n-\u200b\nNo one clear line of evolving social inequality on the NWC \n-\u200b\nSocial inequality develops at different times, due to different conditions, increases during \nsome period, but decrease at other periods  \n\n-\u200b\nThe long history of labrets (5,000 years on the North Coast) suggests that societies of the \nNW Coast had social hierarchy along the\u2026 \n-\u200b\n The first evidence of hereditary political authority and family wealth (bead burials \naround 4000 yBP) emerges prior to the development of intensive food production\u2026 \n-\u200b\nThis form of political authority eventually dies off \n-\u200b\nIt is replaced with a form of societal stratification that was deeply tied to \nhousehold/labor organization, food storage/wealth, ritual and warfare \nArctic Lowlands: Life on the Ice \n-\u200b\nA massive region that stretches across the coasts of Alaska, Northern Canada, and Greenland \n-\u200b\nGeography \n-\u200b\nCoastal Plain along the Arctic ocean  \n-\u200b\nMountainous in West \n-\u200b\nFlatter to the East  \n-\u200b\nVast archipelago of large islands, stretching from the Yukon all the way to Greenland \n-\u200b\nEnvironment \n-\u200b\nThe coldest environment in North America \n-\u200b\nIncredibly harsh winters, no sunlight for weeks, air temperatures regularly going below \n-40 degrees C \n-\u200b\nWarm very short summers with continuous light \n-\u200b\nSea ice covers the ocean for half the year \n-\u200b\nDuring the summer this ice recedes  \n-\u200b\nFlora and Fauna \n-\u200b\nFew plants grow in this area, a few grasses, mosses and lichens \n-\u200b\nTreeless landscape \n-\u200b\nLack of wood \n-\u200b\nHeat and light source, building material, tool materials \n-\u200b\nSome access to driftwood \n-\u200b\nSummer:  \n-\u200b\nLandscape is rich in wildlife in the summer  \n-\u200b\nNineteen different species of whale can be found in the Arctic ocean, eight of seal, two of \nwalrus \n-\u200b\nMigratory birds: Ducks, ptarmigans, arctic terns, snow geese, and a hundred other birds \nuse the arctic as a summer breeding ground \n-\u200b\nCaribou migrate into the coastal areas during the summer and autumn \n-\u200b\nRich runs of char, salmon, pike, herring, halibut and cod \n-\u200b\nMost of these creatures are seasonal. Sea-mammals, caribou, birds, are fish are migratory \n-\u200b\nNonmigratory animals: polar bears, Musk-ox, Ringed Seal \n-\u200b\nRinged Seals have long sharp claws that allow them to maintain breathing holes in the ice \n\u2192 because they live under ice packs all winter long  \n-\u200b\nWho lived here \n-\u200b\nAlmost all the peoples of the arctic shared a similar toolkit, culture, and language \n-\u200b\nInuit - a group of related cultures that extend almost completely across the Arctic \n-\u200b\nA summer toolkit, boats, and harpoons \n-\u200b\nHunting large whales and walruses in the open water \n\n-\u200b\nLiving out on the sea ice \u2192 Igloos \n-\u200b\nHunting seals at their breathing holes \n-\u200b\nDogs and sleds \n-\u200b\nRich ceremonial culture and artistic tradition: myths, songs, dances centered around \nshamans \n-\u200b\nArcheology in the Arctic \n-\u200b\nExcellent preservation \n-\u200b\nLittle soil buildup \n-\u200b\nLife on the ice is invisible \n-\u200b\nNo soil buildup over time (~1cm) \n-\u200b\nArcheological sites remain untouched and uncovered over time \n10/21 \n-\u200b\nSub-Regions \n-\u200b\nWestern Arctic:  \n-\u200b\nAlaska \n-\u200b\nAleutian Islands \n-\u200b\nSt. Lawrence Island \n-\u200b\nEastern Arctic: \n-\u200b\nMackenzie Delta \n-\u200b\nArctic Archipelago \n-\u200b\nEastern Canada, Labrador and Quebec \n-\u200b\nGreenland \n-\u200b\nHigh Arctic \n-\u200b\nPeriods \n-\u200b\nWestern Arctic: \n-\u200b\nPaleoarctic: 10,000-5,000 yBP \n-\u200b\nTerrestrial hunting  \n-\u200b\nHighly mobile \n-\u200b\nAtl-atl \n-\u200b\nArctic Small Tool Tradition (ASTt): 5,000-2,700 yBP \n-\u200b\nA mixed coastal/terrestrial toolkit that develops/spreads to the Western \nArctic \n-\u200b\nVery small, highly mobile toolkit  \n-\u200b\nMicroblade technology, cores, small projectile points, and burins \n-\u200b\nUsed for hunting \u2192 arrow \n-\u200b\nBow and Arrow:  \n-\u200b\nFirst extensive use of bow and arrow hunting in North \nAmerica \n-\u200b\nMostly used for caribou hunting \n-\u200b\nHarpoons:  \n-\u200b\nSea-Mammal hunting took place with boats and harpoons \n-\u200b\nSpears with detachable heads that were connected to a line \n-\u200b\nCoast in the summer and inland in the winter \n\n-\u200b\nHunted fish and sea mammals on the coast (summer) and caribou in land \n(winter) \n-\u200b\nNorton Tradition: 2,700-1500 yBP \n-\u200b\nThule?inuit: 1,500 yBO-Contact  \n-\u200b\nEastern  \n-\u200b\nASTt in the East (Pre-Dorset): 4,500-2,800 yBP \n-\u200b\nThe glaciers had recently melted  \n-\u200b\nPeople carrying this ASTt technology spread along the coastline  \n-\u200b\nHide tents, mixed coastal (summer) and terrestrial (winter) hunting \n-\u200b\nHighly mobile  \n-\u200b\nDisko Bay Sites:  \n-\u200b\nIncredible preservation of a number of pre-dorset tent villages \n-\u200b\nDriftwood destination \u2192 lots of wood  \n-\u200b\nToolkits:  \n-\u200b\nBow and arrows, harpoons, kayaks \n-\u200b\nDogs:  \n-\u200b\nCarrying packs \n-\u200b\nHunting \n-\u200b\nFood  \n-\u200b\nDorset Tradition: 2,800-800 yBP \n-\u200b\nDeveloped out of pre-dorset, larger populations \n-\u200b\nAcross eastern arctic from Greenland to Newfoundland \n-\u200b\nExclusively coastal adapted toolkit, year-round marine resources  \n-\u200b\nSeals, walrus, beluga, and narwhal. With some exploitation of birds. \n-\u200b\nNo evidence that dorset hunters used bows and arrows, dogs or boats \n-\u200b\nDespite all of these tools being used by their predecessors \n-\u200b\nHunting on the ice:  \n-\u200b\nHunting associated with hunting ringed seals  \n-\u200b\nBreathing holes \n-\u200b\nHarpoons  \n-\u200b\nSnow knives \u2192 evidence of Igloos \n-\u200b\nOne of the most important adaptations to the Arctic \n-\u200b\nWinter living on ice/winter exploitation of the ocean  \n-\u200b\nLamps:  \n-\u200b\nMade from soapstone \n-\u200b\nBurnt seal/while fat  \n-\u200b\nLight and heat on the ice  \n-\u200b\nDuring the winter, people move between large permanent coastal villages \nand small (2-3 families) hunting communities on the ice \n-\u200b\nSemi-subterranean winter houses built of whale bones, stone, and animal \nhides  \n-\u200b\nIn the spring and summer, many dorset communities would come camp \ntogether  \n\n-\u200b\nBuild large long houses where hide tents were clustered next to each \nother. Evidence of seasonal gathering for ceremonial activities.  \n-\u200b\nThule/Inuit: 800 yBP-Contact \n10/23 \n-\u200b\nThule \n-\u200b\nBy 1500 yBP, the Thule culture emerged along the coast in Alaska, maybe from Siberia? \n-\u200b\nHighly mobile population, exclusively coastal hunters focused on large sea mammals \n-\u200b\nAncestors of the Inuit \n-\u200b\nUmiaq \n-\u200b\nAlong with the kayak, the Thile built the umiaq \n-\u200b\nLarger, open, made from wood/bone and hide \n-\u200b\nUp to 10m in length, carried people and goods  \n-\u200b\nWhaling Toolkit \n-\u200b\nUmiaqs, toggling harkoons, and floats enabled hunting bowhead whales, walruses, \nbelugas, and narwhal in the open ocean \n-\u200b\nMuktuk  \n-\u200b\nMainstay of diet \n-\u200b\nWhale blubber \n-\u200b\nEaten raw or frozen  \n-\u200b\nWinter Hunting \n-\u200b\nCombining dogs, harpoons, and igloos out on the sea ice \n-\u200b\nUsing dogs to locate breathing holes of ringed seals  \n-\u200b\nDog Sledding \n-\u200b\nWooden sleds that run on the ice and can be pulled by teams of dogs \n-\u200b\nSleds allowed them to move very quickly during the winter months, and carry a \nsignificant amount of materials along with them  \n-\u200b\nUmiaks allowed the same during the summer \n-\u200b\nImpact of new tools \n-\u200b\nHuge population expansion \n-\u200b\nLikely due to the requirements of whale hunting and rowing the umiak, villages grow in \nsize. Need for organization of labor \n-\u200b\nLived in large semi-subterranean whale bone houses during the winter \n-\u200b\nDesigned to keep cold air out  \n-\u200b\n \n-\u200b\nThule Migration \n-\u200b\nThe whaling toolkit, the sledding and boat technology allowed the Thule to move great \ndistances carrying substantial amounts of gear \n-\u200b\nAround 1000 yBP the Thule begin to spread to the Eastern Arctic  \n-\u200b\nWhat about the Dorset? \n-\u200b\nNo modern Indigenous groups appear to be descended from Dorset \n-\u200b\nEvidence of Thule adoption of certain Dorset technologies and scavenging of Dorset \ntools \n-\u200b\nInuit oral histories speak about the Tuniit, a group of shy and physically imposing people \nwho lived in the area before them  \n\n-\u200b\nAround 800 yBP, Thule/Inuit encountered the Norse in Greenland  \n-\u200b\nBy 500 yBP, the Norse abandoned Greenland, due to changing environment and pressure \nfrom the Inuit \n-\u200b\nAs thule spread across the whole arctic, different populations settle into particular areas \n-\u200b\nThis creates the modern distribution of Inuit groups  \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n\n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n\n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n-\u200b\n \n\n-\u200b\n \n-\u200b\n \n",
        "added_at": "2025-12-01T16:28:34.526101"
      }
    ]
  },
  "41c11d2b-837d-4a87-a535-dbe7251fe278": {
    "created_at": "2025-12-01T16:43:37.019332",
    "files": [
      {
        "name": "326 Proj.pdf",
        "type": "document",
        "text": "Python\ngit add .  \ngit commit -m \u201c   \u201c \ngit push -u origin main  \n \n \nmain.py:  \n \nimport os \nimport json \nimport uuid \nimport subprocess \nfrom datetime import datetime \nfrom typing import Dict, Any \n \nimport fitz  # PyMuPDF \nfrom pptx import Presentation \nfrom fastapi import FastAPI, Request, UploadFile, File, Form \nfrom fastapi.responses import HTMLResponse, PlainTextResponse \nfrom fastapi.staticfiles import StaticFiles \nfrom fastapi.templating import Jinja2Templates \nimport google.generativeai as genai \n \n# Ai model  \ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\")) \nGEM_MODEL = genai.GenerativeModel(\"gemini-1.5-flash\") \n \n# ----------------- CONFIG ----------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \nUPLOAD_DIR = os.path.join(BASE_DIR, \"upload\") \nAUDIO_DIR = os.path.join(UPLOAD_DIR, \"audio\") \nDOC_DIR = os.path.join(UPLOAD_DIR, \"docs\") \nSESSIONS_PATH = os.path.join(BASE_DIR, \"sessions.json\") \n \nos.makedirs(UPLOAD_DIR, exist_ok=True) \nos.makedirs(AUDIO_DIR, exist_ok=True) \nos.makedirs(DOC_DIR, exist_ok=True) \n \n#client = (api_key=os.getenv(\"GEMINI_API_KEY\")) \n \napp = FastAPI(title=\"Helektron Study Assistant\") \napp.mount(\"/static\", StaticFiles(directory=os.path.join(BASE_DIR, \"static\")), \nname=\"static\") \ntemplates = Jinja2Templates(directory=os.path.join(BASE_DIR, \"templates\")) \n \n\n# ----------------- SESSION STORAGE ----------------- \ndef load_sessions() -> Dict[str, Any]: \n    if not os.path.exists(SESSIONS_PATH): \n        return {} \n    with open(SESSIONS_PATH, \"r\") as f: \n        try: \n            return json.load(f) \n        except json.JSONDecodeError: \n            return {} \n \ndef save_sessions(sessions: Dict[str, Any]) -> None: \n    with open(SESSIONS_PATH, \"w\") as f: \n        json.dump(sessions, f, indent=2) \n \nsessions: Dict[str, Any] = load_sessions() \n \ndef get_or_create_session(session_id: str | None) -> str: \n    global sessions \n    if session_id and session_id in sessions: \n        return session_id \n    new_id = str(uuid.uuid4()) \n    sessions[new_id] = { \n        \"created_at\": datetime.utcnow().isoformat(), \n        \"files\": [],   # list of {\"name\",\"type\",\"text\"} \n    } \n    save_sessions(sessions) \n    return new_id \n \ndef add_text_to_session(session_id: str, filename: str, file_type: str, text: \nstr) -> None: \n    global sessions \n    if session_id not in sessions: \n        session_id = get_or_create_session(None) \n    sessions[session_id][\"files\"].append({ \n        \"name\": filename, \n        \"type\": file_type, \n        \"text\": text, \n        \"added_at\": datetime.utcnow().isoformat() \n    }) \n    save_sessions(sessions) \n \ndef get_session_text(session_id: str) -> str: \n    session = sessions.get(session_id) \n    if not session: \n\n        return \"\" \n    blocks = [f\"--- {f['name']} ({f['type']}) ---\\n{f['text']}\" for f in \nsession[\"files\"] if f.get(\"text\")] \n    return \"\\n\\n\".join(blocks) \n \n# ----------------- HELPERS: FILE TEXT EXTRACTION ----------------- \ndef extract_txt(path: str) -> str: \n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        return f.read() \n \ndef extract_pdf(path: str) -> str: \n    doc = fitz.open(path) \n    texts = [] \n    for page in doc: \n        texts.append(page.get_text()) \n    doc.close() \n    return \"\\n\".join(texts) \n \ndef extract_pptx(path: str) -> str: \n    prs = Presentation(path) \n    texts = [] \n    for slide in prs.slides: \n        for shape in slide.shapes: \n            if hasattr(shape, \"text\"): \n                texts.append(shape.text) \n    return \"\\n\".join(texts) \n \ndef transcribe_audio(path: str) -> str: \n    # Find whisper binary \n    candidates = [ \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"build\", \"bin\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper-main\", \"build\", \"bin\", \"whisper-cli\"), \n    ] \n    whisper_bin = next((c for c in candidates if os.path.exists(c)), None) \n    if not whisper_bin: \n        raise RuntimeError(\"Whisper binary not found. Build whisper.cpp \nfirst.\") \n \n    wav_path = path + \".wav\" \n    subprocess.run( \n        [\"ffmpeg\", \"-y\", \"-i\", path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \n\"-ac\", \"1\", wav_path], \n        check=True \n\n    ) \n \n    out_prefix = path + \"_out\" \n    subprocess.run( \n        [whisper_bin, \"-m\", os.path.join(BASE_DIR, \"whisper.cpp\", \"models\", \n\"ggml-base.en.bin\"), \n         \"-f\", wav_path, \"--output-txt\", \"--output-file\", out_prefix], \n        check=True \n    ) \n \n    txt_path = out_prefix + \".txt\" \n    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        transcript = f.read() \n \n    # cleanup \n    for p in [wav_path, txt_path]: \n        if os.path.exists(p): \n            os.remove(p) \n \n    return transcript \n \n# ----------------- GPT PROMPTS ----------------- \ndef get_summary_prompt(transcript: str) -> str: \n    return f\"\"\" \nBased on the following combined study materials (lectures, slides, PDFs, notes, \ntranscripts), \ncreate a **structured, detailed, and academically accurate study summary**. \n \nOrganize the output into the following clearly labeled sections: \n \n- **Overview**: Briefly describe the overall topic and goals. \n- **Key Concepts Introduced**: List and explain major ideas, theories, \nformulas, or processes. \n- **Detailed Topic Breakdown**: Group related ideas and summarize explanations, \nreasoning steps, and relationships. \n- **Important Definitions**: Provide concise definitions for technical terms or \ndomain-specific vocabulary. \n- **Examples or Applications**: Summarize any examples, demonstrations, or \nreal-world applications. \n- **Main Takeaways**: What a student should remember after studying this \nmaterial. \n \nExpectations: \n- Use clear, concise bullet points. \n\n- Prioritize conceptual correctness. \n- Use simple academic language suitable for undergraduate study. \n- If information is unclear or missing, label it as `TBD`. \n \n---BEGIN MATERIAL--- \n{transcript} \n---END MATERIAL--- \n\"\"\" \n \ndef get_keyterms_prompt(text: str) -> str: \n    return f\"\"\" \nFrom the combined study materials below, extract **10\u201320 key terms** with short \ndefinitions. \n \nFor each term, include: \n- The term \n- A 1\u20132 sentence definition \n- (Optional) A quick note on why it matters in this context. \n \nFormat as a bulleted list. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_questions_prompt(text: str) -> str: \n    return f\"\"\" \nCreate **8\u201312 practice questions** based on the combined study materials below. \n \nInclude a mix of: \n- Conceptual understanding questions \n- Short-answer questions \n- Application/problem-style questions (where possible) \n \nDo NOT provide answers, only questions. \nGroup them into sections if appropriate (e.g., 'Conceptual', 'Application'). \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_resources_prompt(text: str) -> str: \n\n    return f\"\"\" \nBased on the topics and concepts in the combined study materials below, \nrecommend **3\u20137 high-quality external resources** for further study. \n \nEach resource should include: \n- Title \n- Type (e.g., textbook chapter, .edu article, video lecture) \n- Source (e.g., university, well-known platform) \n- 1\u20132 sentences on why it is relevant. \n \nPrefer: \n- .edu domains \n- Reputable textbooks \n- Well-known educational channels \n \nYou do NOT need to provide URLs, just clear, identifiable references. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef call_gemini(prompt: str) -> str: \n    try: \n        response = GEM_MODEL.generate_content(prompt) \n        return response.text.strip() \n    except Exception as e: \n        return f\"[Gemini API Error: {e}]\" \n \n# ----------------- ROUTES ----------------- \n@app.get(\"/\", response_class=HTMLResponse) \ndef index(request: Request): \n    return templates.TemplateResponse(\"index.html\", {\"request\": request}) \n \n# Upload any material (txt, pdf, pptx, mp4, m4a) \n@app.post(\"/upload\", response_class=HTMLResponse) \nasync def upload_material( \n    request: Request, \n    file: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    global sessions \n \n    session_id = get_or_create_session(session_id) \n\n    filename = file.filename \n    ext = filename.split(\".\")[-1].lower() \n \n    # Save file \n    if ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n        save_dir = AUDIO_DIR \n        kind = \"audio\" \n    else: \n        save_dir = DOC_DIR \n        kind = \"document\" \n \n    os.makedirs(save_dir, exist_ok=True) \n    saved_path = os.path.join(save_dir, f\"{uuid.uuid4().hex}_{filename}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await file.read()) \n \n    # Extract text \n    try: \n        if ext == \"txt\": \n            text = extract_txt(saved_path) \n        elif ext == \"pdf\": \n            text = extract_pdf(saved_path) \n        elif ext in [\"pptx\"]: \n            text = extract_pptx(saved_path) \n        elif ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n            text = transcribe_audio(saved_path) \n        else: \n            text = f\"[Unsupported file type: {ext}]\" \n    except Exception as e: \n        text = f\"[Error processing file {filename}: {e}]\" \n \n    add_text_to_session(session_id, filename, kind, text) \n \n    # Render updated materials list (fragment) \n    session = sessions[session_id] \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n\n \n# Live transcription: audio blob from the browser \n@app.post(\"/upload_live_audio\", response_class=HTMLResponse) \nasync def upload_live_audio( \n    request: Request, \n    audio: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    session_id = get_or_create_session(session_id) \n    filename = audio.filename or \"live_recording.webm\" \n    ext = filename.split(\".\")[-1].lower() \n \n    os.makedirs(AUDIO_DIR, exist_ok=True) \n    saved_path = os.path.join(AUDIO_DIR, f\"live_{uuid.uuid4().hex}.{ext}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await audio.read()) \n \n    try: \n        text = transcribe_audio(saved_path) \n    except Exception as e: \n        text = f\"[Error transcribing live audio: {e}]\" \n \n    add_text_to_session(session_id, filename, \"audio-live\", text) \n    session = sessions[session_id] \n \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# ---- Study tools (summary, key terms, questions, resources) ---- \n \n@app.get(\"/summary/{session_id}\", response_class=PlainTextResponse) \ndef generate_summary(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_summary_prompt(text) \n\nPython\n        content = call_gemini(prompt) \n    # Return plain text directly to HTMX target \n    return content \n \n \n@app.get(\"/keyterms/{session_id}\", response_class=PlainTextResponse) \ndef generate_keyterms(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_keyterms_prompt(text) \n        content = call_gemini(prompt) \n    return content \n \n \n@app.get(\"/questions/{session_id}\", response_class=PlainTextResponse) \ndef generate_questions_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_questions_prompt(text) \n        content = call_gemini(prompt) \n    return content \n \n \n@app.get(\"/resources/{session_id}\", response_class=PlainTextResponse) \ndef generate_resources_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_resources_prompt(text) \n        content = call_gemini(prompt) \n    return content \nRag Folder:  \n1.\u200b build_vector_store.py  \n \nimport os \nimport json \n\nimport faiss \nfrom langchain_community.vectorstores import FAISS \nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain_core.documents import Document \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter \n \n# -------------------------------------------- \n# Correct project paths \n# -------------------------------------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \n \n# Uploaded document text is stored here by your FastAPI app \nDOC_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"docs\") \n \n# Vector store output directory \nVS_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"vs\") \n \nos.makedirs(VS_DIR, exist_ok=True) \n \n \n# -------------------------------------------- \n# Load documents from upload/docs/ \n# -------------------------------------------- \ndef load_documents(): \n    docs = [] \n    if not os.path.exists(DOC_DIR): \n        print(f\"Missing directory {DOC_DIR}. No documents found.\") \n        return docs \n \n    for filename in os.listdir(DOC_DIR): \n        path = os.path.join(DOC_DIR, filename) \n        if not os.path.isfile(path): \n            continue \n \n        try: \n            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n                text = f.read().strip() \n                if text: \n                    docs.append(Document(page_content=text, metadata={\"source\": \nfilename})) \n        except Exception as e: \n            print(f\"Error loading {filename}: {e}\") \n \n    return docs \n\n \n \n# -------------------------------------------- \n# Build vector store \n# -------------------------------------------- \ndef build_vector_store(): \n    print(f\"Building vector store from {DOC_DIR}...\") \n \n    docs = load_documents() \n \n    if not docs: \n        print(\"No documents found in upload/docs/. Upload files in the web app \nfirst.\") \n        return \n \n    print(f\"Loaded {len(docs)} raw documents.\") \n \n    # ------------------------------- \n    # Chunk documents \n    # ------------------------------- \n    splitter = RecursiveCharacterTextSplitter( \n        chunk_size=1000, # ~750 tokens \n        chunk_overlap=200, # contextual overlap \n        length_function=len \n    ) \n \n    split_docs = [] \n    for doc in docs: \n        chunks = splitter.split_text(doc.page_content) \n        for chunk in chunks: \n            split_docs.append( \n                Document( \n                    page_content=chunk, \n                    metadata=doc.metadata  # keep source info \n                ) \n            ) \n \n    print(f\"Created {len(split_docs)} total chunks.\") \n \n    # ------------------------------- \n    # Generate embeddings \n    # ------------------------------- \n    embeddings = HuggingFaceEmbeddings() \n \n\nPython\n    print(\"Generating embeddings...\") \n    vectorstore = FAISS.from_documents(split_docs, embeddings) \n \n    # ------------------------------- \n    # Save FAISS index + metadata \n    # ------------------------------- \n    print(\"Saving FAISS index and metadata...\") \n \n    faiss.write_index(vectorstore.index, os.path.join(VS_DIR, \"vs.faiss\")) \n \n    with open(os.path.join(VS_DIR, \"docs.json\"), \"w\") as f: \n        json.dump([d.dict() for d in split_docs], f, indent=2) \n \n    print(\"\\n\ud83c\udf89 Vector store built successfully!\") \n    print(f\"Saved to: {VS_DIR}\") \n \n \nif __name__ == \"__main__\": \n    build_vector_store() \n \n2.\u200b rag_utils.py \n \nimport os \nimport json \nimport numpy as np \nfrom google import genai \n \n# Load Gemini client \nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\")) \n \n \n# ----------------------------- \n# Helper: Load vector store \n# ----------------------------- \ndef load_store(path): \n    if not os.path.exists(path): \n        return [] \n    with open(path, \"r\") as f: \n        return json.load(f) \n \n \n\n# ----------------------------- \n# Helper: Save vector store \n# ----------------------------- \ndef save_store(path, data): \n    with open(path, \"w\") as f: \n        json.dump(data, f, indent=4) \n \n \n# ----------------------------- \n# Embed text using Gemini \n# ----------------------------- \ndef embed_text(text: str): \n    \"\"\" \n    Uses Gemini embeddings. \n    Returns list[float]. \n    \"\"\" \n \n    result = client.models.embed_content( \n        model=\"text-embedding-004\", \n        contents=text, \n    ) \n \n    # Gemini wraps embeddings inside .output_embedding \n    return result.output_embedding \n \n \n# ----------------------------- \n# Add text to class vector store \n# ----------------------------- \ndef add_to_vector_store(class_id: str, text: str): \n    \"\"\" \n    Creates or appends to the vector store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    store = load_store(store_path) \n \n    embedding = embed_text(text) \n \n    store.append({ \n        \"text\": text, \n        \"embedding\": embedding, \n\n    }) \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Build store from scratch (optional) \n# ----------------------------- \ndef build_vector_store(class_id: str, text: str): \n    \"\"\" \n    Clears and rebuilds the entire store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    embedding = embed_text(text) \n \n    store = [ \n        { \n            \"text\": text, \n            \"embedding\": embedding, \n        } \n    ] \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Search RAG store using cosine similarity \n# ----------------------------- \ndef search_vector_store(class_id: str, query: str): \n    \"\"\" \n    Returns the most relevant text chunk for the given query. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    store = load_store(store_path) \n \n    if not store: \n        return None \n \n    query_embedding = embed_text(query) \n    query_vec = np.array(query_embedding) \n\nHTML\n \n    best_score = -999 \n    best_text = None \n \n    for item in store: \n        vec = np.array(item[\"embedding\"]) \n \n        # cosine similarity \n        score = np.dot(query_vec, vec) / ( \n            np.linalg.norm(query_vec) * np.linalg.norm(vec) \n        ) \n \n        if score > best_score: \n            best_score = score \n            best_text = item[\"text\"] \n \n    return best_text \n \n \nTemplates \n1.\u200b templates/index.html  \n \n{% extends \"base.html\" %} \n{% block content %} \n \n<h1>Helektron Study Assistant</h1> \n<p class=\"subtitle\"> \n    Upload lecture materials (PDF, slides, audio, notes), then generate \nsummaries, key terms, questions, and more. \n</p> \n \n<div class=\"layout\"> \n \n    <!-- LEFT PANEL --> \n    <div class=\"card\"> \n        <h2>1. Upload Materials</h2> \n \n        <div class=\"upload-box\"> \n            <p><strong>Upload files:</strong> .txt, .pdf, .pptx, .mp4, .m4a</p> \n \n            <form \n\n                hx-post=\"/upload\" \n                hx-target=\"#materials-panel\" \n                hx-swap=\"innerHTML\" \n                hx-indicator=\"#upload-loading\" \n                enctype=\"multipart/form-data\"> \n \n                <input type=\"file\" name=\"file\" required> \n                <input type=\"hidden\" name=\"session_id\" id=\"session_id_input\"> \n                <br><br> \n                <button class=\"primary\" type=\"submit\">Upload</button> \n            </form> \n        </div> \n \n        <div class=\"upload-box\"> \n            <p><strong>Live Transcription:</strong> record audio and transcribe \nwith Whisper.</p> \n            <button class=\"secondary\" id=\"record-btn\">\ud83c\udf99 Start \nRecording</button> \n            <p id=\"record-status\" style=\"font-size: 12px; color:#555; \nmargin-top:6px;\"></p> \n        </div> \n \n        <div id=\"upload-loading\" class=\"htmx-indicator\"> \n            Uploading/processing material... \n        </div> \n \n        <!-- \ud83d\udd25 THIS GETS UPDATED BY upload_status.html --> \n        <div id=\"materials-panel\"> \n            <em>No materials uploaded yet.</em> \n        </div> \n \n    </div> \n \n    <!-- RIGHT PANEL --> \n    <div class=\"card\"> \n        <h2>2. Generate Study Material</h2> \n \n        <p style=\"font-size:13px; color:#555;\">Choose what you want the AI to \ngenerate.</p> \n \n        <div style=\"display:flex; gap:8px; flex-wrap:wrap; \nmargin-bottom:10px;\"> \n            <button class=\"primary\" \n                    id=\"btn-summary\" \n\n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Summary \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-keyterms\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Key Terms \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-questions\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Practice Questions \n            </button> \n \n            <button class=\"secondary\" \n                    id=\"btn-resources\" \n                    disabled \n                    hx-get=\"\" \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                External Resources \n            </button> \n        </div> \n \n        <div id=\"generate-loading\" class=\"htmx-indicator\"> \n            Generating material... \n        </div> \n \n        <div id=\"results-panel\"> \n            <em>Your generated study content will appear here.</em> \n        </div> \n \n    </div> \n\nHTML\nHTML\n</div> \n \n<script src=\"/static/app.js\"></script> \n \n{% endblock %} \n \n2.\u200b templates/base.html  \n \n<!DOCTYPE html> \n<html lang=\"en\"> \n<head> \n    <meta charset=\"UTF-8\"> \n    <title>Helektron Study Assistant</title> \n    <link rel=\"stylesheet\" href=\"/static/styles.css\"> \n</head> \n \n<body> \n<div class=\"app-shell\"> \n    {% block content %}{% endblock %} \n</div> \n \n<script src=\"https://unpkg.com/htmx.org@1.9.10\"></script> \n</body> \n</html> \n \n3.\u200b Fragments folder \na.\u200b templates/fragments/class_list.html  \n \n<ul> \n    {% for cls in classes %} \n    <li> \n        <button  \n            hx-post=\"/set_active_class\"  \n            hx-vals='{\"class_id\": \"{{ cls.id }}\"}' \n            hx-target=\"#upload-list\" \n            hx-swap=\"innerHTML\" \n            class=\"{% if cls.id == active_class_id %}active{% endif %}\" \n\nHTML\nHTML\nHTML\n        > \n            {{ cls.name }} \n        </button> \n    </li> \n    {% endfor %} \n</ul> \n \nb.\u200b templates/fragments/upload_status.html  \n \n{% if session_id %} \n<div data-session-id=\"{{ session_id }}\"></div> \n{% endif %} \n \n{% if files %} \n<ul> \n    {% for f in files %} \n    <li><strong>{{ f.name }}</strong> ({{ f.type }})</li> \n    {% endfor %} \n</ul> \n{% else %} \n<em>No materials uploaded yet.</em> \n{% endif %} \n \nc.\u200b templates/fragments/summary.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n \nd.\u200b templates/fragments/resources.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n\nHTML\nHTML\nCSS\ne.\u200b templates/fragments/questions.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \nf.\u200b\ntemplates/fragments/keyterms.html  \n \n<div class=\"ai-output\"> \n    {{ content | safe }} \n</div> \n \n \nStatic:  \n1.\u200b static/styles.css \n \nbody { \n    margin: 0; \n    font-family: system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", \nsans-serif; \n    background: #f4f4fb; \n} \n \n.app-shell { \n    max-width: 1100px; \n    margin: 0 auto; \n    padding: 24px; \n} \n \nh1 { \n    font-size: 28px; \n    margin-bottom: 8px; \n} \n \n.subtitle { \n    color: #555; \n    margin-bottom: 24px; \n} \n\n \n.layout { \n    display: grid; \n    grid-template-columns: 1.2fr 1.8fr; \n    gap: 20px; \n} \n \n.card { \n    background: white; \n    border-radius: 12px; \n    padding: 16px 20px; \n    box-shadow: 0 1px 3px rgba(15,23,42,0.08); \n    border: 1px solid #e2e8f0; \n} \n \n.card h2 { \n    font-size: 18px; \n    margin-top: 0; \n    margin-bottom: 12px; \n} \n \n.upload-box { \n    border: 2px dashed #cbd5f5; \n    border-radius: 10px; \n    padding: 16px; \n    text-align: center; \n    background: #f9fafb; \n    margin-bottom: 12px; \n} \n \n.upload-box input[type=\"file\"] { \n    margin-top: 8px; \n} \n \nbutton.primary { \n    background: #4f46e5; \n    color: white; \n    border: none; \n    border-radius: 8px; \n    padding: 10px 16px; \n    font-weight: 600; \n    cursor: pointer; \n    font-size: 14px; \n} \n\n \nbutton.primary:disabled { \n    opacity: 0.6; \n    cursor: not-allowed; \n} \n \nbutton.primary:hover:not(:disabled) { \n    background: #4338ca; \n} \n \nbutton.secondary { \n    background: white; \n    color: #4f46e5; \n    border: 1px solid #cbd5f5; \n    border-radius: 8px; \n    padding: 8px 14px; \n    font-weight: 500; \n    cursor: pointer; \n    font-size: 13px; \n} \n \nbutton.secondary:hover { \n    background: #eef2ff; \n} \n \n#materials-list { \n    font-size: 14px; \n    max-height: 200px; \n    overflow-y: auto; \n} \n \n#materials-list ul { \n    padding-left: 18px; \n} \n \n#results-panel { \n    min-height: 250px; \n    font-size: 14px; \n    white-space: pre-wrap; \n} \n \n/* htmx loading indicator */ \n.htmx-indicator { \n    display: none; \n\nJavaScript\n    margin-bottom: 8px; \n    font-size: 13px; \n    color: #6b21a8; \n} \n \n.htmx-request .htmx-indicator { \n    display: block; \n} \n2.\u200b static/app.js \n \nlet currentSessionId = null; \n \nfunction setSessionId(id) { \n    currentSessionId = id; \n    const hidden = document.getElementById(\"session_id_input\"); \n    if (hidden) hidden.value = id; \n    enableStudyButtons(); \n} \n \nfunction enableStudyButtons() { \n    if (!currentSessionId) return; \n \n    const summaryBtn = document.getElementById(\"btn-summary\"); \n    const keyBtn     = document.getElementById(\"btn-keyterms\"); \n    const qBtn       = document.getElementById(\"btn-questions\"); \n    const rBtn       = document.getElementById(\"btn-resources\"); \n \n    [summaryBtn, keyBtn, qBtn, rBtn].forEach(btn => btn.disabled = false); \n \n    summaryBtn.setAttribute(\"hx-get\", `/summary/${currentSessionId}`); \n    keyBtn.setAttribute(\"hx-get\", `/keyterms/${currentSessionId}`); \n    qBtn.setAttribute(\"hx-get\", `/questions/${currentSessionId}`); \n    rBtn.setAttribute(\"hx-get\", `/resources/${currentSessionId}`); \n} \n \ndocument.body.addEventListener(\"htmx:afterSettle\", function () { \n    // Look for the element *inside* #materials-panel that has data-session-id \n    const sidElem = document.querySelector(\"#materials-panel \n[data-session-id]\"); \n    const sid = sidElem ? sidElem.getAttribute(\"data-session-id\") : null; \n \n\n    if (sid && sid !== currentSessionId) { \n        setSessionId(sid); \n    } \n}); \n \n// recording JS: \nlet mediaRecorder = null; \nlet recordedChunks = []; \n \nconst recordBtn = document.getElementById(\"record-btn\"); \nconst recordStatus = document.getElementById(\"record-status\"); \n \nrecordBtn.addEventListener(\"click\", async () => { \n    if (!mediaRecorder || mediaRecorder.state === \"inactive\") { \n        try { \n            const stream = await navigator.mediaDevices.getUserMedia({ audio: \ntrue }); \n            mediaRecorder = new MediaRecorder(stream); \n            recordedChunks = []; \n \n            mediaRecorder.ondataavailable = (e) => { \n                if (e.data.size > 0) recordedChunks.push(e.data); \n            }; \n \n            mediaRecorder.onstop = async () => { \n                const blob = new Blob(recordedChunks, { type: \"audio/webm\" }); \n                await uploadLiveAudio(blob); \n                stream.getTracks().forEach(t => t.stop()); \n                recordStatus.textContent = \"Recording finished. \nTranscribing...\"; \n            }; \n \n            mediaRecorder.start(); \n            recordBtn.textContent = \"\u23f9 Stop Recording\"; \n            recordStatus.textContent = \"Recording...\"; \n        } catch (err) { \n            console.error(err); \n            alert(\"Could not access microphone.\"); \n        } \n    } else if (mediaRecorder.state === \"recording\") { \n        mediaRecorder.stop(); \n        recordBtn.textContent = \"\ud83c\udf99 Start Recording\"; \n    } \n}); \n\nJavaScript\n \nasync function uploadLiveAudio(blob) { \n    const formData = new FormData(); \n    formData.append(\"audio\", blob, \"live_recording.webm\"); \n    if (currentSessionId) { \n        formData.append(\"session_id\", currentSessionId); \n    } \n \n    const resp = await fetch(\"/upload_live_audio\", { \n        method: \"POST\", \n        body: formData \n    }); \n \n    const html = await resp.text(); \n    document.getElementById(\"materials-panel\").innerHTML = html; \n    recordStatus.textContent = \"Live audio processed.\"; \n} \n3.\u200b static/htmx.min.js \n \n(function(e,t){if(typeof define===\"function\"&&define.amd){define([],t)}else if(typeof module===\"object\"&&module.exports){module.exports=t()}else{e.htmx=e.htmx||t()}})(typeof self!==\"undefined\"?self:this,function(){return function(){\"use strict\";var \nQ={onLoad:F,process:zt,on:de,off:ge,trigger:ce,ajax:Nr,find:C,findAll:f,closest:v,values:function(e,t){var r=dr(e,t||\"post\");return \nr.values},remove:_,addClass:z,removeClass:n,toggleClass:$,takeClass:W,defineExtension:Ur,removeExtension:Br,logAll:V,logNone:j,logger:null,config:{historyEnabled:true,historyCacheSize:10,refreshOnHistoryMiss:false,defaultSwapStyle:\"innerHTML\",defaultSwapDelay:0,defaultSettleDelay:20,includeIndicatorStyles:true,indicat\norClass:\"htmx-indicator\",requestClass:\"htmx-request\",addedClass:\"htmx-added\",settlingClass:\"htmx-settling\",swappingClass:\"htmx-swapping\",allowEval:true,allowScriptTags:true,inlineScriptNonce:\"\",attributesToSettle:[\"class\",\"style\",\"width\",\"height\"],withCredentials:false,timeout:0,wsReconnectDelay:\"full-jitter\",wsBinary\nType:\"blob\",disableSelector:\"[hx-disable], \n[data-hx-disable]\",useTemplateFragments:false,scrollBehavior:\"smooth\",defaultFocusScroll:false,getCacheBusterParam:false,globalViewTransitions:false,methodsThatUseUrlParams:[\"get\"],selfRequestsOnly:false,ignoreTitle:false,scrollIntoViewOnBoost:true,triggerSpecsCache:null},parseInterval:d,_:t,createEventSource:function\n(e){return new EventSource(e,{withCredentials:true})},createWebSocket:function(e){var t=new WebSocket(e,[]);t.binaryType=Q.config.wsBinaryType;return t},version:\"1.9.10\"};var \nr={addTriggerHandler:Lt,bodyContains:se,canAccessLocalStorage:U,findThisElement:xe,filterValues:yr,hasAttribute:o,getAttributeValue:te,getClosestAttributeValue:ne,getClosestMatch:c,getExpressionVars:Hr,getHeaders:xr,getInputValues:dr,getInternalData:ae,getSwapSpecification:wr,getTriggerSpecs:it,getTarget:ye,makeFragme\nnt:l,mergeObjects:le,makeSettleInfo:T,oobSwap:Ee,querySelectorExt:ue,selectAndSwap:je,settleImmediately:nr,shouldCancel:ut,triggerEvent:ce,triggerErrorEvent:fe,withExtensions:R};var w=[\"get\",\"post\",\"put\",\"delete\",\"patch\"];var i=w.map(function(e){return\"[hx-\"+e+\"], [data-hx-\"+e+\"]\"}).join(\", \");var \nS=e(\"head\"),q=e(\"title\"),H=e(\"svg\",true);function e(e,t=false){return new RegExp(`<${e}(\\\\s[^>]*>|>)([\\\\s\\\\S]*?)<\\\\/${e}>`,t?\"gim\":\"im\")}function d(e){if(e==undefined){return undefined}let t=NaN;if(e.slice(-2)==\"ms\"){t=parseFloat(e.slice(0,-2))}else if(e.slice(-1)==\"s\"){t=parseFloat(e.slice(0,-1))*1e3}else \nif(e.slice(-1)==\"m\"){t=parseFloat(e.slice(0,-1))*1e3*60}else{t=parseFloat(e)}return isNaN(t)?undefined:t}function ee(e,t){return e.getAttribute&&e.getAttribute(t)}function o(e,t){return e.hasAttribute&&(e.hasAttribute(t)||e.hasAttribute(\"data-\"+t))}function te(e,t){return ee(e,t)||ee(e,\"data-\"+t)}function u(e){return \ne.parentElement}function re(){return document}function c(e,t){while(e&&!t(e)){e=u(e)}return e?e:null}function L(e,t,r){var n=te(t,r);var i=te(t,\"hx-disinherit\");if(e!==t&&i&&(i===\"*\"||i.split(\" \").indexOf(r)>=0)){return\"unset\"}else{return n}}function ne(t,r){var n=null;c(t,function(e){return \nn=L(t,e,r)});if(n!==\"unset\"){return n}}function h(e,t){var r=e.matches||e.matchesSelector||e.msMatchesSelector||e.mozMatchesSelector||e.webkitMatchesSelector||e.oMatchesSelector;return r&&r.call(e,t)}function A(e){var t=/<([a-z][^\\/\\0>\\x20\\t\\r\\n\\f]*)/i;var r=t.exec(e);if(r){return \nr[1].toLowerCase()}else{return\"\"}}function a(e,t){var r=new DOMParser;var n=r.parseFromString(e,\"text/html\");var i=n.body;while(t>0){t--;i=i.firstChild}if(i==null){i=re().createDocumentFragment()}return i}function N(e){return/<body/.test(e)}function l(e){var t=!N(e);var r=A(e);var \nn=e;if(r===\"head\"){n=n.replace(S,\"\")}if(Q.config.useTemplateFragments&&t){var i=a(\"<body><template>\"+n+\"</template></body>\",0);return i.querySelector(\"template\").content}switch(r){case\"thead\":case\"tbody\":case\"tfoot\":case\"colgroup\":case\"caption\":return a(\"<table>\"+n+\"</table>\",1);case\"col\":return \na(\"<table><colgroup>\"+n+\"</colgroup></table>\",2);case\"tr\":return a(\"<table><tbody>\"+n+\"</tbody></table>\",2);case\"td\":case\"th\":return a(\"<table><tbody><tr>\"+n+\"</tr></tbody></table>\",3);case\"script\":case\"style\":return a(\"<div>\"+n+\"</div>\",1);default:return a(n,0)}}function ie(e){if(e){e()}}function I(e,t){return \nObject.prototype.toString.call(e)===\"[object \"+t+\"]\"}function k(e){return I(e,\"Function\")}function P(e){return I(e,\"Object\")}function ae(e){var t=\"htmx-internal-data\";var r=e[t];if(!r){r=e[t]={}}return r}function M(e){var t=[];if(e){for(var r=0;r<e.length;r++){t.push(e[r])}}return t}function oe(e,t){if(e){for(var \nr=0;r<e.length;r++){t(e[r])}}}function X(e){var t=e.getBoundingClientRect();var r=t.top;var n=t.bottom;return r<window.innerHeight&&n>=0}function se(e){if(e.getRootNode&&e.getRootNode()instanceof window.ShadowRoot){return re().body.contains(e.getRootNode().host)}else{return re().body.contains(e)}}function D(e){return \ne.trim().split(/\\s+/)}function le(e,t){for(var r in t){if(t.hasOwnProperty(r)){e[r]=t[r]}}return e}function E(e){try{return JSON.parse(e)}catch(e){b(e);return null}}function U(){var e=\"htmx:localStorageTest\";try{localStorage.setItem(e,e);localStorage.removeItem(e);return true}catch(e){return false}}function \nB(t){try{var e=new URL(t);if(e){t=e.pathname+e.search}if(!/^\\/$/.test(t)){t=t.replace(/\\/+$/,\"\")}return t}catch(e){return t}}function t(e){return Tr(re().body,function(){return eval(e)})}function F(t){var e=Q.on(\"htmx:load\",function(e){t(e.detail.elt)});return e}function \nV(){Q.logger=function(e,t,r){if(console){console.log(t,e,r)}}}function j(){Q.logger=null}function C(e,t){if(t){return e.querySelector(t)}else{return C(re(),e)}}function f(e,t){if(t){return e.querySelectorAll(t)}else{return f(re(),e)}}function \n_(e,t){e=g(e);if(t){setTimeout(function(){_(e);e=null},t)}else{e.parentElement.removeChild(e)}}function z(e,t,r){e=g(e);if(r){setTimeout(function(){z(e,t);e=null},r)}else{e.classList&&e.classList.add(t)}}function \nn(e,t,r){e=g(e);if(r){setTimeout(function(){n(e,t);e=null},r)}else{if(e.classList){e.classList.remove(t);if(e.classList.length===0){e.removeAttribute(\"class\")}}}}function $(e,t){e=g(e);e.classList.toggle(t)}function W(e,t){e=g(e);oe(e.parentElement.children,function(e){n(e,t)});z(e,t)}function \nv(e,t){e=g(e);if(e.closest){return e.closest(t)}else{do{if(e==null||h(e,t)){return e}}while(e=e&&u(e));return null}}function s(e,t){return e.substring(0,t.length)===t}function G(e,t){return e.substring(e.length-t.length)===t}function J(e){var t=e.trim();if(s(t,\"<\")&&G(t,\"/>\")){return \nt.substring(1,t.length-2)}else{return t}}function Z(e,t){if(t.indexOf(\"closest \")===0){return[v(e,J(t.substr(8)))]}else if(t.indexOf(\"find \")===0){return[C(e,J(t.substr(5)))]}else if(t===\"next\"){return[e.nextElementSibling]}else if(t.indexOf(\"next \")===0){return[K(e,J(t.substr(5)))]}else \nif(t===\"previous\"){return[e.previousElementSibling]}else if(t.indexOf(\"previous \")===0){return[Y(e,J(t.substr(9)))]}else if(t===\"document\"){return[document]}else if(t===\"window\"){return[window]}else if(t===\"body\"){return[document.body]}else{return re().querySelectorAll(J(t))}}var K=function(e,t){var \nr=re().querySelectorAll(t);for(var n=0;n<r.length;n++){var i=r[n];if(i.compareDocumentPosition(e)===Node.DOCUMENT_POSITION_PRECEDING){return i}}};var Y=function(e,t){var r=re().querySelectorAll(t);for(var n=r.length-1;n>=0;n--){var i=r[n];if(i.compareDocumentPosition(e)===Node.DOCUMENT_POSITION_FOLLOWING){return \ni}}};function ue(e,t){if(t){return Z(e,t)[0]}else{return Z(re().body,e)[0]}}function g(e){if(I(e,\"String\")){return C(e)}else{return e}}function ve(e,t,r){if(k(t)){return{target:re().body,event:e,listener:t}}else{return{target:g(e),event:t,listener:r}}}function de(t,r,n){jr(function(){var \ne=ve(t,r,n);e.target.addEventListener(e.event,e.listener)});var e=k(r);return e?r:n}function ge(t,r,n){jr(function(){var e=ve(t,r,n);e.target.removeEventListener(e.event,e.listener)});return k(r)?r:n}var me=re().createElement(\"output\");function pe(e,t){var r=ne(e,t);if(r){if(r===\"this\"){return[xe(e,t)]}else{var \nn=Z(e,r);if(n.length===0){b('The selector \"'+r+'\" on '+t+\" returned no matches!\");return[me]}else{return n}}}}function xe(e,t){return c(e,function(e){return te(e,t)!=null})}function ye(e){var t=ne(e,\"hx-target\");if(t){if(t===\"this\"){return xe(e,\"hx-target\")}else{return ue(e,t)}}else{var r=ae(e);if(r.boosted){return \nre().body}else{return e}}}function be(e){var t=Q.config.attributesToSettle;for(var r=0;r<t.length;r++){if(e===t[r]){return true}}return false}function \nwe(t,r){oe(t.attributes,function(e){if(!r.hasAttribute(e.name)&&be(e.name)){t.removeAttribute(e.name)}});oe(r.attributes,function(e){if(be(e.name)){t.setAttribute(e.name,e.value)}})}function Se(e,t){var r=Fr(t);for(var n=0;n<r.length;n++){var i=r[n];try{if(i.isInlineSwap(e)){return true}}catch(e){b(e)}}return \ne===\"outerHTML\"}function Ee(e,i,a){var t=\"#\"+ee(i,\"id\");var o=\"outerHTML\";if(e===\"true\"){}else if(e.indexOf(\":\")>0){o=e.substr(0,e.indexOf(\":\"));t=e.substr(e.indexOf(\":\")+1,e.length)}else{o=e}var r=re().querySelectorAll(t);if(r){oe(r,function(e){var t;var \nr=i.cloneNode(true);t=re().createDocumentFragment();t.appendChild(r);if(!Se(o,e)){t=r}var \nn={shouldSwap:true,target:e,fragment:t};if(!ce(e,\"htmx:oobBeforeSwap\",n))return;e=n.target;if(n[\"shouldSwap\"]){Fe(o,e,e,t,a)}oe(a.elts,function(e){ce(e,\"htmx:oobAfterSwap\",n)})});i.parentNode.removeChild(i)}else{i.parentNode.removeChild(i);fe(re().body,\"htmx:oobErrorNoTarget\",{content:i})}return e}function \nCe(e,t,r){var n=ne(e,\"hx-select-oob\");if(n){var i=n.split(\",\");for(var a=0;a<i.length;a++){var o=i[a].split(\":\",2);var s=o[0].trim();if(s.indexOf(\"#\")===0){s=s.substring(1)}var l=o[1]||\"true\";var u=t.querySelector(\"#\"+s);if(u){Ee(l,u,r)}}}oe(f(t,\"[hx-swap-oob], [data-hx-swap-oob]\"),function(e){var \nt=te(e,\"hx-swap-oob\");if(t!=null){Ee(t,e,r)}})}function Re(e){oe(f(e,\"[hx-preserve], [data-hx-preserve]\"),function(e){var t=te(e,\"id\");var r=re().getElementById(t);if(r!=null){e.parentNode.replaceChild(r,e)}})}function Te(o,e,s){oe(e.querySelectorAll(\"[id]\"),function(e){var t=ee(e,\"id\");if(t&&t.length>0){var \nr=t.replace(\"'\",\"\\\\'\");var n=e.tagName.replace(\":\",\"\\\\:\");var i=o.querySelector(n+\"[id='\"+r+\"']\");if(i&&i!==o){var a=e.cloneNode();we(e,i);s.tasks.push(function(){we(e,a)})}}})}function Oe(e){return function(){n(e,Q.config.addedClass);zt(e);Nt(e);qe(e);ce(e,\"htmx:load\")}}function qe(e){var t=\"[autofocus]\";var \nr=h(e,t)?e:e.querySelector(t);if(r!=null){r.focus()}}function m(e,t,r,n){Te(e,r,n);while(r.childNodes.length>0){var i=r.firstChild;z(i,Q.config.addedClass);e.insertBefore(i,t);if(i.nodeType!==Node.TEXT_NODE&&i.nodeType!==Node.COMMENT_NODE){n.tasks.push(Oe(i))}}}function He(e,t){var \nr=0;while(r<e.length){t=(t<<5)-t+e.charCodeAt(r++)|0}return t}function Le(e){var t=0;if(e.attributes){for(var r=0;r<e.attributes.length;r++){var n=e.attributes[r];if(n.value){t=He(n.name,t);t=He(n.value,t)}}}return t}function Ae(e){var t=ae(e);if(t.onHandlers){for(var r=0;r<t.onHandlers.length;r++){const \nn=t.onHandlers[r];e.removeEventListener(n.event,n.listener)}delete t.onHandlers}}function Ne(e){var \nt=ae(e);if(t.timeout){clearTimeout(t.timeout)}if(t.webSocket){t.webSocket.close()}if(t.sseEventSource){t.sseEventSource.close()}if(t.listenerInfos){oe(t.listenerInfos,function(e){if(e.on){e.on.removeEventListener(e.trigger,e.listener)}})}Ae(e);oe(Object.keys(t),function(e){delete t[e]})}function \np(e){ce(e,\"htmx:beforeCleanupElement\");Ne(e);if(e.children){oe(e.children,function(e){p(e)})}}function Ie(t,e,r){if(t.tagName===\"BODY\"){return Ue(t,e,r)}else{var n;var i=t.previousSibling;m(u(t),t,e,r);if(i==null){n=u(t).firstChild}else{n=i.nextSibling}r.elts=r.elts.filter(function(e){return \ne!=t});while(n&&n!==t){if(n.nodeType===Node.ELEMENT_NODE){r.elts.push(n)}n=n.nextElementSibling}p(t);u(t).removeChild(t)}}function ke(e,t,r){return m(e,e.firstChild,t,r)}function Pe(e,t,r){return m(u(e),e,t,r)}function Me(e,t,r){return m(e,null,t,r)}function Xe(e,t,r){return m(u(e),e.nextSibling,t,r)}function \nDe(e,t,r){p(e);return u(e).removeChild(e)}function Ue(e,t,r){var n=e.firstChild;m(e,n,t,r);if(n){while(n.nextSibling){p(n.nextSibling);e.removeChild(n.nextSibling)}p(n);e.removeChild(n)}}function Be(e,t,r){var n=r||ne(e,\"hx-select\");if(n){var \ni=re().createDocumentFragment();oe(t.querySelectorAll(n),function(e){i.appendChild(e)});t=i}return t}function \nFe(e,t,r,n,i){switch(e){case\"none\":return;case\"outerHTML\":Ie(r,n,i);return;case\"afterbegin\":ke(r,n,i);return;case\"beforebegin\":Pe(r,n,i);return;case\"beforeend\":Me(r,n,i);return;case\"afterend\":Xe(r,n,i);return;case\"delete\":De(r,n,i);return;default:var a=Fr(t);for(var o=0;o<a.length;o++){var s=a[o];try{var \nl=s.handleSwap(e,r,n,i);if(l){if(typeof l.length!==\"undefined\"){for(var u=0;u<l.length;u++){var f=l[u];if(f.nodeType!==Node.TEXT_NODE&&f.nodeType!==Node.COMMENT_NODE){i.tasks.push(Oe(f))}}}return}}catch(e){b(e)}}if(e===\"innerHTML\"){Ue(r,n,i)}else{Fe(Q.config.defaultSwapStyle,t,r,n,i)}}}function \nVe(e){if(e.indexOf(\"<title\")>-1){var t=e.replace(H,\"\");var r=t.match(q);if(r){return r[2]}}}function je(e,t,r,n,i,a){i.title=Ve(n);var o=l(n);if(o){Ce(r,o,i);o=Be(r,o,a);Re(o);return Fe(e,r,t,o,i)}}function _e(e,t,r){var n=e.getResponseHeader(t);if(n.indexOf(\"{\")===0){var i=E(n);for(var a in \ni){if(i.hasOwnProperty(a)){var o=i[a];if(!P(o)){o={value:o}}ce(r,a,o)}}}else{var s=n.split(\",\");for(var l=0;l<s.length;l++){ce(r,s[l].trim(),[])}}}var ze=/\\s/;var x=/[\\s,]/;var $e=/[_$a-zA-Z]/;var We=/[_$a-zA-Z0-9]/;var Ge=['\"',\"'\",\"/\"];var Je=/[^\\s]/;var Ze=/[{(]/;var Ke=/[})]/;function Ye(e){var t=[];var \nr=0;while(r<e.length){if($e.exec(e.charAt(r))){var n=r;while(We.exec(e.charAt(r+1))){r++}t.push(e.substr(n,r-n+1))}else if(Ge.indexOf(e.charAt(r))!==-1){var i=e.charAt(r);var n=r;r++;while(r<e.length&&e.charAt(r)!==i){if(e.charAt(r)===\"\\\\\"){r++}r++}t.push(e.substr(n,r-n+1))}else{var a=e.charAt(r);t.push(a)}r++}return \nt}function Qe(e,t,r){return $e.exec(e.charAt(0))&&e!==\"true\"&&e!==\"false\"&&e!==\"this\"&&e!==r&&t!==\".\"}function et(e,t,r){if(t[0]===\"[\"){t.shift();var n=1;var i=\" return (function(\"+r+\"){ return (\";var a=null;while(t.length>0){var o=t[0];if(o===\"]\"){n--;if(n===0){if(a===null){i=i+\"true\"}t.shift();i+=\")})\";try{var \ns=Tr(e,function(){return Function(i)()},function(){return true});s.source=i;return s}catch(e){fe(re().body,\"htmx:syntax:error\",{error:e,source:i});return null}}}else if(o===\"[\"){n++}if(Qe(o,a,r)){i+=\"((\"+r+\".\"+o+\") ? (\"+r+\".\"+o+\") : (window.\"+o+\"))\"}else{i=i+o}a=t.shift()}}}function y(e,t){var \nr=\"\";while(e.length>0&&!t.test(e[0])){r+=e.shift()}return r}function tt(e){var t;if(e.length>0&&Ze.test(e[0])){e.shift();t=y(e,Ke).trim();e.shift()}else{t=y(e,x)}return t}var rt=\"input, textarea, select\";function nt(e,t,r){var n=[];var i=Ye(t);do{y(i,Je);var a=i.length;var \no=y(i,/[,\\[\\s]/);if(o!==\"\"){if(o===\"every\"){var s={trigger:\"every\"};y(i,Je);s.pollInterval=d(y(i,/[,\\[\\s]/));y(i,Je);var l=et(e,i,\"event\");if(l){s.eventFilter=l}n.push(s)}else if(o.indexOf(\"sse:\")===0){n.push({trigger:\"sse\",sseEvent:o.substr(4)})}else{var u={trigger:o};var \nl=et(e,i,\"event\");if(l){u.eventFilter=l}while(i.length>0&&i[0]!==\",\"){y(i,Je);var f=i.shift();if(f===\"changed\"){u.changed=true}else if(f===\"once\"){u.once=true}else if(f===\"consume\"){u.consume=true}else if(f===\"delay\"&&i[0]===\":\"){i.shift();u.delay=d(y(i,x))}else \nif(f===\"from\"&&i[0]===\":\"){i.shift();if(Ze.test(i[0])){var c=tt(i)}else{var c=y(i,x);if(c===\"closest\"||c===\"find\"||c===\"next\"||c===\"previous\"){i.shift();var h=tt(i);if(h.length>0){c+=\" \"+h}}}u.from=c}else if(f===\"target\"&&i[0]===\":\"){i.shift();u.target=tt(i)}else \nif(f===\"throttle\"&&i[0]===\":\"){i.shift();u.throttle=d(y(i,x))}else if(f===\"queue\"&&i[0]===\":\"){i.shift();u.queue=y(i,x)}else if(f===\"root\"&&i[0]===\":\"){i.shift();u[f]=tt(i)}else \nif(f===\"threshold\"&&i[0]===\":\"){i.shift();u[f]=y(i,x)}else{fe(e,\"htmx:syntax:error\",{token:i.shift()})}}n.push(u)}}if(i.length===a){fe(e,\"htmx:syntax:error\",{token:i.shift()})}y(i,Je)}while(i[0]===\",\"&&i.shift());if(r){r[t]=n}return n}function it(e){var t=te(e,\"hx-trigger\");var r=[];if(t){var \nn=Q.config.triggerSpecsCache;r=n&&n[t]||nt(e,t,n)}if(r.length>0){return r}else if(h(e,\"form\")){return[{trigger:\"submit\"}]}else if(h(e,'input[type=\"button\"], input[type=\"submit\"]')){return[{trigger:\"click\"}]}else if(h(e,rt)){return[{trigger:\"change\"}]}else{return[{trigger:\"click\"}]}}function \nat(e){ae(e).cancelled=true}function ot(e,t,r){var n=ae(e);n.timeout=setTimeout(function(){if(se(e)&&n.cancelled!==true){if(!ct(r,e,Wt(\"hx:poll:trigger\",{triggerSpec:r,target:e}))){t(e)}ot(e,t,r)}},r.pollInterval)}function st(e){return location.hostname===e.hostname&&ee(e,\"href\")&&ee(e,\"href\").indexOf(\"#\")!==0}function \nlt(t,r,e){if(t.tagName===\"A\"&&st(t)&&(t.target===\"\"||t.target===\"_self\")||t.tagName===\"FORM\"){r.boosted=true;var n,i;if(t.tagName===\"A\"){n=\"get\";i=ee(t,\"href\")}else{var \na=ee(t,\"method\");n=a?a.toLowerCase():\"get\";if(n===\"get\"){}i=ee(t,\"action\")}e.forEach(function(e){ht(t,function(e,t){if(v(e,Q.config.disableSelector)){p(e);return}he(n,i,e,t)},r,e,true)})}}function ut(e,t){if(e.type===\"submit\"||e.type===\"click\"){if(t.tagName===\"FORM\"){return true}if(h(t,'input[type=\"submit\"], \nbutton')&&v(t,\"form\")!==null){return true}if(t.tagName===\"A\"&&t.href&&(t.getAttribute(\"href\")===\"#\"||t.getAttribute(\"href\").indexOf(\"#\")!==0)){return true}}return false}function ft(e,t){return ae(e).boosted&&e.tagName===\"A\"&&t.type===\"click\"&&(t.ctrlKey||t.metaKey)}function ct(e,t,r){var \nn=e.eventFilter;if(n){try{return n.call(t,r)!==true}catch(e){fe(re().body,\"htmx:eventFilter:error\",{error:e,source:n.source});return true}}return false}function ht(a,o,e,s,l){var u=ae(a);var t;if(s.from){t=Z(a,s.from)}else{t=[a]}if(s.changed){t.forEach(function(e){var t=ae(e);t.lastValue=e.value})}oe(t,function(n){var \ni=function(e){if(!se(a)){n.removeEventListener(s.trigger,i);return}if(ft(a,e)){return}if(l||ut(e,a)){e.preventDefault()}if(ct(s,a,e)){return}var \nt=ae(e);t.triggerSpec=s;if(t.handledFor==null){t.handledFor=[]}if(t.handledFor.indexOf(a)<0){t.handledFor.push(a);if(s.consume){e.stopPropagation()}if(s.target&&e.target){if(!h(e.target,s.target)){return}}if(s.once){if(u.triggeredOnce){return}else{u.triggeredOnce=true}}if(s.changed){var \nr=ae(n);if(r.lastValue===n.value){return}r.lastValue=n.value}if(u.delayed){clearTimeout(u.delayed)}if(u.throttle){return}if(s.throttle>0){if(!u.throttle){o(a,e);u.throttle=setTimeout(function(){u.throttle=null},s.throttle)}}else \nif(s.delay>0){u.delayed=setTimeout(function(){o(a,e)},s.delay)}else{ce(a,\"htmx:trigger\");o(a,e)}}};if(e.listenerInfos==null){e.listenerInfos=[]}e.listenerInfos.push({trigger:s.trigger,listener:i,on:n});n.addEventListener(s.trigger,i)})}var vt=false;var dt=null;function \ngt(){if(!dt){dt=function(){vt=true};window.addEventListener(\"scroll\",dt);setInterval(function(){if(vt){vt=false;oe(re().querySelectorAll(\"[hx-trigger='revealed'],[data-hx-trigger='revealed']\"),function(e){mt(e)})}},200)}}function mt(t){if(!o(t,\"data-hx-revealed\")&&X(t)){t.setAttribute(\"data-hx-revealed\",\"true\");var \ne=ae(t);if(e.initHash){ce(t,\"revealed\")}else{t.addEventListener(\"htmx:afterProcessNode\",function(e){ce(t,\"revealed\")},{once:true})}}}function pt(e,t,r){var n=D(r);for(var i=0;i<n.length;i++){var a=n[i].split(/:(.+)/);if(a[0]===\"connect\"){xt(e,a[1],0)}if(a[0]===\"send\"){bt(e)}}}function \nxt(s,r,n){if(!se(s)){return}if(r.indexOf(\"/\")==0){var e=location.hostname+(location.port?\":\"+location.port:\"\");if(location.protocol==\"https:\"){r=\"wss://\"+e+r}else if(location.protocol==\"http:\"){r=\"ws://\"+e+r}}var \nt=Q.createWebSocket(r);t.onerror=function(e){fe(s,\"htmx:wsError\",{error:e,socket:t});yt(s)};t.onclose=function(e){if([1006,1012,1013].indexOf(e.code)>=0){var t=wt(n);setTimeout(function(){xt(s,r,n+1)},t)}};t.onopen=function(e){n=0};ae(s).webSocket=t;t.addEventListener(\"message\",function(e){if(yt(s)){return}var \nt=e.data;R(s,function(e){t=e.transformResponse(t,null,s)});var r=T(s);var n=l(t);var i=M(n.children);for(var a=0;a<i.length;a++){var o=i[a];Ee(te(o,\"hx-swap-oob\")||\"true\",o,r)}nr(r.tasks)})}function yt(e){if(!se(e)){ae(e).webSocket.close();return true}}function bt(u){var f=c(u,function(e){return \nae(e).webSocket!=null});if(f){u.addEventListener(it(u)[0].trigger,function(e){var t=ae(f).webSocket;var r=xr(u,f);var n=dr(u,\"post\");var i=n.errors;var a=n.values;var o=Hr(u);var s=le(a,o);var \nl=yr(s,u);l[\"HEADERS\"]=r;if(i&&i.length>0){ce(u,\"htmx:validation:halted\",i);return}t.send(JSON.stringify(l));if(ut(e,u)){e.preventDefault()}})}else{fe(u,\"htmx:noWebSocketSourceError\")}}function wt(e){var t=Q.config.wsReconnectDelay;if(typeof t===\"function\"){return t(e)}if(t===\"full-jitter\"){var r=Math.min(e,6);var \nn=1e3*Math.pow(2,r);return n*Math.random()}b('htmx.config.wsReconnectDelay must either be a function or the string \"full-jitter\"')}function St(e,t,r){var n=D(r);for(var i=0;i<n.length;i++){var a=n[i].split(/:(.+)/);if(a[0]===\"connect\"){Et(e,a[1])}if(a[0]===\"swap\"){Ct(e,a[1])}}}function Et(t,e){var \nr=Q.createEventSource(e);r.onerror=function(e){fe(t,\"htmx:sseError\",{error:e,source:r});Tt(t)};ae(t).sseEventSource=r}function Ct(a,o){var s=c(a,Ot);if(s){var l=ae(s).sseEventSource;var u=function(e){if(Tt(s)){return}if(!se(a)){l.removeEventListener(o,u);return}var \nt=e.data;R(a,function(e){t=e.transformResponse(t,null,a)});var r=wr(a);var n=ye(a);var i=T(a);je(r.swapStyle,n,a,t,i);nr(i.tasks);ce(a,\"htmx:sseMessage\",e)};ae(a).sseListener=u;l.addEventListener(o,u)}else{fe(a,\"htmx:noSSESourceError\")}}function Rt(e,t,r){var n=c(e,Ot);if(n){var i=ae(n).sseEventSource;var \na=function(){if(!Tt(n)){if(se(e)){t(e)}else{i.removeEventListener(r,a)}}};ae(e).sseListener=a;i.addEventListener(r,a)}else{fe(e,\"htmx:noSSESourceError\")}}function Tt(e){if(!se(e)){ae(e).sseEventSource.close();return true}}function Ot(e){return ae(e).sseEventSource!=null}function qt(e,t,r,n){var \ni=function(){if(!r.loaded){r.loaded=true;t(e)}};if(n>0){setTimeout(i,n)}else{i()}}function Ht(t,i,e){var a=false;oe(w,function(r){if(o(t,\"hx-\"+r)){var n=te(t,\"hx-\"+r);a=true;i.path=n;i.verb=r;e.forEach(function(e){Lt(t,e,i,function(e,t){if(v(e,Q.config.disableSelector)){p(e);return}he(r,n,e,t)})})}});return a}function \nLt(n,e,t,r){if(e.sseEvent){Rt(n,r,e.sseEvent)}else if(e.trigger===\"revealed\"){gt();ht(n,r,t,e);mt(n)}else if(e.trigger===\"intersect\"){var i={};if(e.root){i.root=ue(n,e.root)}if(e.threshold){i.threshold=parseFloat(e.threshold)}var a=new IntersectionObserver(function(e){for(var t=0;t<e.length;t++){var \nr=e[t];if(r.isIntersecting){ce(n,\"intersect\");break}}},i);a.observe(n);ht(n,r,t,e)}else if(e.trigger===\"load\"){if(!ct(e,n,Wt(\"load\",{elt:n}))){qt(n,r,t,e.delay)}}else if(e.pollInterval>0){t.polling=true;ot(n,r,e)}else{ht(n,r,t,e)}}function \nAt(e){if(Q.config.allowScriptTags&&(e.type===\"text/javascript\"||e.type===\"module\"||e.type===\"\")){var t=re().createElement(\"script\");oe(e.attributes,function(e){t.setAttribute(e.name,e.value)});t.textContent=e.textContent;t.async=false;if(Q.config.inlineScriptNonce){t.nonce=Q.config.inlineScriptNonce}var \nr=e.parentElement;try{r.insertBefore(t,e)}catch(e){b(e)}finally{if(e.parentElement){e.parentElement.removeChild(e)}}}}function Nt(e){if(h(e,\"script\")){At(e)}oe(f(e,\"script\"),function(e){At(e)})}function It(e){var t=e.attributes;for(var r=0;r<t.length;r++){var \nn=t[r].name;if(s(n,\"hx-on:\")||s(n,\"data-hx-on:\")||s(n,\"hx-on-\")||s(n,\"data-hx-on-\")){return true}}return false}function kt(e){var t=null;var r=[];if(It(e)){r.push(e)}if(document.evaluate){var n=document.evaluate('.//*[@*[ starts-with(name(), \"hx-on:\") or starts-with(name(), \"data-hx-on:\") or'+' starts-with(name(), \n\"hx-on-\") or starts-with(name(), \"data-hx-on-\") ]]',e);while(t=n.iterateNext())r.push(t)}else{var i=e.getElementsByTagName(\"*\");for(var a=0;a<i.length;a++){if(It(i[a])){r.push(i[a])}}}return r}function Pt(e){if(e.querySelectorAll){var t=\", [hx-boost] a, [data-hx-boost] a, a[hx-boost], a[data-hx-boost]\";var \nr=e.querySelectorAll(i+t+\", form, [type='submit'], [hx-sse], [data-hx-sse], [hx-ws],\"+\" [data-hx-ws], [hx-ext], [data-hx-ext], [hx-trigger], [data-hx-trigger], [hx-on], [data-hx-on]\");return r}else{return[]}}function Mt(e){var t=v(e.target,\"button, input[type='submit']\");var \n\nr=Dt(e);if(r){r.lastButtonClicked=t}}function Xt(e){var t=Dt(e);if(t){t.lastButtonClicked=null}}function Dt(e){var t=v(e.target,\"button, input[type='submit']\");if(!t){return}var r=g(\"#\"+ee(t,\"form\"))||v(t,\"form\");if(!r){return}return ae(r)}function \nUt(e){e.addEventListener(\"click\",Mt);e.addEventListener(\"focusin\",Mt);e.addEventListener(\"focusout\",Xt)}function Bt(e){var t=Ye(e);var r=0;for(var n=0;n<t.length;n++){const i=t[n];if(i===\"{\"){r++}else if(i===\"}\"){r--}}return r}function Ft(t,e,r){var n=ae(t);if(!Array.isArray(n.onHandlers)){n.onHandlers=[]}var i;var \na=function(e){return Tr(t,function(){if(!i){i=new Function(\"event\",r)}i.call(t,e)})};t.addEventListener(e,a);n.onHandlers.push({event:e,listener:a})}function Vt(e){var t=te(e,\"hx-on\");if(t){var r={};var n=t.split(\"\\n\");var i=null;var a=0;while(n.length>0){var o=n.shift();var \ns=o.match(/^\\s*([a-zA-Z:\\-\\.]+:)(.*)/);if(a===0&&s){o.split(\":\");i=s[1].slice(0,-1);r[i]=s[2]}else{r[i]+=o}a+=Bt(o)}for(var l in r){Ft(e,l,r[l])}}}function jt(e){Ae(e);for(var t=0;t<e.attributes.length;t++){var r=e.attributes[t].name;var n=e.attributes[t].value;if(s(r,\"hx-on\")||s(r,\"data-hx-on\")){var \ni=r.indexOf(\"-on\")+3;var a=r.slice(i,i+1);if(a===\"-\"||a===\":\"){var o=r.slice(i+1);if(s(o,\":\")){o=\"htmx\"+o}else if(s(o,\"-\")){o=\"htmx:\"+o.slice(1)}else if(s(o,\"htmx-\")){o=\"htmx:\"+o.slice(5)}Ft(e,o,n)}}}}function _t(t){if(v(t,Q.config.disableSelector)){p(t);return}var \nr=ae(t);if(r.initHash!==Le(t)){Ne(t);r.initHash=Le(t);Vt(t);ce(t,\"htmx:beforeProcessNode\");if(t.value){r.lastValue=t.value}var e=it(t);var n=Ht(t,r,e);if(!n){if(ne(t,\"hx-boost\")===\"true\"){lt(t,r,e)}else \nif(o(t,\"hx-trigger\")){e.forEach(function(e){Lt(t,e,r,function(){})})}}if(t.tagName===\"FORM\"||ee(t,\"type\")===\"submit\"&&o(t,\"form\")){Ut(t)}var i=te(t,\"hx-sse\");if(i){St(t,r,i)}var a=te(t,\"hx-ws\");if(a){pt(t,r,a)}ce(t,\"htmx:afterProcessNode\")}}function \nzt(e){e=g(e);if(v(e,Q.config.disableSelector)){p(e);return}_t(e);oe(Pt(e),function(e){_t(e)});oe(kt(e),jt)}function $t(e){return e.replace(/([a-z0-9])([A-Z])/g,\"$1-$2\").toLowerCase()}function Wt(e,t){var r;if(window.CustomEvent&&typeof window.CustomEvent===\"function\"){r=new \nCustomEvent(e,{bubbles:true,cancelable:true,detail:t})}else{r=re().createEvent(\"CustomEvent\");r.initCustomEvent(e,true,true,t)}return r}function fe(e,t,r){ce(e,t,le({error:t},r))}function Gt(e){return e===\"htmx:afterProcessNode\"}function R(e,t){oe(Fr(e),function(e){try{t(e)}catch(e){b(e)}})}function \nb(e){if(console.error){console.error(e)}else if(console.log){console.log(\"ERROR: \",e)}}function ce(e,t,r){e=g(e);if(r==null){r={}}r[\"elt\"]=e;var n=Wt(t,r);if(Q.logger&&!Gt(t)){Q.logger(e,t,r)}if(r.error){b(r.error);ce(e,\"htmx:error\",{errorInfo:r})}var i=e.dispatchEvent(n);var a=$t(t);if(i&&a!==t){var \no=Wt(a,n.detail);i=i&&e.dispatchEvent(o)}R(e,function(e){i=i&&(e.onEvent(t,n)!==false&&!n.defaultPrevented)});return i}var Jt=location.pathname+location.search;function Zt(){var e=re().querySelector(\"[hx-history-elt],[data-hx-history-elt]\");return e||re().body}function \nKt(e,t,r,n){if(!U()){return}if(Q.config.historyCacheSize<=0){localStorage.removeItem(\"htmx-history-cache\");return}e=B(e);var i=E(localStorage.getItem(\"htmx-history-cache\"))||[];for(var a=0;a<i.length;a++){if(i[a].url===e){i.splice(a,1);break}}var \no={url:e,content:t,title:r,scroll:n};ce(re().body,\"htmx:historyItemCreated\",{item:o,cache:i});i.push(o);while(i.length>Q.config.historyCacheSize){i.shift()}while(i.length>0){try{localStorage.setItem(\"htmx-history-cache\",JSON.stringify(i));break}catch(e){fe(re().body,\"htmx:historyCacheError\",{cause:e,cache:i});i.shift(\n)}}}function Yt(e){if(!U()){return null}e=B(e);var t=E(localStorage.getItem(\"htmx-history-cache\"))||[];for(var r=0;r<t.length;r++){if(t[r].url===e){return t[r]}}return null}function Qt(e){var t=Q.config.requestClass;var r=e.cloneNode(true);oe(f(r,\".\"+t),function(e){n(e,t)});return r.innerHTML}function er(){var \ne=Zt();var t=Jt||location.pathname+location.search;var r;try{r=re().querySelector('[hx-history=\"false\" i],[data-hx-history=\"false\" \ni]')}catch(e){r=re().querySelector('[hx-history=\"false\"],[data-hx-history=\"false\"]')}if(!r){ce(re().body,\"htmx:beforeHistorySave\",{path:t,historyElt:e});Kt(t,Qt(e),re().title,window.scrollY)}if(Q.config.historyEnabled)history.replaceState({htmx:true},re().title,window.location.href)}function \ntr(e){if(Q.config.getCacheBusterParam){e=e.replace(/org\\.htmx\\.cache-buster=[^&]*&?/,\"\");if(G(e,\"&\")||G(e,\"?\")){e=e.slice(0,-1)}}if(Q.config.historyEnabled){history.pushState({htmx:true},\"\",e)}Jt=e}function rr(e){if(Q.config.historyEnabled)history.replaceState({htmx:true},\"\",e);Jt=e}function \nnr(e){oe(e,function(e){e.call()})}function ir(a){var e=new XMLHttpRequest;var \no={path:a,xhr:e};ce(re().body,\"htmx:historyCacheMiss\",o);e.open(\"GET\",a,true);e.setRequestHeader(\"HX-Request\",\"true\");e.setRequestHeader(\"HX-History-Restore-Request\",\"true\");e.setRequestHeader(\"HX-Current-URL\",re().location.href);e.onload=function(){if(this.status>=200&&this.status<400){ce(re().body,\"htmx:historyCache\nMissLoad\",o);var e=l(this.response);e=e.querySelector(\"[hx-history-elt],[data-hx-history-elt]\")||e;var t=Zt();var r=T(t);var n=Ve(this.response);if(n){var \ni=C(\"title\");if(i){i.innerHTML=n}else{window.document.title=n}}Ue(t,e,r);nr(r.tasks);Jt=a;ce(re().body,\"htmx:historyRestore\",{path:a,cacheMiss:true,serverResponse:this.response})}else{fe(re().body,\"htmx:historyCacheMissLoadError\",o)}};e.send()}function ar(e){er();e=e||location.pathname+location.search;var \nt=Yt(e);if(t){var r=l(t.content);var n=Zt();var i=T(n);Ue(n,r,i);nr(i.tasks);document.title=t.title;setTimeout(function(){window.scrollTo(0,t.scroll)},0);Jt=e;ce(re().body,\"htmx:historyRestore\",{path:e,item:t})}else{if(Q.config.refreshOnHistoryMiss){window.location.reload(true)}else{ir(e)}}}function or(e){var \nt=pe(e,\"hx-indicator\");if(t==null){t=[e]}oe(t,function(e){var t=ae(e);t.requestCount=(t.requestCount||0)+1;e.classList[\"add\"].call(e.classList,Q.config.requestClass)});return t}function sr(e){var t=pe(e,\"hx-disabled-elt\");if(t==null){t=[]}oe(t,function(e){var \nt=ae(e);t.requestCount=(t.requestCount||0)+1;e.setAttribute(\"disabled\",\"\")});return t}function lr(e,t){oe(e,function(e){var t=ae(e);t.requestCount=(t.requestCount||0)-1;if(t.requestCount===0){e.classList[\"remove\"].call(e.classList,Q.config.requestClass)}});oe(t,function(e){var \nt=ae(e);t.requestCount=(t.requestCount||0)-1;if(t.requestCount===0){e.removeAttribute(\"disabled\")}})}function ur(e,t){for(var r=0;r<e.length;r++){var n=e[r];if(n.isSameNode(t)){return true}}return false}function fr(e){if(e.name===\"\"||e.name==null||e.disabled||v(e,\"fieldset[disabled]\")){return \nfalse}if(e.type===\"button\"||e.type===\"submit\"||e.tagName===\"image\"||e.tagName===\"reset\"||e.tagName===\"file\"){return false}if(e.type===\"checkbox\"||e.type===\"radio\"){return e.checked}return true}function cr(e,t,r){if(e!=null&&t!=null){var n=r[e];if(n===undefined){r[e]=t}else \nif(Array.isArray(n)){if(Array.isArray(t)){r[e]=n.concat(t)}else{n.push(t)}}else{if(Array.isArray(t)){r[e]=[n].concat(t)}else{r[e]=[n,t]}}}}function hr(t,r,n,e,i){if(e==null||ur(t,e)){return}else{t.push(e)}if(fr(e)){var a=ee(e,\"name\");var \no=e.value;if(e.multiple&&e.tagName===\"SELECT\"){o=M(e.querySelectorAll(\"option:checked\")).map(function(e){return e.value})}if(e.files){o=M(e.files)}cr(a,o,r);if(i){vr(e,n)}}if(h(e,\"form\")){var s=e.elements;oe(s,function(e){hr(t,r,n,e,i)})}}function \nvr(e,t){if(e.willValidate){ce(e,\"htmx:validation:validate\");if(!e.checkValidity()){t.push({elt:e,message:e.validationMessage,validity:e.validity});ce(e,\"htmx:validation:failed\",{message:e.validationMessage,validity:e.validity})}}}function dr(e,t){var r=[];var n={};var i={};var a=[];var \no=ae(e);if(o.lastButtonClicked&&!se(o.lastButtonClicked)){o.lastButtonClicked=null}var \ns=h(e,\"form\")&&e.noValidate!==true||te(e,\"hx-validate\")===\"true\";if(o.lastButtonClicked){s=s&&o.lastButtonClicked.formNoValidate!==true}if(t!==\"get\"){hr(r,i,a,v(e,\"form\"),s)}hr(r,n,a,e,s);if(o.lastButtonClicked||e.tagName===\"BUTTON\"||e.tagName===\"INPUT\"&&ee(e,\"type\")===\"submit\"){var l=o.lastButtonClicked||e;var \nu=ee(l,\"name\");cr(u,l.value,i)}var f=pe(e,\"hx-include\");oe(f,function(e){hr(r,n,a,e,s);if(!h(e,\"form\")){oe(e.querySelectorAll(rt),function(e){hr(r,n,a,e,s)})}});n=le(n,i);return{errors:a,values:n}}function gr(e,t,r){if(e!==\"\"){e+=\"&\"}if(String(r)===\"[object Object]\"){r=JSON.stringify(r)}var \nn=encodeURIComponent(r);e+=encodeURIComponent(t)+\"=\"+n;return e}function mr(e){var t=\"\";for(var r in e){if(e.hasOwnProperty(r)){var n=e[r];if(Array.isArray(n)){oe(n,function(e){t=gr(t,r,e)})}else{t=gr(t,r,n)}}}return t}function pr(e){var t=new FormData;for(var r in e){if(e.hasOwnProperty(r)){var \nn=e[r];if(Array.isArray(n)){oe(n,function(e){t.append(r,e)})}else{t.append(r,n)}}}return t}function xr(e,t,r){var \nn={\"HX-Request\":\"true\",\"HX-Trigger\":ee(e,\"id\"),\"HX-Trigger-Name\":ee(e,\"name\"),\"HX-Target\":te(t,\"id\"),\"HX-Current-URL\":re().location.href};Rr(e,\"hx-headers\",false,n);if(r!==undefined){n[\"HX-Prompt\"]=r}if(ae(e).boosted){n[\"HX-Boosted\"]=\"true\"}return n}function yr(t,e){var \nr=ne(e,\"hx-params\");if(r){if(r===\"none\"){return{}}else if(r===\"*\"){return t}else if(r.indexOf(\"not \")===0){oe(r.substr(4).split(\",\"),function(e){e=e.trim();delete t[e]});return t}else{var n={};oe(r.split(\",\"),function(e){e=e.trim();n[e]=t[e]});return n}}else{return t}}function br(e){return \nee(e,\"href\")&&ee(e,\"href\").indexOf(\"#\")>=0}function wr(e,t){var r=t?t:ne(e,\"hx-swap\");var n={swapStyle:ae(e).boosted?\"innerHTML\":Q.config.defaultSwapStyle,swapDelay:Q.config.defaultSwapDelay,settleDelay:Q.config.defaultSettleDelay};if(Q.config.scrollIntoViewOnBoost&&ae(e).boosted&&!br(e)){n[\"show\"]=\"top\"}if(r){var \ni=D(r);if(i.length>0){for(var a=0;a<i.length;a++){var o=i[a];if(o.indexOf(\"swap:\")===0){n[\"swapDelay\"]=d(o.substr(5))}else if(o.indexOf(\"settle:\")===0){n[\"settleDelay\"]=d(o.substr(7))}else if(o.indexOf(\"transition:\")===0){n[\"transition\"]=o.substr(11)===\"true\"}else \nif(o.indexOf(\"ignoreTitle:\")===0){n[\"ignoreTitle\"]=o.substr(12)===\"true\"}else if(o.indexOf(\"scroll:\")===0){var s=o.substr(7);var l=s.split(\":\");var u=l.pop();var f=l.length>0?l.join(\":\"):null;n[\"scroll\"]=u;n[\"scrollTarget\"]=f}else if(o.indexOf(\"show:\")===0){var c=o.substr(5);var l=c.split(\":\");var h=l.pop();var \nf=l.length>0?l.join(\":\"):null;n[\"show\"]=h;n[\"showTarget\"]=f}else if(o.indexOf(\"focus-scroll:\")===0){var v=o.substr(\"focus-scroll:\".length);n[\"focusScroll\"]=v==\"true\"}else if(a==0){n[\"swapStyle\"]=o}else{b(\"Unknown modifier in hx-swap: \"+o)}}}}return n}function Sr(e){return \nne(e,\"hx-encoding\")===\"multipart/form-data\"||h(e,\"form\")&&ee(e,\"enctype\")===\"multipart/form-data\"}function Er(t,r,n){var i=null;R(r,function(e){if(i==null){i=e.encodeParameters(t,n,r)}});if(i!=null){return i}else{if(Sr(r)){return pr(n)}else{return mr(n)}}}function T(e){return{tasks:[],elts:[e]}}function Cr(e,t){var \nr=e[0];var n=e[e.length-1];if(t.scroll){var i=null;if(t.scrollTarget){i=ue(r,t.scrollTarget)}if(t.scroll===\"top\"&&(r||i)){i=i||r;i.scrollTop=0}if(t.scroll===\"bottom\"&&(n||i)){i=i||n;i.scrollTop=i.scrollHeight}}if(t.show){var i=null;if(t.showTarget){var \na=t.showTarget;if(t.showTarget===\"window\"){a=\"body\"}i=ue(r,a)}if(t.show===\"top\"&&(r||i)){i=i||r;i.scrollIntoView({block:\"start\",behavior:Q.config.scrollBehavior})}if(t.show===\"bottom\"&&(n||i)){i=i||n;i.scrollIntoView({block:\"end\",behavior:Q.config.scrollBehavior})}}}function \nRr(e,t,r,n){if(n==null){n={}}if(e==null){return n}var i=te(e,t);if(i){var a=i.trim();var o=r;if(a===\"unset\"){return null}if(a.indexOf(\"javascript:\")===0){a=a.substr(11);o=true}else if(a.indexOf(\"js:\")===0){a=a.substr(3);o=true}if(a.indexOf(\"{\")!==0){a=\"{\"+a+\"}\"}var s;if(o){s=Tr(e,function(){return Function(\"return \n(\"+a+\")\")()},{})}else{s=E(a)}for(var l in s){if(s.hasOwnProperty(l)){if(n[l]==null){n[l]=s[l]}}}}return Rr(u(e),t,r,n)}function Tr(e,t,r){if(Q.config.allowEval){return t()}else{fe(e,\"htmx:evalDisallowedError\");return r}}function Or(e,t){return Rr(e,\"hx-vars\",true,t)}function qr(e,t){return \nRr(e,\"hx-vals\",false,t)}function Hr(e){return le(Or(e),qr(e))}function Lr(t,r,n){if(n!==null){try{t.setRequestHeader(r,n)}catch(e){t.setRequestHeader(r,encodeURIComponent(n));t.setRequestHeader(r+\"-URI-AutoEncoded\",\"true\")}}}function Ar(t){if(t.responseURL&&typeof URL!==\"undefined\"){try{var e=new \nURL(t.responseURL);return e.pathname+e.search}catch(e){fe(re().body,\"htmx:badResponseUrl\",{url:t.responseURL})}}}function O(e,t){return t.test(e.getAllResponseHeaders())}function Nr(e,t,r){e=e.toLowerCase();if(r){if(r instanceof Element||I(r,\"String\")){return \nhe(e,t,null,null,{targetOverride:g(r),returnPromise:true})}else{return he(e,t,g(r.source),r.event,{handler:r.handler,headers:r.headers,values:r.values,targetOverride:g(r.target),swapOverride:r.swap,select:r.select,returnPromise:true})}}else{return he(e,t,null,null,{returnPromise:true})}}function Ir(e){var \nt=[];while(e){t.push(e);e=e.parentElement}return t}function kr(e,t,r){var n;var i;if(typeof URL===\"function\"){i=new URL(t,document.location.href);var a=document.location.origin;n=a===i.origin}else{i=t;n=s(t,document.location.origin)}if(Q.config.selfRequestsOnly){if(!n){return false}}return \nce(e,\"htmx:validateUrl\",le({url:i,sameHost:n},r))}function he(t,r,n,i,a,e){var o=null;var s=null;a=a!=null?a:{};if(a.returnPromise&&typeof Promise!==\"undefined\"){var l=new Promise(function(e,t){o=e;s=t})}if(n==null){n=re().body}var M=a.handler||Mr;var X=a.select||null;if(!se(n)){ie(o);return l}var \nu=a.targetOverride||ye(n);if(u==null||u==me){fe(n,\"htmx:targetError\",{target:te(n,\"hx-target\")});ie(s);return l}var f=ae(n);var c=f.lastButtonClicked;if(c){var h=ee(c,\"formaction\");if(h!=null){r=h}var v=ee(c,\"formmethod\");if(v!=null){if(v.toLowerCase()!==\"dialog\"){t=v}}}var d=ne(n,\"hx-confirm\");if(e===undefined){var \nD=function(e){return he(t,r,n,i,a,!!e)};var U={target:u,elt:n,path:r,verb:t,triggeringEvent:i,etc:a,issueRequest:D,question:d};if(ce(n,\"htmx:confirm\",U)===false){ie(o);return l}}var g=n;var m=ne(n,\"hx-sync\");var p=null;var x=false;if(m){var B=m.split(\":\");var \nF=B[0].trim();if(F===\"this\"){g=xe(n,\"hx-sync\")}else{g=ue(n,F)}m=(B[1]||\"drop\").trim();f=ae(g);if(m===\"drop\"&&f.xhr&&f.abortable!==true){ie(o);return l}else if(m===\"abort\"){if(f.xhr){ie(o);return l}else{x=true}}else if(m===\"replace\"){ce(g,\"htmx:abort\")}else if(m.indexOf(\"queue\")===0){var V=m.split(\" \n\");p=(V[1]||\"last\").trim()}}if(f.xhr){if(f.abortable){ce(g,\"htmx:abort\")}else{if(p==null){if(i){var \ny=ae(i);if(y&&y.triggerSpec&&y.triggerSpec.queue){p=y.triggerSpec.queue}}if(p==null){p=\"last\"}}if(f.queuedRequests==null){f.queuedRequests=[]}if(p===\"first\"&&f.queuedRequests.length===0){f.queuedRequests.push(function(){he(t,r,n,i,a)})}else if(p===\"all\"){f.queuedRequests.push(function(){he(t,r,n,i,a)})}else \nif(p===\"last\"){f.queuedRequests=[];f.queuedRequests.push(function(){he(t,r,n,i,a)})}ie(o);return l}}var b=new XMLHttpRequest;f.xhr=b;f.abortable=x;var w=function(){f.xhr=null;f.abortable=false;if(f.queuedRequests!=null&&f.queuedRequests.length>0){var e=f.queuedRequests.shift();e()}};var j=ne(n,\"hx-prompt\");if(j){var \nS=prompt(j);if(S===null||!ce(n,\"htmx:prompt\",{prompt:S,target:u})){ie(o);w();return l}}if(d&&!e){if(!confirm(d)){ie(o);w();return l}}var E=xr(n,u,S);if(t!==\"get\"&&!Sr(n)){E[\"Content-Type\"]=\"application/x-www-form-urlencoded\"}if(a.headers){E=le(E,a.headers)}var _=dr(n,t);var C=_.errors;var \nR=_.values;if(a.values){R=le(R,a.values)}var z=Hr(n);var $=le(R,z);var T=yr($,n);if(Q.config.getCacheBusterParam&&t===\"get\"){T[\"org.htmx.cache-buster\"]=ee(u,\"id\")||\"true\"}if(r==null||r===\"\"){r=re().location.href}var O=Rr(n,\"hx-request\");var W=ae(n).boosted;var q=Q.config.methodsThatUseUrlParams.indexOf(t)>=0;var \nH={boosted:W,useUrlParams:q,parameters:T,unfilteredParameters:$,headers:E,target:u,verb:t,errors:C,withCredentials:a.credentials||O.credentials||Q.config.withCredentials,timeout:a.timeout||O.timeout||Q.config.timeout,path:r,triggeringEvent:i};if(!ce(n,\"htmx:configRequest\",H)){ie(o);w();return \nl}r=H.path;t=H.verb;E=H.headers;T=H.parameters;C=H.errors;q=H.useUrlParams;if(C&&C.length>0){ce(n,\"htmx:validation:halted\",H);ie(o);w();return l}var G=r.split(\"#\");var J=G[0];var L=G[1];var A=r;if(q){A=J;var \nZ=Object.keys(T).length!==0;if(Z){if(A.indexOf(\"?\")<0){A+=\"?\"}else{A+=\"&\"}A+=mr(T);if(L){A+=\"#\"+L}}}if(!kr(n,A,H)){fe(n,\"htmx:invalidPath\",H);ie(s);return l}b.open(t.toUpperCase(),A,true);b.overrideMimeType(\"text/html\");b.withCredentials=H.withCredentials;b.timeout=H.timeout;if(O.noHeaders){}else{for(var N in \nE){if(E.hasOwnProperty(N)){var K=E[N];Lr(b,N,K)}}}var I={xhr:b,target:u,requestConfig:H,etc:a,boosted:W,select:X,pathInfo:{requestPath:r,finalRequestPath:A,anchor:L}};b.onload=function(){try{var e=Ir(n);I.pathInfo.responsePath=Ar(b);M(n,I);lr(k,P);ce(n,\"htmx:afterRequest\",I);ce(n,\"htmx:afterOnLoad\",I);if(!se(n)){var \nt=null;while(e.length>0&&t==null){var r=e.shift();if(se(r)){t=r}}if(t){ce(t,\"htmx:afterRequest\",I);ce(t,\"htmx:afterOnLoad\",I)}}ie(o);w()}catch(e){fe(n,\"htmx:onLoadError\",le({error:e},I));throw \ne}};b.onerror=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:sendError\",I);ie(s);w()};b.onabort=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:sendAbort\",I);ie(s);w()};b.ontimeout=function(){lr(k,P);fe(n,\"htmx:afterRequest\",I);fe(n,\"htmx:timeout\",I);ie(s);w()};if(!ce(n,\"htmx:beforeRequest\",I)){ie\n(o);w();return l}var k=or(n);var P=sr(n);oe([\"loadstart\",\"loadend\",\"progress\",\"abort\"],function(t){oe([b,b.upload],function(e){e.addEventListener(t,function(e){ce(n,\"htmx:xhr:\"+t,{lengthComputable:e.lengthComputable,loaded:e.loaded,total:e.total})})})});ce(n,\"htmx:beforeSend\",I);var Y=q?null:Er(b,n,T);b.send(Y);return \nl}function Pr(e,t){var r=t.xhr;var n=null;var i=null;if(O(r,/HX-Push:/i)){n=r.getResponseHeader(\"HX-Push\");i=\"push\"}else if(O(r,/HX-Push-Url:/i)){n=r.getResponseHeader(\"HX-Push-Url\");i=\"push\"}else \nif(O(r,/HX-Replace-Url:/i)){n=r.getResponseHeader(\"HX-Replace-Url\");i=\"replace\"}if(n){if(n===\"false\"){return{}}else{return{type:i,path:n}}}var a=t.pathInfo.finalRequestPath;var o=t.pathInfo.responsePath;var s=ne(e,\"hx-push-url\");var l=ne(e,\"hx-replace-url\");var u=ae(e).boosted;var f=null;var \nc=null;if(s){f=\"push\";c=s}else if(l){f=\"replace\";c=l}else if(u){f=\"push\";c=o||a}if(c){if(c===\"false\"){return{}}if(c===\"true\"){c=o||a}if(t.pathInfo.anchor&&c.indexOf(\"#\")===-1){c=c+\"#\"+t.pathInfo.anchor}return{type:f,path:c}}else{return{}}}function Mr(l,u){var f=u.xhr;var c=u.target;var e=u.etc;var \nt=u.requestConfig;var h=u.select;if(!ce(l,\"htmx:beforeOnLoad\",u))return;if(O(f,/HX-Trigger:/i)){_e(f,\"HX-Trigger\",l)}if(O(f,/HX-Location:/i)){er();var r=f.getResponseHeader(\"HX-Location\");var v;if(r.indexOf(\"{\")===0){v=E(r);r=v[\"path\"];delete v[\"path\"]}Nr(\"GET\",r,v).then(function(){tr(r)});return}var \nn=O(f,/HX-Refresh:/i)&&\"true\"===f.getResponseHeader(\"HX-Refresh\");if(O(f,/HX-Redirect:/i)){location.href=f.getResponseHeader(\"HX-Redirect\");n&&location.reload();return}if(n){location.reload();return}if(O(f,/HX-Retarget:/i)){if(f.getResponseHeader(\"HX-Retarget\")===\"this\"){u.target=l}else{u.target=ue(l,f.getResponseHead\ner(\"HX-Retarget\"))}}var d=Pr(l,u);var i=f.status>=200&&f.status<400&&f.status!==204;var g=f.response;var a=f.status>=400;var m=Q.config.ignoreTitle;var \no=le({shouldSwap:i,serverResponse:g,isError:a,ignoreTitle:m},u);if(!ce(c,\"htmx:beforeSwap\",o))return;c=o.target;g=o.serverResponse;a=o.isError;m=o.ignoreTitle;u.target=c;u.failed=a;u.successful=!a;if(o.shouldSwap){if(f.status===286){at(l)}R(l,function(e){g=e.transformResponse(g,f,l)});if(d.type){er()}var \ns=e.swapOverride;if(O(f,/HX-Reswap:/i)){s=f.getResponseHeader(\"HX-Reswap\")}var v=wr(l,s);if(v.hasOwnProperty(\"ignoreTitle\")){m=v.ignoreTitle}c.classList.add(Q.config.swappingClass);var p=null;var x=null;var y=function(){try{var e=document.activeElement;var \nt={};try{t={elt:e,start:e?e.selectionStart:null,end:e?e.selectionEnd:null}}catch(e){}var \nr;if(h){r=h}if(O(f,/HX-Reselect:/i)){r=f.getResponseHeader(\"HX-Reselect\")}if(d.type){ce(re().body,\"htmx:beforeHistoryUpdate\",le({history:d},u));if(d.type===\"push\"){tr(d.path);ce(re().body,\"htmx:pushedIntoHistory\",{path:d.path})}else{rr(d.path);ce(re().body,\"htmx:replacedInHistory\",{path:d.path})}}var \nn=T(c);je(v.swapStyle,c,l,g,n,r);if(t.elt&&!se(t.elt)&&ee(t.elt,\"id\")){var i=document.getElementById(ee(t.elt,\"id\"));var \na={preventScroll:v.focusScroll!==undefined?!v.focusScroll:!Q.config.defaultFocusScroll};if(i){if(t.start&&i.setSelectionRange){try{i.setSelectionRange(t.start,t.end)}catch(e){}}i.focus(a)}}c.classList.remove(Q.config.swappingClass);oe(n.elts,function(e){if(e.classList){e.classList.add(Q.config.settlingClass)}ce(e,\"htm\nx:afterSwap\",u)});if(O(f,/HX-Trigger-After-Swap:/i)){var o=l;if(!se(l)){o=re().body}_e(f,\"HX-Trigger-After-Swap\",o)}var s=function(){oe(n.tasks,function(e){e.call()});oe(n.elts,function(e){if(e.classList){e.classList.remove(Q.config.settlingClass)}ce(e,\"htmx:afterSettle\",u)});if(u.pathInfo.anchor){var \ne=re().getElementById(u.pathInfo.anchor);if(e){e.scrollIntoView({block:\"start\",behavior:\"auto\"})}}if(n.title&&!m){var t=C(\"title\");if(t){t.innerHTML=n.title}else{window.document.title=n.title}}Cr(n.elts,v);if(O(f,/HX-Trigger-After-Settle:/i)){var \nr=l;if(!se(l)){r=re().body}_e(f,\"HX-Trigger-After-Settle\",r)}ie(p)};if(v.settleDelay>0){setTimeout(s,v.settleDelay)}else{s()}}catch(e){fe(l,\"htmx:swapError\",u);ie(x);throw e}};var b=Q.config.globalViewTransitions;if(v.hasOwnProperty(\"transition\")){b=v.transition}if(b&&ce(l,\"htmx:beforeTransition\",u)&&typeof \nPromise!==\"undefined\"&&document.startViewTransition){var w=new Promise(function(e,t){p=e;x=t});var S=y;y=function(){document.startViewTransition(function(){S();return w})}}if(v.swapDelay>0){setTimeout(y,v.swapDelay)}else{y()}}if(a){fe(l,\"htmx:responseError\",le({error:\"Response Status Error Code \"+f.status+\" from \n\"+u.pathInfo.requestPath},u))}}var Xr={};function Dr(){return{init:function(e){return null},onEvent:function(e,t){return true},transformResponse:function(e,t,r){return e},isInlineSwap:function(e){return false},handleSwap:function(e,t,r,n){return false},encodeParameters:function(e,t,r){return null}}}function \nUr(e,t){if(t.init){t.init(r)}Xr[e]=le(Dr(),t)}function Br(e){delete Xr[e]}function Fr(e,r,n){if(e==undefined){return r}if(r==undefined){r=[]}if(n==undefined){n=[]}var t=te(e,\"hx-ext\");if(t){oe(t.split(\",\"),function(e){e=e.replace(/ /g,\"\");if(e.slice(0,7)==\"ignore:\"){n.push(e.slice(7));return}if(n.indexOf(e)<0){var \nt=Xr[e];if(t&&r.indexOf(t)<0){r.push(t)}}})}return Fr(u(e),r,n)}var Vr=false;re().addEventListener(\"DOMContentLoaded\",function(){Vr=true});function jr(e){if(Vr||re().readyState===\"complete\"){e()}else{re().addEventListener(\"DOMContentLoaded\",e)}}function \n_r(){if(Q.config.includeIndicatorStyles!==false){re().head.insertAdjacentHTML(\"beforeend\",\"<style>                      .\"+Q.config.indicatorClass+\"{opacity:0}                      .\"+Q.config.requestClass+\" .\"+Q.config.indicatorClass+\"{opacity:1; transition: opacity 200ms ease-in;}                      \n.\"+Q.config.requestClass+\".\"+Q.config.indicatorClass+\"{opacity:1; transition: opacity 200ms ease-in;}                    </style>\")}}function zr(){var e=re().querySelector('meta[name=\"htmx-config\"]');if(e){return E(e.content)}else{return null}}function $r(){var \ne=zr();if(e){Q.config=le(Q.config,e)}}jr(function(){$r();_r();var e=re().body;zt(e);var t=re().querySelectorAll(\"[hx-trigger='restored'],[data-hx-trigger='restored']\");e.addEventListener(\"htmx:abort\",function(e){var t=e.target;var r=ae(t);if(r&&r.xhr){r.xhr.abort()}});const \nr=window.onpopstate?window.onpopstate.bind(window):null;window.onpopstate=function(e){if(e.state&&e.state.htmx){ar();oe(t,function(e){ce(e,\"htmx:restored\",{document:re(),triggerEvent:ce})})}else{if(r){r(e)}}};setTimeout(function(){ce(e,\"htmx:load\",{});e=null},0)});return Q}()}); \n \nFix:  \n1.\u200b The following buttons do not work on panel 2. Generate Study Material: \na.\u200b Summary \nb.\u200b Key Terms \nc.\u200b Practice Questions \nd.\u200b External Resources \n \n",
        "added_at": "2025-12-01T16:43:37.117373"
      }
    ]
  },
  "ad1a1a86-b1b7-4d67-bcc9-88a9d4bc6aaf": {
    "created_at": "2025-12-02T01:44:31.718433",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-12-02T01:44:31.724437"
      }
    ]
  },
  "8452e8a7-7cf9-474b-a4da-270de744dda8": {
    "created_at": "2025-12-02T01:46:27.925273",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-12-02T01:46:27.930425"
      }
    ]
  }
}